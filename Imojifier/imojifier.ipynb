{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "colab_type": "code",
    "id": "ZbWRR7AfYslG",
    "outputId": "f4f679e4-f1fa-4d1e-9d21-7b9a038cffc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.17.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.33.6)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (2.0.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (2.0.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.11.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.7)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (41.4.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.16.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.6.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.7)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.9.11)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
      "2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "!pip install tensorflow==2.0.0\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "from keras.utils import to_categorical\n",
    "from numpy import argmax\n",
    "nltk.download('punkt')\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Softmax, LSTM, Embedding\n",
    "#tf.enable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b2xsPLIbZL-A"
   },
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "    return words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u4VpB4PufCYq"
   },
   "outputs": [],
   "source": [
    "def read_training_and_test_files():\n",
    "  X_train = pd.read_csv('train_emoji.csv', names = ['X', 'Y'])\n",
    "  X_test = pd.read_csv('test_emoji.csv', names = ['X', 'Y'])  \n",
    "  return X_train, X_test\n",
    "  \n",
    "def read_files_and_retrieve_wordtovec_map(filename):\n",
    "  X_train = pd.read_csv('train_emoji.csv', names = ['X', 'Y'])\n",
    "  X_test = pd.read_csv('test_emoji.csv', names = ['X', 'Y'])\n",
    "  words, word_to_vec_map = read_glove_vecs(filename)\n",
    "  return X_train, X_test, word_to_vec_map\n",
    "\n",
    "def process_training_and_test_values(X_train, X_test):\n",
    "  X_train_X = X_train['X'].values.tolist()\n",
    "  X_train_Y = X_train['Y'].values.tolist()\n",
    "\n",
    "  X_test_X = X_test['X'].values.tolist()\n",
    "  X_test_Y = X_test['Y'].values.tolist()\n",
    "\n",
    "  X_train_lower = [x.lower() for x in X_train_X]\n",
    "  X_test_lower = [x.lower() for x in X_test_X]\n",
    "  return X_train_lower, X_test_lower, X_train_Y, X_test_Y\n",
    "  \n",
    "def tokenize_values(values):\n",
    "  return [nltk.word_tokenize(item) for item in values]\n",
    "\n",
    "def convert_to_nparray(values, value_type):\n",
    "  return np.asarray(values).astype(value_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sTlTQNeSflSp"
   },
   "outputs": [],
   "source": [
    "\"\"\" Converts sentences to average word embeddings. \"\"\"\n",
    "def sentence_to_avg(corpus, word_to_vec_map, vector_dim):\n",
    "    avg = np.zeros((vector_dim,))\n",
    "    averages = []\n",
    "    for sentence in corpus:\n",
    "      sum_val = 0\n",
    "      for word in sentence:\n",
    "        try:\n",
    "          sum_val += word_to_vec_map[word]\n",
    "        except:\n",
    "          pass\n",
    "      avg = sum_val/len(sentence)\n",
    "      averages.append(avg)\n",
    "    return averages\n",
    "\n",
    "\"\"\" Converts sentences to min word embeddings \"\"\"\n",
    "def sentence_to_min(corpus, word_to_vec_map, vector_dim):\n",
    "  minimum = []\n",
    "  for sentence in corpus:\n",
    "    min_val = np.ones((vector_dim,)) * np.inf\n",
    "    for word in sentence:\n",
    "      try:\n",
    "        min_val = np.minimum(min_val, word_to_vec_map[word])\n",
    "      except:\n",
    "        pass\n",
    "    minimum.append(min_val)\n",
    "  return minimum\n",
    "\n",
    "\"\"\" Converts sentences to max word embeddings\"\"\"\n",
    "def sentence_to_max(corpus, word_to_vec_map, vector_dim):\n",
    "  maximum = []\n",
    "  for sentence in corpus:\n",
    "    max_val = np.zeros((vector_dim,))\n",
    "    for word in sentence:\n",
    "      try:\n",
    "        max_val = np.maximum(max_val, word_to_vec_map[word])\n",
    "      except:\n",
    "        pass\n",
    "    maximum.append(max_val)\n",
    "  return maximum\n",
    "\n",
    "\"\"\" Converts sentences to word average embeddings\"\"\"\n",
    "def combine_min_max_avg(corpus, word_to_vec_map, vector_dim):\n",
    "  avg = sentence_to_avg(corpus, word_to_vec_map, vector_dim)\n",
    "  minimum = sentence_to_min(corpus, word_to_vec_map, vector_dim)\n",
    "  maximum = sentence_to_max(corpus, word_to_vec_map, vector_dim)    \n",
    "  return list(zip(avg, minimum, maximum))\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bhIAJqqBPb1C"
   },
   "outputs": [],
   "source": [
    "def Model_to_Fit(hidden_layers, classes):\n",
    "  hidden_layer_size, num_classes = hidden_layers, classes\n",
    "  layers = [\n",
    "          tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "      ]\n",
    "\n",
    "  model = tf.keras.Sequential(layers)\n",
    "  optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "  model.compile(optimizer= optimizer, loss='sparse_categorical_crossentropy', validation_split = 0.2,  metrics = ['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3dsvT2ptcLIL"
   },
   "outputs": [],
   "source": [
    "def CreateConfusionMatrixandReturnPredictions(actual_labels, predicted_labels):\n",
    "  max_indices = []\n",
    "  for row in predicted_labels:\n",
    "    maximum_val = 0\n",
    "    max_index = 0\n",
    "    for index, value in enumerate(row):\n",
    "      if (value > maximum_val):\n",
    "        max_index =  index\n",
    "        maximum_val = value\n",
    "    max_indices.append(max_index)\n",
    "  print(tf.math.confusion_matrix(actual_labels, max_indices, num_classes = 5))\n",
    "  return max_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9_HC7TVk4bh"
   },
   "outputs": [],
   "source": [
    "def ShowCorrectandIncorrectPredictions(test_label_set, pred_val):\n",
    "  correctness_of_predictions = test_label_dataset == pred_val\n",
    "  incorrect_values = []\n",
    "  correct_values = []\n",
    "  for i, w in enumerate(correctness_of_predictions):\n",
    "    if(w == False):\n",
    "      incorrect_values.append((test_label_dataset[i], pred_val[i], i))\n",
    "    if(w == True):\n",
    "      correct_values.append((test_label_dataset[i], pred_val[i], i))\n",
    "\n",
    "  print(\"\"\"----------------------------\n",
    "Incorrect Predictions\"\"\")\n",
    "  for i in incorrect_values:\n",
    "    print(X_test_lower[i[2]], i[0], i[1])\n",
    "  \n",
    "  print(\"\"\"-------------------------- \n",
    "Correct Predictions\"\"\")\n",
    "  for i in correct_values:\n",
    "    print(X_test_lower[i[2]], i[0], i[1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gV-KpPzgP8p7",
    "outputId": "17a2a1f7-b179-4f43-cd39-5cf2e7f25a9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 132 samples\n",
      "Epoch 1/100\n",
      "132/132 [==============================] - 1s 5ms/sample - loss: 2.5661 - accuracy: 0.1288\n",
      "Epoch 2/100\n",
      "132/132 [==============================] - 0s 152us/sample - loss: 2.4809 - accuracy: 0.1288\n",
      "Epoch 3/100\n",
      "132/132 [==============================] - 0s 187us/sample - loss: 2.4012 - accuracy: 0.1288\n",
      "Epoch 4/100\n",
      "132/132 [==============================] - 0s 120us/sample - loss: 2.3252 - accuracy: 0.1288\n",
      "Epoch 5/100\n",
      "132/132 [==============================] - 0s 125us/sample - loss: 2.2538 - accuracy: 0.1288\n",
      "Epoch 6/100\n",
      "132/132 [==============================] - 0s 142us/sample - loss: 2.1886 - accuracy: 0.1288\n",
      "Epoch 7/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 2.1265 - accuracy: 0.1288\n",
      "Epoch 8/100\n",
      "132/132 [==============================] - 0s 108us/sample - loss: 2.0670 - accuracy: 0.1364\n",
      "Epoch 9/100\n",
      "132/132 [==============================] - 0s 109us/sample - loss: 2.0106 - accuracy: 0.1364\n",
      "Epoch 10/100\n",
      "132/132 [==============================] - 0s 115us/sample - loss: 1.9574 - accuracy: 0.1364\n",
      "Epoch 11/100\n",
      "132/132 [==============================] - 0s 107us/sample - loss: 1.9071 - accuracy: 0.1364\n",
      "Epoch 12/100\n",
      "132/132 [==============================] - 0s 115us/sample - loss: 1.8607 - accuracy: 0.1364\n",
      "Epoch 13/100\n",
      "132/132 [==============================] - 0s 119us/sample - loss: 1.8160 - accuracy: 0.1364\n",
      "Epoch 14/100\n",
      "132/132 [==============================] - 0s 146us/sample - loss: 1.7766 - accuracy: 0.1364\n",
      "Epoch 15/100\n",
      "132/132 [==============================] - 0s 153us/sample - loss: 1.7410 - accuracy: 0.1439\n",
      "Epoch 16/100\n",
      "132/132 [==============================] - 0s 174us/sample - loss: 1.7072 - accuracy: 0.1515\n",
      "Epoch 17/100\n",
      "132/132 [==============================] - 0s 120us/sample - loss: 1.6779 - accuracy: 0.1742\n",
      "Epoch 18/100\n",
      "132/132 [==============================] - 0s 130us/sample - loss: 1.6493 - accuracy: 0.1970\n",
      "Epoch 19/100\n",
      "132/132 [==============================] - 0s 130us/sample - loss: 1.6243 - accuracy: 0.2121\n",
      "Epoch 20/100\n",
      "132/132 [==============================] - 0s 147us/sample - loss: 1.5987 - accuracy: 0.2045\n",
      "Epoch 21/100\n",
      "132/132 [==============================] - 0s 109us/sample - loss: 1.5745 - accuracy: 0.2121\n",
      "Epoch 22/100\n",
      "132/132 [==============================] - 0s 170us/sample - loss: 1.5518 - accuracy: 0.2348\n",
      "Epoch 23/100\n",
      "132/132 [==============================] - 0s 136us/sample - loss: 1.5311 - accuracy: 0.2424\n",
      "Epoch 24/100\n",
      "132/132 [==============================] - 0s 162us/sample - loss: 1.5122 - accuracy: 0.2576\n",
      "Epoch 25/100\n",
      "132/132 [==============================] - 0s 185us/sample - loss: 1.4940 - accuracy: 0.2879\n",
      "Epoch 26/100\n",
      "132/132 [==============================] - 0s 159us/sample - loss: 1.4772 - accuracy: 0.2955\n",
      "Epoch 27/100\n",
      "132/132 [==============================] - 0s 185us/sample - loss: 1.4613 - accuracy: 0.3106\n",
      "Epoch 28/100\n",
      "132/132 [==============================] - 0s 193us/sample - loss: 1.4465 - accuracy: 0.3258\n",
      "Epoch 29/100\n",
      "132/132 [==============================] - 0s 171us/sample - loss: 1.4328 - accuracy: 0.3333\n",
      "Epoch 30/100\n",
      "132/132 [==============================] - 0s 223us/sample - loss: 1.4193 - accuracy: 0.3409\n",
      "Epoch 31/100\n",
      "132/132 [==============================] - 0s 159us/sample - loss: 1.4062 - accuracy: 0.3561\n",
      "Epoch 32/100\n",
      "132/132 [==============================] - 0s 167us/sample - loss: 1.3930 - accuracy: 0.3712\n",
      "Epoch 33/100\n",
      "132/132 [==============================] - 0s 163us/sample - loss: 1.3806 - accuracy: 0.3636\n",
      "Epoch 34/100\n",
      "132/132 [==============================] - 0s 146us/sample - loss: 1.3682 - accuracy: 0.3712\n",
      "Epoch 35/100\n",
      "132/132 [==============================] - 0s 290us/sample - loss: 1.3563 - accuracy: 0.3788\n",
      "Epoch 36/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 1.3442 - accuracy: 0.3864\n",
      "Epoch 37/100\n",
      "132/132 [==============================] - 0s 147us/sample - loss: 1.3342 - accuracy: 0.4015\n",
      "Epoch 38/100\n",
      "132/132 [==============================] - 0s 187us/sample - loss: 1.3238 - accuracy: 0.4242\n",
      "Epoch 39/100\n",
      "132/132 [==============================] - 0s 159us/sample - loss: 1.3137 - accuracy: 0.4394\n",
      "Epoch 40/100\n",
      "132/132 [==============================] - 0s 161us/sample - loss: 1.3043 - accuracy: 0.4545\n",
      "Epoch 41/100\n",
      "132/132 [==============================] - 0s 184us/sample - loss: 1.2953 - accuracy: 0.4545\n",
      "Epoch 42/100\n",
      "132/132 [==============================] - 0s 149us/sample - loss: 1.2866 - accuracy: 0.4697\n",
      "Epoch 43/100\n",
      "132/132 [==============================] - 0s 230us/sample - loss: 1.2777 - accuracy: 0.4697\n",
      "Epoch 44/100\n",
      "132/132 [==============================] - 0s 169us/sample - loss: 1.2683 - accuracy: 0.4848\n",
      "Epoch 45/100\n",
      "132/132 [==============================] - 0s 186us/sample - loss: 1.2595 - accuracy: 0.4924\n",
      "Epoch 46/100\n",
      "132/132 [==============================] - 0s 192us/sample - loss: 1.2509 - accuracy: 0.5152\n",
      "Epoch 47/100\n",
      "132/132 [==============================] - 0s 216us/sample - loss: 1.2420 - accuracy: 0.5379\n",
      "Epoch 48/100\n",
      "132/132 [==============================] - 0s 234us/sample - loss: 1.2333 - accuracy: 0.5379\n",
      "Epoch 49/100\n",
      "132/132 [==============================] - 0s 219us/sample - loss: 1.2256 - accuracy: 0.5379\n",
      "Epoch 50/100\n",
      "132/132 [==============================] - 0s 173us/sample - loss: 1.2174 - accuracy: 0.5379\n",
      "Epoch 51/100\n",
      "132/132 [==============================] - 0s 248us/sample - loss: 1.2091 - accuracy: 0.5530\n",
      "Epoch 52/100\n",
      "132/132 [==============================] - 0s 222us/sample - loss: 1.2015 - accuracy: 0.5682\n",
      "Epoch 53/100\n",
      "132/132 [==============================] - 0s 168us/sample - loss: 1.1939 - accuracy: 0.5758\n",
      "Epoch 54/100\n",
      "132/132 [==============================] - 0s 173us/sample - loss: 1.1872 - accuracy: 0.5758\n",
      "Epoch 55/100\n",
      "132/132 [==============================] - 0s 232us/sample - loss: 1.1797 - accuracy: 0.5758\n",
      "Epoch 56/100\n",
      "132/132 [==============================] - 0s 206us/sample - loss: 1.1726 - accuracy: 0.5758\n",
      "Epoch 57/100\n",
      "132/132 [==============================] - 0s 151us/sample - loss: 1.1658 - accuracy: 0.5758\n",
      "Epoch 58/100\n",
      "132/132 [==============================] - 0s 131us/sample - loss: 1.1594 - accuracy: 0.5833\n",
      "Epoch 59/100\n",
      "132/132 [==============================] - 0s 164us/sample - loss: 1.1524 - accuracy: 0.5833\n",
      "Epoch 60/100\n",
      "132/132 [==============================] - 0s 146us/sample - loss: 1.1458 - accuracy: 0.5909\n",
      "Epoch 61/100\n",
      "132/132 [==============================] - 0s 116us/sample - loss: 1.1388 - accuracy: 0.6061\n",
      "Epoch 62/100\n",
      "132/132 [==============================] - 0s 136us/sample - loss: 1.1327 - accuracy: 0.6212\n",
      "Epoch 63/100\n",
      "132/132 [==============================] - 0s 108us/sample - loss: 1.1259 - accuracy: 0.6364\n",
      "Epoch 64/100\n",
      "132/132 [==============================] - 0s 105us/sample - loss: 1.1195 - accuracy: 0.6364\n",
      "Epoch 65/100\n",
      "132/132 [==============================] - 0s 110us/sample - loss: 1.1132 - accuracy: 0.6439\n",
      "Epoch 66/100\n",
      "132/132 [==============================] - 0s 175us/sample - loss: 1.1068 - accuracy: 0.6591\n",
      "Epoch 67/100\n",
      "132/132 [==============================] - 0s 113us/sample - loss: 1.1004 - accuracy: 0.6591\n",
      "Epoch 68/100\n",
      "132/132 [==============================] - 0s 142us/sample - loss: 1.0946 - accuracy: 0.6667\n",
      "Epoch 69/100\n",
      "132/132 [==============================] - 0s 152us/sample - loss: 1.0887 - accuracy: 0.6667\n",
      "Epoch 70/100\n",
      "132/132 [==============================] - 0s 110us/sample - loss: 1.0830 - accuracy: 0.6591\n",
      "Epoch 71/100\n",
      "132/132 [==============================] - 0s 98us/sample - loss: 1.0770 - accuracy: 0.6742\n",
      "Epoch 72/100\n",
      "132/132 [==============================] - 0s 114us/sample - loss: 1.0713 - accuracy: 0.6742\n",
      "Epoch 73/100\n",
      "132/132 [==============================] - 0s 116us/sample - loss: 1.0655 - accuracy: 0.6818\n",
      "Epoch 74/100\n",
      "132/132 [==============================] - 0s 137us/sample - loss: 1.0597 - accuracy: 0.6818\n",
      "Epoch 75/100\n",
      "132/132 [==============================] - 0s 133us/sample - loss: 1.0539 - accuracy: 0.6818\n",
      "Epoch 76/100\n",
      "132/132 [==============================] - 0s 106us/sample - loss: 1.0482 - accuracy: 0.6818\n",
      "Epoch 77/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 1.0434 - accuracy: 0.7045\n",
      "Epoch 78/100\n",
      "132/132 [==============================] - 0s 125us/sample - loss: 1.0379 - accuracy: 0.7121\n",
      "Epoch 79/100\n",
      "132/132 [==============================] - 0s 128us/sample - loss: 1.0332 - accuracy: 0.7121\n",
      "Epoch 80/100\n",
      "132/132 [==============================] - 0s 160us/sample - loss: 1.0286 - accuracy: 0.7197\n",
      "Epoch 81/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 1.0233 - accuracy: 0.7197\n",
      "Epoch 82/100\n",
      "132/132 [==============================] - 0s 109us/sample - loss: 1.0182 - accuracy: 0.7197\n",
      "Epoch 83/100\n",
      "132/132 [==============================] - 0s 120us/sample - loss: 1.0128 - accuracy: 0.7197\n",
      "Epoch 84/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 1.0081 - accuracy: 0.7197\n",
      "Epoch 85/100\n",
      "132/132 [==============================] - 0s 118us/sample - loss: 1.0031 - accuracy: 0.7121\n",
      "Epoch 86/100\n",
      "132/132 [==============================] - 0s 109us/sample - loss: 0.9981 - accuracy: 0.7197\n",
      "Epoch 87/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 0.9931 - accuracy: 0.7348\n",
      "Epoch 88/100\n",
      "132/132 [==============================] - 0s 143us/sample - loss: 0.9881 - accuracy: 0.7348\n",
      "Epoch 89/100\n",
      "132/132 [==============================] - 0s 162us/sample - loss: 0.9826 - accuracy: 0.7273\n",
      "Epoch 90/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 0.9775 - accuracy: 0.7273\n",
      "Epoch 91/100\n",
      "132/132 [==============================] - 0s 131us/sample - loss: 0.9730 - accuracy: 0.7273\n",
      "Epoch 92/100\n",
      "132/132 [==============================] - 0s 121us/sample - loss: 0.9680 - accuracy: 0.7273\n",
      "Epoch 93/100\n",
      "132/132 [==============================] - 0s 117us/sample - loss: 0.9633 - accuracy: 0.7273\n",
      "Epoch 94/100\n",
      "132/132 [==============================] - 0s 101us/sample - loss: 0.9589 - accuracy: 0.7348\n",
      "Epoch 95/100\n",
      "132/132 [==============================] - 0s 95us/sample - loss: 0.9540 - accuracy: 0.7500\n",
      "Epoch 96/100\n",
      "132/132 [==============================] - 0s 101us/sample - loss: 0.9496 - accuracy: 0.7424\n",
      "Epoch 97/100\n",
      "132/132 [==============================] - 0s 124us/sample - loss: 0.9452 - accuracy: 0.7424\n",
      "Epoch 98/100\n",
      "132/132 [==============================] - 0s 161us/sample - loss: 0.9411 - accuracy: 0.7424\n",
      "Epoch 99/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 0.9363 - accuracy: 0.7500\n",
      "Epoch 100/100\n",
      "132/132 [==============================] - 0s 106us/sample - loss: 0.9322 - accuracy: 0.7500\n",
      "56/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 1ms/sample - loss: 1.0540 - accuracy: 0.6786\n",
      "[0.9820391450609479, 0.6785714]\n",
      "tf.Tensor(\n",
      "[[ 2  0  2  2  1]\n",
      " [ 0  5  1  2  0]\n",
      " [ 1  0 15  1  1]\n",
      " [ 0  1  4 11  0]\n",
      " [ 0  0  0  2  5]], shape=(5, 5), dtype=int32)\n",
      "----------------------------\n",
      "Incorrect Predictions\n",
      "work is hard\t 3.0 2\n",
      "this girl is messing with me\t 3.0 2\n",
      "work is horrible\t 3.0 2\n",
      "i love taking breaks\t 0.0 3\n",
      "you brighten my day\t 2.0 0\n",
      "she is a bully\t 3.0 2\n",
      "my grandmother is the love of my life\t 0.0 2\n",
      "enjoy your game 1.0 2\n",
      "he can pitch really well\t 1.0 3\n",
      "dance with me\t 2.0 4\n",
      "i am hungry 4.0 3\n",
      "i like to laugh\t 2.0 3\n",
      "i will  run 1.0 3\n",
      "i like your jacket \t 0.0 3\n",
      "i love you to the stars and back\t 0.0 4\n",
      "yesterday we lost again\t 3.0 1\n",
      "family is all i have\t 0.0 2\n",
      "i did not have breakfast  4.0 3\n",
      "-------------------------- \n",
      "Correct Predictions\n",
      "i want to eat\t 4.0 4\n",
      "he did not answer\t 3.0 3\n",
      "he got a very nice raise\t 2.0 2\n",
      "she got me a nice present\t 2.0 2\n",
      "ha ha ha it was so funny\t 2.0 2\n",
      "he is a good friend\t 2.0 2\n",
      "i am upset\t 3.0 3\n",
      "we had such a lovely dinner tonight\t 2.0 2\n",
      "where is the food\t 4.0 4\n",
      "stop making this joke ha ha ha\t 2.0 2\n",
      "where is the ball\t 1.0 1\n",
      "are you serious 3.0 3\n",
      "let us go play baseball\t 1.0 1\n",
      "this stupid grader is not working \t 3.0 3\n",
      "congratulation for having a baby\t 2.0 2\n",
      "stop pissing me off 3.0 3\n",
      "any suggestions for dinner\t 4.0 4\n",
      "i boiled rice\t 4.0 4\n",
      "why are you feeling bad\t 3.0 3\n",
      "i am upset\t 3.0 3\n",
      "give me the ball 1.0 1\n",
      "valentine day is near\t 2.0 2\n",
      "i miss you so much\t 0.0 0\n",
      "throw the ball\t 1.0 1\n",
      "my life is so boring\t 3.0 3\n",
      "she said yes\t 2.0 2\n",
      "will you be my valentine\t 2.0 2\n",
      "see you at the restaurant\t 4.0 4\n",
      "i miss her\t 0.0 0\n",
      "what is your favorite baseball game\t 1.0 1\n",
      "good job\t 2.0 2\n",
      "what you did was awesome\t 2.0 2\n",
      "ha ha ha lol\t 2.0 2\n",
      "i do not want to joke\t 3.0 3\n",
      "go away\t 3.0 3\n",
      "you are failing this exercise\t 3.0 3\n",
      "good joke\t 2.0 2\n",
      "you deserve this nice prize\t 2.0 2\n"
     ]
    }
   ],
   "source": [
    "# Glove vector of 50 Dimensions for Twitter using Avg\n",
    "X_train, X_test, word_to_vec_map = read_files_and_retrieve_wordtovec_map('glove.twitter.27B.50d.txt')\n",
    "X_train_lower, X_test_lower, X_train_Y, X_test_Y = process_training_and_test_values(X_train, X_test)\n",
    "tokenized_training_data = tokenize_values(X_train_lower)\n",
    "tokenized_test_labels = tokenize_values(X_test_lower)\n",
    "\n",
    "avg = sentence_to_avg(tokenized_training_data, word_to_vec_map, 50) \n",
    "test_avg = sentence_to_avg(tokenized_test_labels, word_to_vec_map, 50)\n",
    "\n",
    "\n",
    "training_dataset = convert_to_nparray(avg, 'float32')\n",
    "testing_dataset = convert_to_nparray(test_avg, 'float32')\n",
    "\n",
    "training_label_dataset = convert_to_nparray(X_train_Y, 'float32')\n",
    "test_label_dataset = convert_to_nparray(X_test_Y, 'float32')\n",
    "\n",
    "model = Model_to_Fit(512, 5)\n",
    "model.fit(x = training_dataset, y = training_label_dataset, epochs = 100)\n",
    "\n",
    "predictions = model.predict(testing_dataset)\n",
    "model.get_weights()\n",
    "print(model.evaluate(testing_dataset, test_label_dataset))\n",
    "\n",
    "pred_val = CreateConfusionMatrixandReturnPredictions(test_label_dataset, convert_to_nparray(predictions, 'float32'))\n",
    "ShowCorrectandIncorrectPredictions(test_label_dataset, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RGbZ5VOXQbez",
    "outputId": "0144b199-981c-461f-c8a5-635d676a5339"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 132 samples\n",
      "Epoch 1/100\n",
      "132/132 [==============================] - 0s 2ms/sample - loss: 2.3069 - accuracy: 0.2803\n",
      "Epoch 2/100\n",
      "132/132 [==============================] - 0s 160us/sample - loss: 2.1812 - accuracy: 0.2879\n",
      "Epoch 3/100\n",
      "132/132 [==============================] - 0s 149us/sample - loss: 2.0771 - accuracy: 0.2955\n",
      "Epoch 4/100\n",
      "132/132 [==============================] - 0s 174us/sample - loss: 1.9761 - accuracy: 0.2955\n",
      "Epoch 5/100\n",
      "132/132 [==============================] - 0s 212us/sample - loss: 1.8987 - accuracy: 0.2879\n",
      "Epoch 6/100\n",
      "132/132 [==============================] - 0s 179us/sample - loss: 1.8309 - accuracy: 0.2727\n",
      "Epoch 7/100\n",
      "132/132 [==============================] - 0s 173us/sample - loss: 1.7752 - accuracy: 0.2652\n",
      "Epoch 8/100\n",
      "132/132 [==============================] - 0s 164us/sample - loss: 1.7276 - accuracy: 0.2576\n",
      "Epoch 9/100\n",
      "132/132 [==============================] - 0s 176us/sample - loss: 1.6933 - accuracy: 0.2652\n",
      "Epoch 10/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 1.6634 - accuracy: 0.2652\n",
      "Epoch 11/100\n",
      "132/132 [==============================] - 0s 190us/sample - loss: 1.6409 - accuracy: 0.2424\n",
      "Epoch 12/100\n",
      "132/132 [==============================] - 0s 181us/sample - loss: 1.6196 - accuracy: 0.2576\n",
      "Epoch 13/100\n",
      "132/132 [==============================] - 0s 222us/sample - loss: 1.6030 - accuracy: 0.2727\n",
      "Epoch 14/100\n",
      "132/132 [==============================] - 0s 190us/sample - loss: 1.5867 - accuracy: 0.2727\n",
      "Epoch 15/100\n",
      "132/132 [==============================] - 0s 188us/sample - loss: 1.5690 - accuracy: 0.2727\n",
      "Epoch 16/100\n",
      "132/132 [==============================] - 0s 197us/sample - loss: 1.5551 - accuracy: 0.2652\n",
      "Epoch 17/100\n",
      "132/132 [==============================] - 0s 295us/sample - loss: 1.5436 - accuracy: 0.2955\n",
      "Epoch 18/100\n",
      "132/132 [==============================] - 0s 141us/sample - loss: 1.5335 - accuracy: 0.2955\n",
      "Epoch 19/100\n",
      "132/132 [==============================] - 0s 180us/sample - loss: 1.5231 - accuracy: 0.3030\n",
      "Epoch 20/100\n",
      "132/132 [==============================] - 0s 177us/sample - loss: 1.5117 - accuracy: 0.2955\n",
      "Epoch 21/100\n",
      "132/132 [==============================] - 0s 179us/sample - loss: 1.5033 - accuracy: 0.2955\n",
      "Epoch 22/100\n",
      "132/132 [==============================] - 0s 173us/sample - loss: 1.4941 - accuracy: 0.3030\n",
      "Epoch 23/100\n",
      "132/132 [==============================] - 0s 162us/sample - loss: 1.4846 - accuracy: 0.3182\n",
      "Epoch 24/100\n",
      "132/132 [==============================] - 0s 143us/sample - loss: 1.4757 - accuracy: 0.3258\n",
      "Epoch 25/100\n",
      "132/132 [==============================] - 0s 204us/sample - loss: 1.4665 - accuracy: 0.3182\n",
      "Epoch 26/100\n",
      "132/132 [==============================] - 0s 174us/sample - loss: 1.4578 - accuracy: 0.3258\n",
      "Epoch 27/100\n",
      "132/132 [==============================] - 0s 164us/sample - loss: 1.4494 - accuracy: 0.3409\n",
      "Epoch 28/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 1.4408 - accuracy: 0.3561\n",
      "Epoch 29/100\n",
      "132/132 [==============================] - 0s 199us/sample - loss: 1.4332 - accuracy: 0.3561\n",
      "Epoch 30/100\n",
      "132/132 [==============================] - 0s 170us/sample - loss: 1.4260 - accuracy: 0.3636\n",
      "Epoch 31/100\n",
      "132/132 [==============================] - 0s 180us/sample - loss: 1.4183 - accuracy: 0.3712\n",
      "Epoch 32/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 1.4114 - accuracy: 0.3712\n",
      "Epoch 33/100\n",
      "132/132 [==============================] - 0s 161us/sample - loss: 1.4038 - accuracy: 0.3712\n",
      "Epoch 34/100\n",
      "132/132 [==============================] - 0s 223us/sample - loss: 1.3957 - accuracy: 0.3712\n",
      "Epoch 35/100\n",
      "132/132 [==============================] - 0s 151us/sample - loss: 1.3879 - accuracy: 0.3712\n",
      "Epoch 36/100\n",
      "132/132 [==============================] - 0s 184us/sample - loss: 1.3803 - accuracy: 0.3712\n",
      "Epoch 37/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 1.3727 - accuracy: 0.3864\n",
      "Epoch 38/100\n",
      "132/132 [==============================] - 0s 192us/sample - loss: 1.3664 - accuracy: 0.3712\n",
      "Epoch 39/100\n",
      "132/132 [==============================] - 0s 209us/sample - loss: 1.3605 - accuracy: 0.3636\n",
      "Epoch 40/100\n",
      "132/132 [==============================] - 0s 135us/sample - loss: 1.3535 - accuracy: 0.3636\n",
      "Epoch 41/100\n",
      "132/132 [==============================] - 0s 156us/sample - loss: 1.3462 - accuracy: 0.3712\n",
      "Epoch 42/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 1.3384 - accuracy: 0.3864\n",
      "Epoch 43/100\n",
      "132/132 [==============================] - 0s 203us/sample - loss: 1.3325 - accuracy: 0.4015\n",
      "Epoch 44/100\n",
      "132/132 [==============================] - 0s 206us/sample - loss: 1.3248 - accuracy: 0.3939\n",
      "Epoch 45/100\n",
      "132/132 [==============================] - 0s 148us/sample - loss: 1.3174 - accuracy: 0.4015\n",
      "Epoch 46/100\n",
      "132/132 [==============================] - 0s 144us/sample - loss: 1.3096 - accuracy: 0.4167\n",
      "Epoch 47/100\n",
      "132/132 [==============================] - 0s 170us/sample - loss: 1.3030 - accuracy: 0.4242\n",
      "Epoch 48/100\n",
      "132/132 [==============================] - 0s 191us/sample - loss: 1.2957 - accuracy: 0.4545\n",
      "Epoch 49/100\n",
      "132/132 [==============================] - 0s 210us/sample - loss: 1.2880 - accuracy: 0.4621\n",
      "Epoch 50/100\n",
      "132/132 [==============================] - 0s 175us/sample - loss: 1.2816 - accuracy: 0.4697\n",
      "Epoch 51/100\n",
      "132/132 [==============================] - 0s 172us/sample - loss: 1.2746 - accuracy: 0.4773\n",
      "Epoch 52/100\n",
      "132/132 [==============================] - 0s 213us/sample - loss: 1.2678 - accuracy: 0.4848\n",
      "Epoch 53/100\n",
      "132/132 [==============================] - 0s 172us/sample - loss: 1.2608 - accuracy: 0.5152\n",
      "Epoch 54/100\n",
      "132/132 [==============================] - 0s 185us/sample - loss: 1.2542 - accuracy: 0.5227\n",
      "Epoch 55/100\n",
      "132/132 [==============================] - 0s 163us/sample - loss: 1.2474 - accuracy: 0.5076\n",
      "Epoch 56/100\n",
      "132/132 [==============================] - 0s 268us/sample - loss: 1.2424 - accuracy: 0.5076\n",
      "Epoch 57/100\n",
      "132/132 [==============================] - 0s 180us/sample - loss: 1.2353 - accuracy: 0.5152\n",
      "Epoch 58/100\n",
      "132/132 [==============================] - 0s 187us/sample - loss: 1.2272 - accuracy: 0.5227\n",
      "Epoch 59/100\n",
      "132/132 [==============================] - 0s 170us/sample - loss: 1.2222 - accuracy: 0.5455\n",
      "Epoch 60/100\n",
      "132/132 [==============================] - 0s 179us/sample - loss: 1.2162 - accuracy: 0.5455\n",
      "Epoch 61/100\n",
      "132/132 [==============================] - 0s 271us/sample - loss: 1.2093 - accuracy: 0.5530\n",
      "Epoch 62/100\n",
      "132/132 [==============================] - 0s 168us/sample - loss: 1.2031 - accuracy: 0.5606\n",
      "Epoch 63/100\n",
      "132/132 [==============================] - 0s 169us/sample - loss: 1.1964 - accuracy: 0.5606\n",
      "Epoch 64/100\n",
      "132/132 [==============================] - 0s 159us/sample - loss: 1.1906 - accuracy: 0.5682\n",
      "Epoch 65/100\n",
      "132/132 [==============================] - 0s 167us/sample - loss: 1.1851 - accuracy: 0.5758\n",
      "Epoch 66/100\n",
      "132/132 [==============================] - 0s 146us/sample - loss: 1.1787 - accuracy: 0.5758\n",
      "Epoch 67/100\n",
      "132/132 [==============================] - 0s 170us/sample - loss: 1.1732 - accuracy: 0.5833\n",
      "Epoch 68/100\n",
      "132/132 [==============================] - 0s 156us/sample - loss: 1.1683 - accuracy: 0.5833\n",
      "Epoch 69/100\n",
      "132/132 [==============================] - 0s 142us/sample - loss: 1.1624 - accuracy: 0.5833\n",
      "Epoch 70/100\n",
      "132/132 [==============================] - 0s 157us/sample - loss: 1.1564 - accuracy: 0.6061\n",
      "Epoch 71/100\n",
      "132/132 [==============================] - 0s 164us/sample - loss: 1.1512 - accuracy: 0.5909\n",
      "Epoch 72/100\n",
      "132/132 [==============================] - 0s 159us/sample - loss: 1.1451 - accuracy: 0.6061\n",
      "Epoch 73/100\n",
      "132/132 [==============================] - 0s 206us/sample - loss: 1.1401 - accuracy: 0.6061\n",
      "Epoch 74/100\n",
      "132/132 [==============================] - 0s 154us/sample - loss: 1.1340 - accuracy: 0.6212\n",
      "Epoch 75/100\n",
      "132/132 [==============================] - 0s 148us/sample - loss: 1.1300 - accuracy: 0.6061\n",
      "Epoch 76/100\n",
      "132/132 [==============================] - 0s 167us/sample - loss: 1.1250 - accuracy: 0.6212\n",
      "Epoch 77/100\n",
      "132/132 [==============================] - 0s 167us/sample - loss: 1.1197 - accuracy: 0.6212\n",
      "Epoch 78/100\n",
      "132/132 [==============================] - 0s 167us/sample - loss: 1.1150 - accuracy: 0.6212\n",
      "Epoch 79/100\n",
      "132/132 [==============================] - 0s 184us/sample - loss: 1.1104 - accuracy: 0.6364\n",
      "Epoch 80/100\n",
      "132/132 [==============================] - 0s 144us/sample - loss: 1.1049 - accuracy: 0.6439\n",
      "Epoch 81/100\n",
      "132/132 [==============================] - 0s 130us/sample - loss: 1.1000 - accuracy: 0.6439\n",
      "Epoch 82/100\n",
      "132/132 [==============================] - 0s 143us/sample - loss: 1.0930 - accuracy: 0.6439\n",
      "Epoch 83/100\n",
      "132/132 [==============================] - 0s 199us/sample - loss: 1.0878 - accuracy: 0.6364\n",
      "Epoch 84/100\n",
      "132/132 [==============================] - 0s 129us/sample - loss: 1.0827 - accuracy: 0.6439\n",
      "Epoch 85/100\n",
      "132/132 [==============================] - 0s 120us/sample - loss: 1.0778 - accuracy: 0.6439\n",
      "Epoch 86/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 1.0733 - accuracy: 0.6439\n",
      "Epoch 87/100\n",
      "132/132 [==============================] - 0s 161us/sample - loss: 1.0682 - accuracy: 0.6439\n",
      "Epoch 88/100\n",
      "132/132 [==============================] - 0s 177us/sample - loss: 1.0643 - accuracy: 0.6439\n",
      "Epoch 89/100\n",
      "132/132 [==============================] - 0s 235us/sample - loss: 1.0598 - accuracy: 0.6515\n",
      "Epoch 90/100\n",
      "132/132 [==============================] - 0s 166us/sample - loss: 1.0545 - accuracy: 0.6591\n",
      "Epoch 91/100\n",
      "132/132 [==============================] - 0s 214us/sample - loss: 1.0498 - accuracy: 0.6742\n",
      "Epoch 92/100\n",
      "132/132 [==============================] - 0s 178us/sample - loss: 1.0452 - accuracy: 0.6742\n",
      "Epoch 93/100\n",
      "132/132 [==============================] - 0s 186us/sample - loss: 1.0404 - accuracy: 0.6742\n",
      "Epoch 94/100\n",
      "132/132 [==============================] - 0s 221us/sample - loss: 1.0359 - accuracy: 0.6818\n",
      "Epoch 95/100\n",
      "132/132 [==============================] - 0s 205us/sample - loss: 1.0307 - accuracy: 0.6818\n",
      "Epoch 96/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 1.0260 - accuracy: 0.6818\n",
      "Epoch 97/100\n",
      "132/132 [==============================] - 0s 172us/sample - loss: 1.0215 - accuracy: 0.6742\n",
      "Epoch 98/100\n",
      "132/132 [==============================] - 0s 281us/sample - loss: 1.0165 - accuracy: 0.6894\n",
      "Epoch 99/100\n",
      "132/132 [==============================] - 0s 195us/sample - loss: 1.0122 - accuracy: 0.6894\n",
      "Epoch 100/100\n",
      "132/132 [==============================] - 0s 161us/sample - loss: 1.0074 - accuracy: 0.6742\n",
      "56/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 1ms/sample - loss: 1.2571 - accuracy: 0.4821\n",
      "[1.2327137674604143, 0.48214287]\n",
      "tf.Tensor(\n",
      "[[ 0  0  5  2  0]\n",
      " [ 0  1  4  3  0]\n",
      " [ 0  0 10  8  0]\n",
      " [ 0  0  4 12  0]\n",
      " [ 0  0  0  3  4]], shape=(5, 5), dtype=int32)\n",
      "----------------------------\n",
      "Incorrect Predictions\n",
      "he got a very nice raise\t 2.0 3\n",
      "she got me a nice present\t 2.0 3\n",
      "where is the ball\t 1.0 2\n",
      "work is hard\t 3.0 2\n",
      "this stupid grader is not working \t 3.0 2\n",
      "work is horrible\t 3.0 2\n",
      "any suggestions for dinner\t 4.0 3\n",
      "i love taking breaks\t 0.0 2\n",
      "you brighten my day\t 2.0 3\n",
      "give me the ball 1.0 3\n",
      "my grandmother is the love of my life\t 0.0 3\n",
      "enjoy your game 1.0 3\n",
      "valentine day is near\t 2.0 3\n",
      "i miss you so much\t 0.0 2\n",
      "throw the ball\t 1.0 3\n",
      "will you be my valentine\t 2.0 3\n",
      "he can pitch really well\t 1.0 2\n",
      "dance with me\t 2.0 3\n",
      "i am hungry 4.0 3\n",
      "i like to laugh\t 2.0 3\n",
      "i will  run 1.0 2\n",
      "i like your jacket \t 0.0 2\n",
      "i miss her\t 0.0 2\n",
      "what is your favorite baseball game\t 1.0 2\n",
      "i love you to the stars and back\t 0.0 3\n",
      "yesterday we lost again\t 3.0 2\n",
      "family is all i have\t 0.0 2\n",
      "you deserve this nice prize\t 2.0 3\n",
      "i did not have breakfast  4.0 3\n",
      "-------------------------- \n",
      "Correct Predictions\n",
      "i want to eat\t 4.0 4\n",
      "he did not answer\t 3.0 3\n",
      "ha ha ha it was so funny\t 2.0 2\n",
      "he is a good friend\t 2.0 2\n",
      "i am upset\t 3.0 3\n",
      "we had such a lovely dinner tonight\t 2.0 2\n",
      "where is the food\t 4.0 4\n",
      "stop making this joke ha ha ha\t 2.0 2\n",
      "this girl is messing with me\t 3.0 3\n",
      "are you serious 3.0 3\n",
      "let us go play baseball\t 1.0 1\n",
      "congratulation for having a baby\t 2.0 2\n",
      "stop pissing me off 3.0 3\n",
      "i boiled rice\t 4.0 4\n",
      "she is a bully\t 3.0 3\n",
      "why are you feeling bad\t 3.0 3\n",
      "i am upset\t 3.0 3\n",
      "my life is so boring\t 3.0 3\n",
      "she said yes\t 2.0 2\n",
      "see you at the restaurant\t 4.0 4\n",
      "good job\t 2.0 2\n",
      "what you did was awesome\t 2.0 2\n",
      "ha ha ha lol\t 2.0 2\n",
      "i do not want to joke\t 3.0 3\n",
      "go away\t 3.0 3\n",
      "you are failing this exercise\t 3.0 3\n",
      "good joke\t 2.0 2\n"
     ]
    }
   ],
   "source": [
    "# Glove vector of 50 Dimensions for Twitter using Min \n",
    "X_train, X_test, word_to_vec_map = read_files_and_retrieve_wordtovec_map('glove.twitter.27B.50d.txt')\n",
    "X_train_lower, X_test_lower, X_train_Y, X_test_Y = process_training_and_test_values(X_train, X_test)\n",
    "tokenized_training_data = tokenize_values(X_train_lower)\n",
    "tokenized_test_labels = tokenize_values(X_test_lower)\n",
    "\n",
    "minimum = sentence_to_min(tokenized_training_data, word_to_vec_map, 50) \n",
    "test_minimum = sentence_to_min(tokenized_test_labels, word_to_vec_map, 50)\n",
    "\n",
    "\n",
    "training_dataset = convert_to_nparray(minimum, 'float32')\n",
    "testing_dataset = convert_to_nparray(test_minimum, 'float32')\n",
    "\n",
    "training_label_dataset = convert_to_nparray(X_train_Y, 'float32')\n",
    "test_label_dataset = convert_to_nparray(X_test_Y, 'float32')\n",
    "\n",
    "model = Model_to_Fit(512, 5)\n",
    "model.fit(x = training_dataset, y = training_label_dataset, epochs = 100)\n",
    "\n",
    "predictions = model.predict(testing_dataset)\n",
    "pred_val = np.amax(convert_to_nparray(predictions, 'float32'), axis = 1)\n",
    "model.get_weights()\n",
    "print(model.evaluate(testing_dataset, test_label_dataset))\n",
    "\n",
    "pred_val = CreateConfusionMatrixandReturnPredictions(test_label_dataset, convert_to_nparray(predictions, 'float32'))\n",
    "ShowCorrectandIncorrectPredictions(test_label_dataset, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JRgSOJ94Q1i5",
    "outputId": "703c1c0d-902c-413e-99c8-d93b75697d91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 132 samples\n",
      "Epoch 1/100\n",
      "132/132 [==============================] - 0s 4ms/sample - loss: 1.8973 - accuracy: 0.1667\n",
      "Epoch 2/100\n",
      "132/132 [==============================] - 0s 117us/sample - loss: 1.8109 - accuracy: 0.2045\n",
      "Epoch 3/100\n",
      "132/132 [==============================] - 0s 108us/sample - loss: 1.7375 - accuracy: 0.2273\n",
      "Epoch 4/100\n",
      "132/132 [==============================] - 0s 128us/sample - loss: 1.6805 - accuracy: 0.2424\n",
      "Epoch 5/100\n",
      "132/132 [==============================] - 0s 102us/sample - loss: 1.6301 - accuracy: 0.2500\n",
      "Epoch 6/100\n",
      "132/132 [==============================] - 0s 103us/sample - loss: 1.5858 - accuracy: 0.2652\n",
      "Epoch 7/100\n",
      "132/132 [==============================] - 0s 104us/sample - loss: 1.5533 - accuracy: 0.2955\n",
      "Epoch 8/100\n",
      "132/132 [==============================] - 0s 115us/sample - loss: 1.5276 - accuracy: 0.3712\n",
      "Epoch 9/100\n",
      "132/132 [==============================] - 0s 118us/sample - loss: 1.5032 - accuracy: 0.4091\n",
      "Epoch 10/100\n",
      "132/132 [==============================] - 0s 120us/sample - loss: 1.4887 - accuracy: 0.4318\n",
      "Epoch 11/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 1.4763 - accuracy: 0.4242\n",
      "Epoch 12/100\n",
      "132/132 [==============================] - 0s 99us/sample - loss: 1.4648 - accuracy: 0.4318\n",
      "Epoch 13/100\n",
      "132/132 [==============================] - 0s 128us/sample - loss: 1.4548 - accuracy: 0.4394\n",
      "Epoch 14/100\n",
      "132/132 [==============================] - 0s 115us/sample - loss: 1.4451 - accuracy: 0.4394\n",
      "Epoch 15/100\n",
      "132/132 [==============================] - 0s 97us/sample - loss: 1.4367 - accuracy: 0.4242\n",
      "Epoch 16/100\n",
      "132/132 [==============================] - 0s 141us/sample - loss: 1.4292 - accuracy: 0.4015\n",
      "Epoch 17/100\n",
      "132/132 [==============================] - 0s 147us/sample - loss: 1.4215 - accuracy: 0.4091\n",
      "Epoch 18/100\n",
      "132/132 [==============================] - 0s 173us/sample - loss: 1.4149 - accuracy: 0.4318\n",
      "Epoch 19/100\n",
      "132/132 [==============================] - 0s 169us/sample - loss: 1.4085 - accuracy: 0.4318\n",
      "Epoch 20/100\n",
      "132/132 [==============================] - 0s 150us/sample - loss: 1.4009 - accuracy: 0.4318\n",
      "Epoch 21/100\n",
      "132/132 [==============================] - 0s 144us/sample - loss: 1.3940 - accuracy: 0.4394\n",
      "Epoch 22/100\n",
      "132/132 [==============================] - 0s 149us/sample - loss: 1.3870 - accuracy: 0.4394\n",
      "Epoch 23/100\n",
      "132/132 [==============================] - 0s 166us/sample - loss: 1.3821 - accuracy: 0.4470\n",
      "Epoch 24/100\n",
      "132/132 [==============================] - 0s 220us/sample - loss: 1.3746 - accuracy: 0.4470\n",
      "Epoch 25/100\n",
      "132/132 [==============================] - 0s 207us/sample - loss: 1.3677 - accuracy: 0.4621\n",
      "Epoch 26/100\n",
      "132/132 [==============================] - 0s 133us/sample - loss: 1.3614 - accuracy: 0.4621\n",
      "Epoch 27/100\n",
      "132/132 [==============================] - 0s 149us/sample - loss: 1.3549 - accuracy: 0.4621\n",
      "Epoch 28/100\n",
      "132/132 [==============================] - 0s 171us/sample - loss: 1.3489 - accuracy: 0.4621\n",
      "Epoch 29/100\n",
      "132/132 [==============================] - 0s 192us/sample - loss: 1.3428 - accuracy: 0.4773\n",
      "Epoch 30/100\n",
      "132/132 [==============================] - 0s 156us/sample - loss: 1.3364 - accuracy: 0.4848\n",
      "Epoch 31/100\n",
      "132/132 [==============================] - 0s 185us/sample - loss: 1.3306 - accuracy: 0.4924\n",
      "Epoch 32/100\n",
      "132/132 [==============================] - 0s 148us/sample - loss: 1.3246 - accuracy: 0.4924\n",
      "Epoch 33/100\n",
      "132/132 [==============================] - 0s 172us/sample - loss: 1.3184 - accuracy: 0.4848\n",
      "Epoch 34/100\n",
      "132/132 [==============================] - 0s 132us/sample - loss: 1.3124 - accuracy: 0.4848\n",
      "Epoch 35/100\n",
      "132/132 [==============================] - 0s 144us/sample - loss: 1.3060 - accuracy: 0.4697\n",
      "Epoch 36/100\n",
      "132/132 [==============================] - 0s 158us/sample - loss: 1.2996 - accuracy: 0.4697\n",
      "Epoch 37/100\n",
      "132/132 [==============================] - 0s 190us/sample - loss: 1.2934 - accuracy: 0.4773\n",
      "Epoch 38/100\n",
      "132/132 [==============================] - 0s 180us/sample - loss: 1.2874 - accuracy: 0.5000\n",
      "Epoch 39/100\n",
      "132/132 [==============================] - 0s 137us/sample - loss: 1.2816 - accuracy: 0.5152\n",
      "Epoch 40/100\n",
      "132/132 [==============================] - 0s 152us/sample - loss: 1.2760 - accuracy: 0.5303\n",
      "Epoch 41/100\n",
      "132/132 [==============================] - 0s 202us/sample - loss: 1.2698 - accuracy: 0.5379\n",
      "Epoch 42/100\n",
      "132/132 [==============================] - 0s 165us/sample - loss: 1.2635 - accuracy: 0.5530\n",
      "Epoch 43/100\n",
      "132/132 [==============================] - 0s 170us/sample - loss: 1.2579 - accuracy: 0.5530\n",
      "Epoch 44/100\n",
      "132/132 [==============================] - 0s 201us/sample - loss: 1.2523 - accuracy: 0.5455\n",
      "Epoch 45/100\n",
      "132/132 [==============================] - 0s 208us/sample - loss: 1.2474 - accuracy: 0.5303\n",
      "Epoch 46/100\n",
      "132/132 [==============================] - 0s 163us/sample - loss: 1.2426 - accuracy: 0.5379\n",
      "Epoch 47/100\n",
      "132/132 [==============================] - 0s 162us/sample - loss: 1.2364 - accuracy: 0.5530\n",
      "Epoch 48/100\n",
      "132/132 [==============================] - 0s 170us/sample - loss: 1.2298 - accuracy: 0.5530\n",
      "Epoch 49/100\n",
      "132/132 [==============================] - 0s 135us/sample - loss: 1.2237 - accuracy: 0.5682\n",
      "Epoch 50/100\n",
      "132/132 [==============================] - 0s 137us/sample - loss: 1.2182 - accuracy: 0.5682\n",
      "Epoch 51/100\n",
      "132/132 [==============================] - 0s 193us/sample - loss: 1.2128 - accuracy: 0.5833\n",
      "Epoch 52/100\n",
      "132/132 [==============================] - 0s 247us/sample - loss: 1.2079 - accuracy: 0.5833\n",
      "Epoch 53/100\n",
      "132/132 [==============================] - 0s 165us/sample - loss: 1.2021 - accuracy: 0.5833\n",
      "Epoch 54/100\n",
      "132/132 [==============================] - 0s 150us/sample - loss: 1.1968 - accuracy: 0.5909\n",
      "Epoch 55/100\n",
      "132/132 [==============================] - 0s 153us/sample - loss: 1.1915 - accuracy: 0.6061\n",
      "Epoch 56/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 1.1870 - accuracy: 0.6136\n",
      "Epoch 57/100\n",
      "132/132 [==============================] - 0s 179us/sample - loss: 1.1818 - accuracy: 0.6288\n",
      "Epoch 58/100\n",
      "132/132 [==============================] - 0s 175us/sample - loss: 1.1768 - accuracy: 0.6364\n",
      "Epoch 59/100\n",
      "132/132 [==============================] - 0s 167us/sample - loss: 1.1711 - accuracy: 0.6439\n",
      "Epoch 60/100\n",
      "132/132 [==============================] - 0s 175us/sample - loss: 1.1661 - accuracy: 0.6364\n",
      "Epoch 61/100\n",
      "132/132 [==============================] - 0s 177us/sample - loss: 1.1617 - accuracy: 0.6364\n",
      "Epoch 62/100\n",
      "132/132 [==============================] - 0s 162us/sample - loss: 1.1568 - accuracy: 0.6364\n",
      "Epoch 63/100\n",
      "132/132 [==============================] - 0s 189us/sample - loss: 1.1517 - accuracy: 0.6515\n",
      "Epoch 64/100\n",
      "132/132 [==============================] - 0s 247us/sample - loss: 1.1461 - accuracy: 0.6364\n",
      "Epoch 65/100\n",
      "132/132 [==============================] - 0s 178us/sample - loss: 1.1407 - accuracy: 0.6439\n",
      "Epoch 66/100\n",
      "132/132 [==============================] - 0s 184us/sample - loss: 1.1373 - accuracy: 0.6591\n",
      "Epoch 67/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 1.1323 - accuracy: 0.6667\n",
      "Epoch 68/100\n",
      "132/132 [==============================] - 0s 142us/sample - loss: 1.1279 - accuracy: 0.6667\n",
      "Epoch 69/100\n",
      "132/132 [==============================] - 0s 167us/sample - loss: 1.1242 - accuracy: 0.6439\n",
      "Epoch 70/100\n",
      "132/132 [==============================] - 0s 134us/sample - loss: 1.1194 - accuracy: 0.6288\n",
      "Epoch 71/100\n",
      "132/132 [==============================] - 0s 227us/sample - loss: 1.1159 - accuracy: 0.6212\n",
      "Epoch 72/100\n",
      "132/132 [==============================] - 0s 166us/sample - loss: 1.1130 - accuracy: 0.6212\n",
      "Epoch 73/100\n",
      "132/132 [==============================] - 0s 168us/sample - loss: 1.1089 - accuracy: 0.6364\n",
      "Epoch 74/100\n",
      "132/132 [==============================] - 0s 174us/sample - loss: 1.1050 - accuracy: 0.6288\n",
      "Epoch 75/100\n",
      "132/132 [==============================] - 0s 132us/sample - loss: 1.1009 - accuracy: 0.6288\n",
      "Epoch 76/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 1.0944 - accuracy: 0.6742\n",
      "Epoch 77/100\n",
      "132/132 [==============================] - 0s 144us/sample - loss: 1.0889 - accuracy: 0.6818\n",
      "Epoch 78/100\n",
      "132/132 [==============================] - 0s 202us/sample - loss: 1.0832 - accuracy: 0.6742\n",
      "Epoch 79/100\n",
      "132/132 [==============================] - 0s 176us/sample - loss: 1.0786 - accuracy: 0.6970\n",
      "Epoch 80/100\n",
      "132/132 [==============================] - 0s 141us/sample - loss: 1.0736 - accuracy: 0.6970\n",
      "Epoch 81/100\n",
      "132/132 [==============================] - 0s 133us/sample - loss: 1.0688 - accuracy: 0.6970\n",
      "Epoch 82/100\n",
      "132/132 [==============================] - 0s 185us/sample - loss: 1.0640 - accuracy: 0.6970\n",
      "Epoch 83/100\n",
      "132/132 [==============================] - 0s 142us/sample - loss: 1.0613 - accuracy: 0.7045\n",
      "Epoch 84/100\n",
      "132/132 [==============================] - 0s 135us/sample - loss: 1.0563 - accuracy: 0.7197\n",
      "Epoch 85/100\n",
      "132/132 [==============================] - 0s 160us/sample - loss: 1.0534 - accuracy: 0.7197\n",
      "Epoch 86/100\n",
      "132/132 [==============================] - 0s 129us/sample - loss: 1.0486 - accuracy: 0.7197\n",
      "Epoch 87/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 1.0443 - accuracy: 0.7121\n",
      "Epoch 88/100\n",
      "132/132 [==============================] - 0s 199us/sample - loss: 1.0407 - accuracy: 0.7121\n",
      "Epoch 89/100\n",
      "132/132 [==============================] - 0s 170us/sample - loss: 1.0364 - accuracy: 0.7197\n",
      "Epoch 90/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 1.0320 - accuracy: 0.7273\n",
      "Epoch 91/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 1.0279 - accuracy: 0.7197\n",
      "Epoch 92/100\n",
      "132/132 [==============================] - 0s 131us/sample - loss: 1.0234 - accuracy: 0.7273\n",
      "Epoch 93/100\n",
      "132/132 [==============================] - 0s 142us/sample - loss: 1.0190 - accuracy: 0.7273\n",
      "Epoch 94/100\n",
      "132/132 [==============================] - 0s 124us/sample - loss: 1.0152 - accuracy: 0.7197\n",
      "Epoch 95/100\n",
      "132/132 [==============================] - 0s 122us/sample - loss: 1.0107 - accuracy: 0.7273\n",
      "Epoch 96/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 1.0072 - accuracy: 0.7424\n",
      "Epoch 97/100\n",
      "132/132 [==============================] - 0s 151us/sample - loss: 1.0025 - accuracy: 0.7424\n",
      "Epoch 98/100\n",
      "132/132 [==============================] - 0s 118us/sample - loss: 0.9986 - accuracy: 0.7424\n",
      "Epoch 99/100\n",
      "132/132 [==============================] - 0s 136us/sample - loss: 0.9952 - accuracy: 0.7273\n",
      "Epoch 100/100\n",
      "132/132 [==============================] - 0s 144us/sample - loss: 0.9917 - accuracy: 0.7273\n",
      "56/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 1ms/sample - loss: 1.2090 - accuracy: 0.5893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=7443, shape=(5, 5), dtype=int32, numpy=\n",
       "array([[ 7,  0,  0,  0,  0],\n",
       "       [ 8,  0,  0,  0,  0],\n",
       "       [18,  0,  0,  0,  0],\n",
       "       [16,  0,  0,  0,  0],\n",
       "       [ 7,  0,  0,  0,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Glove vector of 50 Dimensions for Twitter using Max\n",
    "X_train, X_test, word_to_vec_map = read_files_and_retrieve_wordtovec_map('glove.twitter.27B.50d.txt')\n",
    "X_train_lower, X_test_lower, X_train_Y, X_test_Y = process_training_and_test_values(X_train, X_test)\n",
    "tokenized_training_data = tokenize_values(X_train_lower)\n",
    "tokenized_test_labels = tokenize_values(X_test_lower)\n",
    "\n",
    "maximum = sentence_to_max(tokenized_training_data, word_to_vec_map, 50) \n",
    "test_maximum = sentence_to_max(tokenized_test_labels, word_to_vec_map, 50)\n",
    "\n",
    "\n",
    "training_dataset = convert_to_nparray(maximum, 'float32')\n",
    "testing_dataset = convert_to_nparray(test_maximum, 'float32')\n",
    "\n",
    "training_label_dataset = convert_to_nparray(X_train_Y, 'float32')\n",
    "test_label_dataset = convert_to_nparray(X_test_Y, 'float32')\n",
    "\n",
    "model = Model_to_Fit(512, 5)\n",
    "model.fit(x = training_dataset, y = training_label_dataset, epochs = 100)\n",
    "\n",
    "predictions = model.predict(testing_dataset)\n",
    "pred_val = np.amax(convert_to_nparray(predictions, 'float32'), axis = 1)\n",
    "model.get_weights()\n",
    "model.evaluate(testing_dataset, test_label_dataset)\n",
    "\n",
    "pred_val = CreateConfusionMatrixandReturnPredictions(test_label_dataset, convert_to_nparray(predictions, 'float32'))\n",
    "ShowCorrectandIncorrectPredictions(test_label_dataset, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "rLshAIcHsHQ3",
    "outputId": "64dc9cdd-60ad-46ca-cad3-d05213b0d6a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 150)\n",
      "Train on 132 samples\n",
      "Epoch 1/100\n",
      "132/132 [==============================] - 0s 2ms/sample - loss: 2.0402 - accuracy: 0.2576\n",
      "Epoch 2/100\n",
      "132/132 [==============================] - 0s 116us/sample - loss: 1.8339 - accuracy: 0.2727\n",
      "Epoch 3/100\n",
      "132/132 [==============================] - 0s 104us/sample - loss: 1.6900 - accuracy: 0.3030\n",
      "Epoch 4/100\n",
      "132/132 [==============================] - 0s 121us/sample - loss: 1.6049 - accuracy: 0.3561\n",
      "Epoch 5/100\n",
      "132/132 [==============================] - 0s 120us/sample - loss: 1.5629 - accuracy: 0.3409\n",
      "Epoch 6/100\n",
      "132/132 [==============================] - 0s 109us/sample - loss: 1.5331 - accuracy: 0.3485\n",
      "Epoch 7/100\n",
      "132/132 [==============================] - 0s 104us/sample - loss: 1.5042 - accuracy: 0.3333\n",
      "Epoch 8/100\n",
      "132/132 [==============================] - 0s 142us/sample - loss: 1.4765 - accuracy: 0.3636\n",
      "Epoch 9/100\n",
      "132/132 [==============================] - 0s 94us/sample - loss: 1.4466 - accuracy: 0.3939\n",
      "Epoch 10/100\n",
      "132/132 [==============================] - 0s 110us/sample - loss: 1.4221 - accuracy: 0.4242\n",
      "Epoch 11/100\n",
      "132/132 [==============================] - 0s 110us/sample - loss: 1.4004 - accuracy: 0.4318\n",
      "Epoch 12/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 1.3761 - accuracy: 0.4545\n",
      "Epoch 13/100\n",
      "132/132 [==============================] - 0s 147us/sample - loss: 1.3526 - accuracy: 0.4545\n",
      "Epoch 14/100\n",
      "132/132 [==============================] - 0s 110us/sample - loss: 1.3292 - accuracy: 0.4545\n",
      "Epoch 15/100\n",
      "132/132 [==============================] - 0s 133us/sample - loss: 1.3047 - accuracy: 0.5000\n",
      "Epoch 16/100\n",
      "132/132 [==============================] - 0s 118us/sample - loss: 1.2801 - accuracy: 0.4924\n",
      "Epoch 17/100\n",
      "132/132 [==============================] - 0s 113us/sample - loss: 1.2571 - accuracy: 0.4924\n",
      "Epoch 18/100\n",
      "132/132 [==============================] - 0s 112us/sample - loss: 1.2360 - accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "132/132 [==============================] - 0s 112us/sample - loss: 1.2146 - accuracy: 0.5530\n",
      "Epoch 20/100\n",
      "132/132 [==============================] - 0s 115us/sample - loss: 1.1963 - accuracy: 0.5758\n",
      "Epoch 21/100\n",
      "132/132 [==============================] - 0s 137us/sample - loss: 1.1773 - accuracy: 0.5682\n",
      "Epoch 22/100\n",
      "132/132 [==============================] - 0s 140us/sample - loss: 1.1603 - accuracy: 0.5758\n",
      "Epoch 23/100\n",
      "132/132 [==============================] - 0s 126us/sample - loss: 1.1396 - accuracy: 0.5909\n",
      "Epoch 24/100\n",
      "132/132 [==============================] - 0s 119us/sample - loss: 1.1236 - accuracy: 0.6136\n",
      "Epoch 25/100\n",
      "132/132 [==============================] - 0s 115us/sample - loss: 1.1044 - accuracy: 0.6288\n",
      "Epoch 26/100\n",
      "132/132 [==============================] - 0s 121us/sample - loss: 1.0848 - accuracy: 0.6591\n",
      "Epoch 27/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 1.0628 - accuracy: 0.6515\n",
      "Epoch 28/100\n",
      "132/132 [==============================] - 0s 129us/sample - loss: 1.0495 - accuracy: 0.6894\n",
      "Epoch 29/100\n",
      "132/132 [==============================] - 0s 136us/sample - loss: 1.0347 - accuracy: 0.7273\n",
      "Epoch 30/100\n",
      "132/132 [==============================] - 0s 159us/sample - loss: 1.0179 - accuracy: 0.7197\n",
      "Epoch 31/100\n",
      "132/132 [==============================] - 0s 133us/sample - loss: 1.0007 - accuracy: 0.7273\n",
      "Epoch 32/100\n",
      "132/132 [==============================] - 0s 142us/sample - loss: 0.9833 - accuracy: 0.7273\n",
      "Epoch 33/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 0.9711 - accuracy: 0.6970\n",
      "Epoch 34/100\n",
      "132/132 [==============================] - 0s 179us/sample - loss: 0.9592 - accuracy: 0.6818\n",
      "Epoch 35/100\n",
      "132/132 [==============================] - 0s 196us/sample - loss: 0.9440 - accuracy: 0.7121\n",
      "Epoch 36/100\n",
      "132/132 [==============================] - 0s 209us/sample - loss: 0.9310 - accuracy: 0.7273\n",
      "Epoch 37/100\n",
      "132/132 [==============================] - 0s 128us/sample - loss: 0.9180 - accuracy: 0.7576\n",
      "Epoch 38/100\n",
      "132/132 [==============================] - 0s 148us/sample - loss: 0.9060 - accuracy: 0.7727\n",
      "Epoch 39/100\n",
      "132/132 [==============================] - 0s 147us/sample - loss: 0.8928 - accuracy: 0.7879\n",
      "Epoch 40/100\n",
      "132/132 [==============================] - 0s 144us/sample - loss: 0.8801 - accuracy: 0.8030\n",
      "Epoch 41/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 0.8678 - accuracy: 0.8030\n",
      "Epoch 42/100\n",
      "132/132 [==============================] - 0s 144us/sample - loss: 0.8560 - accuracy: 0.8106\n",
      "Epoch 43/100\n",
      "132/132 [==============================] - 0s 198us/sample - loss: 0.8445 - accuracy: 0.8182\n",
      "Epoch 44/100\n",
      "132/132 [==============================] - 0s 148us/sample - loss: 0.8335 - accuracy: 0.8182\n",
      "Epoch 45/100\n",
      "132/132 [==============================] - 0s 160us/sample - loss: 0.8248 - accuracy: 0.8333\n",
      "Epoch 46/100\n",
      "132/132 [==============================] - 0s 184us/sample - loss: 0.8152 - accuracy: 0.8182\n",
      "Epoch 47/100\n",
      "132/132 [==============================] - 0s 143us/sample - loss: 0.8027 - accuracy: 0.8333\n",
      "Epoch 48/100\n",
      "132/132 [==============================] - 0s 158us/sample - loss: 0.7923 - accuracy: 0.8409\n",
      "Epoch 49/100\n",
      "132/132 [==============================] - 0s 124us/sample - loss: 0.7853 - accuracy: 0.8258\n",
      "Epoch 50/100\n",
      "132/132 [==============================] - 0s 136us/sample - loss: 0.7789 - accuracy: 0.8106\n",
      "Epoch 51/100\n",
      "132/132 [==============================] - 0s 125us/sample - loss: 0.7687 - accuracy: 0.8333\n",
      "Epoch 52/100\n",
      "132/132 [==============================] - 0s 144us/sample - loss: 0.7562 - accuracy: 0.8333\n",
      "Epoch 53/100\n",
      "132/132 [==============================] - 0s 177us/sample - loss: 0.7478 - accuracy: 0.8485\n",
      "Epoch 54/100\n",
      "132/132 [==============================] - 0s 152us/sample - loss: 0.7366 - accuracy: 0.8485\n",
      "Epoch 55/100\n",
      "132/132 [==============================] - 0s 148us/sample - loss: 0.7283 - accuracy: 0.8636\n",
      "Epoch 56/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 0.7202 - accuracy: 0.8636\n",
      "Epoch 57/100\n",
      "132/132 [==============================] - 0s 153us/sample - loss: 0.7140 - accuracy: 0.8561\n",
      "Epoch 58/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 0.7063 - accuracy: 0.8485\n",
      "Epoch 59/100\n",
      "132/132 [==============================] - 0s 162us/sample - loss: 0.6991 - accuracy: 0.8561\n",
      "Epoch 60/100\n",
      "132/132 [==============================] - 0s 150us/sample - loss: 0.6910 - accuracy: 0.8561\n",
      "Epoch 61/100\n",
      "132/132 [==============================] - 0s 156us/sample - loss: 0.6835 - accuracy: 0.8636\n",
      "Epoch 62/100\n",
      "132/132 [==============================] - 0s 137us/sample - loss: 0.6767 - accuracy: 0.8561\n",
      "Epoch 63/100\n",
      "132/132 [==============================] - 0s 175us/sample - loss: 0.6693 - accuracy: 0.8712\n",
      "Epoch 64/100\n",
      "132/132 [==============================] - 0s 205us/sample - loss: 0.6617 - accuracy: 0.8636\n",
      "Epoch 65/100\n",
      "132/132 [==============================] - 0s 131us/sample - loss: 0.6555 - accuracy: 0.8636\n",
      "Epoch 66/100\n",
      "132/132 [==============================] - 0s 152us/sample - loss: 0.6528 - accuracy: 0.8561\n",
      "Epoch 67/100\n",
      "132/132 [==============================] - 0s 189us/sample - loss: 0.6491 - accuracy: 0.8561\n",
      "Epoch 68/100\n",
      "132/132 [==============================] - 0s 178us/sample - loss: 0.6454 - accuracy: 0.8636\n",
      "Epoch 69/100\n",
      "132/132 [==============================] - 0s 165us/sample - loss: 0.6342 - accuracy: 0.8485\n",
      "Epoch 70/100\n",
      "132/132 [==============================] - 0s 154us/sample - loss: 0.6260 - accuracy: 0.8712\n",
      "Epoch 71/100\n",
      "132/132 [==============================] - 0s 146us/sample - loss: 0.6182 - accuracy: 0.8712\n",
      "Epoch 72/100\n",
      "132/132 [==============================] - 0s 140us/sample - loss: 0.6125 - accuracy: 0.8712\n",
      "Epoch 73/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 0.6079 - accuracy: 0.8712\n",
      "Epoch 74/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 0.6025 - accuracy: 0.8788\n",
      "Epoch 75/100\n",
      "132/132 [==============================] - 0s 132us/sample - loss: 0.5975 - accuracy: 0.8788\n",
      "Epoch 76/100\n",
      "132/132 [==============================] - 0s 208us/sample - loss: 0.5929 - accuracy: 0.8788\n",
      "Epoch 77/100\n",
      "132/132 [==============================] - 0s 166us/sample - loss: 0.5881 - accuracy: 0.8788\n",
      "Epoch 78/100\n",
      "132/132 [==============================] - 0s 171us/sample - loss: 0.5830 - accuracy: 0.8864\n",
      "Epoch 79/100\n",
      "132/132 [==============================] - 0s 170us/sample - loss: 0.5790 - accuracy: 0.8788\n",
      "Epoch 80/100\n",
      "132/132 [==============================] - 0s 124us/sample - loss: 0.5726 - accuracy: 0.8864\n",
      "Epoch 81/100\n",
      "132/132 [==============================] - 0s 174us/sample - loss: 0.5693 - accuracy: 0.8864\n",
      "Epoch 82/100\n",
      "132/132 [==============================] - 0s 167us/sample - loss: 0.5630 - accuracy: 0.8864\n",
      "Epoch 83/100\n",
      "132/132 [==============================] - 0s 200us/sample - loss: 0.5603 - accuracy: 0.8939\n",
      "Epoch 84/100\n",
      "132/132 [==============================] - 0s 146us/sample - loss: 0.5550 - accuracy: 0.8939\n",
      "Epoch 85/100\n",
      "132/132 [==============================] - 0s 180us/sample - loss: 0.5516 - accuracy: 0.9091\n",
      "Epoch 86/100\n",
      "132/132 [==============================] - 0s 168us/sample - loss: 0.5470 - accuracy: 0.9091\n",
      "Epoch 87/100\n",
      "132/132 [==============================] - 0s 237us/sample - loss: 0.5430 - accuracy: 0.9091\n",
      "Epoch 88/100\n",
      "132/132 [==============================] - 0s 161us/sample - loss: 0.5379 - accuracy: 0.9091\n",
      "Epoch 89/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 0.5332 - accuracy: 0.9015\n",
      "Epoch 90/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 0.5307 - accuracy: 0.9091\n",
      "Epoch 91/100\n",
      "132/132 [==============================] - 0s 183us/sample - loss: 0.5257 - accuracy: 0.9091\n",
      "Epoch 92/100\n",
      "132/132 [==============================] - 0s 192us/sample - loss: 0.5219 - accuracy: 0.9091\n",
      "Epoch 93/100\n",
      "132/132 [==============================] - 0s 161us/sample - loss: 0.5174 - accuracy: 0.9167\n",
      "Epoch 94/100\n",
      "132/132 [==============================] - 0s 144us/sample - loss: 0.5130 - accuracy: 0.9167\n",
      "Epoch 95/100\n",
      "132/132 [==============================] - 0s 184us/sample - loss: 0.5089 - accuracy: 0.9091\n",
      "Epoch 96/100\n",
      "132/132 [==============================] - 0s 141us/sample - loss: 0.5061 - accuracy: 0.9091\n",
      "Epoch 97/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 0.5029 - accuracy: 0.9091\n",
      "Epoch 98/100\n",
      "132/132 [==============================] - 0s 170us/sample - loss: 0.5008 - accuracy: 0.9167\n",
      "Epoch 99/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 0.4976 - accuracy: 0.9091\n",
      "Epoch 100/100\n",
      "132/132 [==============================] - 0s 197us/sample - loss: 0.4929 - accuracy: 0.9091\n",
      "56/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 1ms/sample - loss: 0.7338 - accuracy: 0.8214\n",
      "[0.6815964920180184, 0.8214286]\n",
      "tf.Tensor(\n",
      "[[ 6  0  1  0  0]\n",
      " [ 0  7  1  0  0]\n",
      " [ 2  0 15  1  0]\n",
      " [ 0  2  3 11  0]\n",
      " [ 0  0  0  0  7]], shape=(5, 5), dtype=int32)\n",
      "----------------------------\n",
      "Incorrect Predictions\n",
      "work is hard\t 3.0 2\n",
      "work is horrible\t 3.0 2\n",
      "congratulation for having a baby\t 2.0 0\n",
      "you brighten my day\t 2.0 3\n",
      "she is a bully\t 3.0 2\n",
      "he can pitch really well\t 1.0 2\n",
      "dance with me\t 2.0 0\n",
      "go away\t 3.0 1\n",
      "yesterday we lost again\t 3.0 1\n",
      "family is all i have\t 0.0 2\n",
      "-------------------------- \n",
      "Correct Predictions\n",
      "i want to eat\t 4.0 4\n",
      "he did not answer\t 3.0 3\n",
      "he got a very nice raise\t 2.0 2\n",
      "she got me a nice present\t 2.0 2\n",
      "ha ha ha it was so funny\t 2.0 2\n",
      "he is a good friend\t 2.0 2\n",
      "i am upset\t 3.0 3\n",
      "we had such a lovely dinner tonight\t 2.0 2\n",
      "where is the food\t 4.0 4\n",
      "stop making this joke ha ha ha\t 2.0 2\n",
      "where is the ball\t 1.0 1\n",
      "this girl is messing with me\t 3.0 3\n",
      "are you serious 3.0 3\n",
      "let us go play baseball\t 1.0 1\n",
      "this stupid grader is not working \t 3.0 3\n",
      "stop pissing me off 3.0 3\n",
      "any suggestions for dinner\t 4.0 4\n",
      "i love taking breaks\t 0.0 0\n",
      "i boiled rice\t 4.0 4\n",
      "why are you feeling bad\t 3.0 3\n",
      "i am upset\t 3.0 3\n",
      "give me the ball 1.0 1\n",
      "my grandmother is the love of my life\t 0.0 0\n",
      "enjoy your game 1.0 1\n",
      "valentine day is near\t 2.0 2\n",
      "i miss you so much\t 0.0 0\n",
      "throw the ball\t 1.0 1\n",
      "my life is so boring\t 3.0 3\n",
      "she said yes\t 2.0 2\n",
      "will you be my valentine\t 2.0 2\n",
      "i am hungry 4.0 4\n",
      "see you at the restaurant\t 4.0 4\n",
      "i like to laugh\t 2.0 2\n",
      "i will  run 1.0 1\n",
      "i like your jacket \t 0.0 0\n",
      "i miss her\t 0.0 0\n",
      "what is your favorite baseball game\t 1.0 1\n",
      "good job\t 2.0 2\n",
      "i love you to the stars and back\t 0.0 0\n",
      "what you did was awesome\t 2.0 2\n",
      "ha ha ha lol\t 2.0 2\n",
      "i do not want to joke\t 3.0 3\n",
      "you are failing this exercise\t 3.0 3\n",
      "good joke\t 2.0 2\n",
      "you deserve this nice prize\t 2.0 2\n",
      "i did not have breakfast  4.0 4\n"
     ]
    }
   ],
   "source": [
    "# Glove vector of 50 Dimensions for Twitter using Max-Min-Avg \n",
    "X_train, X_test, word_to_vec_map = read_files_and_retrieve_wordtovec_map('glove.twitter.27B.50d.txt')\n",
    "X_train_lower, X_test_lower, X_train_Y, X_test_Y = process_training_and_test_values(X_train, X_test)\n",
    "tokenized_training_data = tokenize_values(X_train_lower)\n",
    "tokenized_test_labels = tokenize_values(X_test_lower)\n",
    "\n",
    "combined_dataset = combine_min_max_avg(tokenized_training_data, word_to_vec_map, 50)\n",
    "test_combined_dataset = combine_min_max_avg(tokenized_test_labels, word_to_vec_map, 50)\n",
    "\n",
    "\n",
    "training_dataset = convert_to_nparray(combined_dataset, 'float32')\n",
    "testing_dataset = convert_to_nparray(test_combined_dataset, 'float32')\n",
    "\n",
    "training_label_dataset = convert_to_nparray(X_train_Y, 'float32')\n",
    "test_label_dataset = convert_to_nparray(X_test_Y, 'float32')\n",
    "\n",
    "train = []\n",
    "for i in training_dataset:\n",
    "  train.append(i.flatten())\n",
    "  \n",
    "test = []\n",
    "for i in testing_dataset:\n",
    "  test.append(i.flatten())\n",
    "  \n",
    "test_val = convert_to_nparray(test, 'float32')\n",
    "\n",
    "train_val = convert_to_nparray(train, 'float32')\n",
    "print(train_val.shape)\n",
    "\n",
    "model = Model_to_Fit(512, 5)\n",
    "model.fit(x = train_val, y = training_label_dataset, epochs = 100)\n",
    "\n",
    "predictions = model.predict(test_val)\n",
    "print(model.evaluate(test_val, test_label_dataset))\n",
    "\n",
    "pred_val = CreateConfusionMatrixandReturnPredictions(test_label_dataset, convert_to_nparray(predictions, 'float32'))\n",
    "ShowCorrectandIncorrectPredictions(test_label_dataset, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "pOlDWwRqOqjQ",
    "outputId": "974c0e12-de44-4719-9cae-df05d748d5d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4yNHGCPcioaa",
    "outputId": "2ad93d45-76e9-4be2-892f-25c5d5efc18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "56/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 82us/sample - loss: 0.9580 - accuracy: 0.8214\n",
      "tf.Tensor(\n",
      "[[ 5  0  0  2  0]\n",
      " [ 0  6  0  2  0]\n",
      " [ 0  0 16  2  0]\n",
      " [ 0  0  2 14  0]\n",
      " [ 0  0  0  2  5]], shape=(5, 5), dtype=int32)\n",
      "----------------------------\n",
      "Incorrect Predictions\n",
      "work is hard\t 3.0 2\n",
      "work is horrible\t 3.0 2\n",
      "you brighten my day\t 2.0 3\n",
      "will you be my valentine\t 2.0 3\n",
      "he can pitch really well\t 1.0 3\n",
      "see you at the restaurant\t 4.0 3\n",
      "i will  run 1.0 3\n",
      "i like your jacket \t 0.0 3\n",
      "family is all i have\t 0.0 3\n",
      "i did not have breakfast  4.0 3\n",
      "-------------------------- \n",
      "Correct Predictions\n",
      "i want to eat\t 4.0 4\n",
      "he did not answer\t 3.0 3\n",
      "he got a very nice raise\t 2.0 2\n",
      "she got me a nice present\t 2.0 2\n",
      "ha ha ha it was so funny\t 2.0 2\n",
      "he is a good friend\t 2.0 2\n",
      "i am upset\t 3.0 3\n",
      "we had such a lovely dinner tonight\t 2.0 2\n",
      "where is the food\t 4.0 4\n",
      "stop making this joke ha ha ha\t 2.0 2\n",
      "where is the ball\t 1.0 1\n",
      "this girl is messing with me\t 3.0 3\n",
      "are you serious 3.0 3\n",
      "let us go play baseball\t 1.0 1\n",
      "this stupid grader is not working \t 3.0 3\n",
      "congratulation for having a baby\t 2.0 2\n",
      "stop pissing me off 3.0 3\n",
      "any suggestions for dinner\t 4.0 4\n",
      "i love taking breaks\t 0.0 0\n",
      "i boiled rice\t 4.0 4\n",
      "she is a bully\t 3.0 3\n",
      "why are you feeling bad\t 3.0 3\n",
      "i am upset\t 3.0 3\n",
      "give me the ball 1.0 1\n",
      "my grandmother is the love of my life\t 0.0 0\n",
      "enjoy your game 1.0 1\n",
      "valentine day is near\t 2.0 2\n",
      "i miss you so much\t 0.0 0\n",
      "throw the ball\t 1.0 1\n",
      "my life is so boring\t 3.0 3\n",
      "she said yes\t 2.0 2\n",
      "dance with me\t 2.0 2\n",
      "i am hungry 4.0 4\n",
      "i like to laugh\t 2.0 2\n",
      "i miss her\t 0.0 0\n",
      "what is your favorite baseball game\t 1.0 1\n",
      "good job\t 2.0 2\n",
      "i love you to the stars and back\t 0.0 0\n",
      "what you did was awesome\t 2.0 2\n",
      "ha ha ha lol\t 2.0 2\n",
      "i do not want to joke\t 3.0 3\n",
      "go away\t 3.0 3\n",
      "yesterday we lost again\t 3.0 3\n",
      "you are failing this exercise\t 3.0 3\n",
      "good joke\t 2.0 2\n",
      "you deserve this nice prize\t 2.0 2\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec of 300 dimensions on Google News using avg\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "X_train, X_test = read_training_and_test_files()\n",
    "X_train_lower, X_test_lower, X_train_Y, X_test_Y = process_training_and_test_values(X_train, X_test)\n",
    "tokenized_training_data = tokenize_values(X_train_lower)\n",
    "tokenized_test_labels = tokenize_values(X_test_lower)\n",
    "\n",
    "\n",
    "avg = sentence_to_avg(tokenized_training_data, model, 300) \n",
    "test_avg = sentence_to_avg(tokenized_test_labels, model, 300)\n",
    "\n",
    "training_dataset = convert_to_nparray(avg, 'float32')\n",
    "testing_dataset = convert_to_nparray(test_avg, 'float32')\n",
    "\n",
    "training_label_dataset = convert_to_nparray(X_train_Y, 'float32')\n",
    "test_label_dataset = convert_to_nparray(X_test_Y, 'float32')\n",
    "\n",
    "model = Model_to_Fit(512, 5)\n",
    "model.fit(x = training_dataset, y = training_label_dataset, epochs = 100)\n",
    "\n",
    "predictions = model.predict(testing_dataset)\n",
    "pred_val = np.amax(convert_to_nparray(predictions, 'float32'), axis = 1)\n",
    "model.get_weights()\n",
    "model.evaluate(testing_dataset, test_label_dataset)\n",
    "\n",
    "#tf.math.confusion_matrix(test_label_dataset, pred_val, num_classes = 5)\n",
    "pred_val = CreateConfusionMatrixandReturnPredictions(test_label_dataset, convert_to_nparray(predictions, 'float32'))\n",
    "ShowCorrectandIncorrectPredictions(test_label_dataset, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Tvc0xeU1fK1U",
    "outputId": "ea5ac53f-ceb2-4d61-feb4-4cea2c3f6179"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 132 samples\n",
      "Epoch 1/100\n",
      "132/132 [==============================] - 0s 2ms/sample - loss: 1.5720 - accuracy: 0.2424\n",
      "Epoch 2/100\n",
      "132/132 [==============================] - 0s 142us/sample - loss: 1.5284 - accuracy: 0.3106\n",
      "Epoch 3/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 1.4972 - accuracy: 0.3561\n",
      "Epoch 4/100\n",
      "132/132 [==============================] - 0s 141us/sample - loss: 1.4663 - accuracy: 0.3636\n",
      "Epoch 5/100\n",
      "132/132 [==============================] - 0s 131us/sample - loss: 1.4380 - accuracy: 0.3788\n",
      "Epoch 6/100\n",
      "132/132 [==============================] - 0s 144us/sample - loss: 1.4123 - accuracy: 0.4394\n",
      "Epoch 7/100\n",
      "132/132 [==============================] - 0s 120us/sample - loss: 1.3883 - accuracy: 0.4470\n",
      "Epoch 8/100\n",
      "132/132 [==============================] - 0s 141us/sample - loss: 1.3604 - accuracy: 0.4470\n",
      "Epoch 9/100\n",
      "132/132 [==============================] - 0s 128us/sample - loss: 1.3351 - accuracy: 0.4545\n",
      "Epoch 10/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 1.3109 - accuracy: 0.4848\n",
      "Epoch 11/100\n",
      "132/132 [==============================] - 0s 143us/sample - loss: 1.2873 - accuracy: 0.5152\n",
      "Epoch 12/100\n",
      "132/132 [==============================] - 0s 179us/sample - loss: 1.2668 - accuracy: 0.5379\n",
      "Epoch 13/100\n",
      "132/132 [==============================] - 0s 164us/sample - loss: 1.2454 - accuracy: 0.6136\n",
      "Epoch 14/100\n",
      "132/132 [==============================] - 0s 142us/sample - loss: 1.2246 - accuracy: 0.6288\n",
      "Epoch 15/100\n",
      "132/132 [==============================] - 0s 192us/sample - loss: 1.2044 - accuracy: 0.6742\n",
      "Epoch 16/100\n",
      "132/132 [==============================] - 0s 135us/sample - loss: 1.1850 - accuracy: 0.6818\n",
      "Epoch 17/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 1.1671 - accuracy: 0.6894\n",
      "Epoch 18/100\n",
      "132/132 [==============================] - 0s 120us/sample - loss: 1.1480 - accuracy: 0.7045\n",
      "Epoch 19/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 1.1304 - accuracy: 0.7197\n",
      "Epoch 20/100\n",
      "132/132 [==============================] - 0s 166us/sample - loss: 1.1138 - accuracy: 0.7273\n",
      "Epoch 21/100\n",
      "132/132 [==============================] - 0s 159us/sample - loss: 1.0987 - accuracy: 0.7500\n",
      "Epoch 22/100\n",
      "132/132 [==============================] - 0s 132us/sample - loss: 1.0813 - accuracy: 0.7424\n",
      "Epoch 23/100\n",
      "132/132 [==============================] - 0s 109us/sample - loss: 1.0637 - accuracy: 0.7652\n",
      "Epoch 24/100\n",
      "132/132 [==============================] - 0s 102us/sample - loss: 1.0485 - accuracy: 0.8030\n",
      "Epoch 25/100\n",
      "132/132 [==============================] - 0s 131us/sample - loss: 1.0336 - accuracy: 0.8182\n",
      "Epoch 26/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 1.0186 - accuracy: 0.8106\n",
      "Epoch 27/100\n",
      "132/132 [==============================] - 0s 158us/sample - loss: 1.0050 - accuracy: 0.8182\n",
      "Epoch 28/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 0.9910 - accuracy: 0.8182\n",
      "Epoch 29/100\n",
      "132/132 [==============================] - 0s 154us/sample - loss: 0.9782 - accuracy: 0.8030\n",
      "Epoch 30/100\n",
      "132/132 [==============================] - 0s 143us/sample - loss: 0.9645 - accuracy: 0.8258\n",
      "Epoch 31/100\n",
      "132/132 [==============================] - 0s 186us/sample - loss: 0.9518 - accuracy: 0.8106\n",
      "Epoch 32/100\n",
      "132/132 [==============================] - 0s 154us/sample - loss: 0.9404 - accuracy: 0.8106\n",
      "Epoch 33/100\n",
      "132/132 [==============================] - 0s 132us/sample - loss: 0.9284 - accuracy: 0.8258\n",
      "Epoch 34/100\n",
      "132/132 [==============================] - 0s 140us/sample - loss: 0.9182 - accuracy: 0.8333\n",
      "Epoch 35/100\n",
      "132/132 [==============================] - 0s 198us/sample - loss: 0.9062 - accuracy: 0.8409\n",
      "Epoch 36/100\n",
      "132/132 [==============================] - 0s 127us/sample - loss: 0.8949 - accuracy: 0.8409\n",
      "Epoch 37/100\n",
      "132/132 [==============================] - 0s 178us/sample - loss: 0.8842 - accuracy: 0.8485\n",
      "Epoch 38/100\n",
      "132/132 [==============================] - 0s 126us/sample - loss: 0.8729 - accuracy: 0.8561\n",
      "Epoch 39/100\n",
      "132/132 [==============================] - 0s 154us/sample - loss: 0.8631 - accuracy: 0.8561\n",
      "Epoch 40/100\n",
      "132/132 [==============================] - 0s 154us/sample - loss: 0.8536 - accuracy: 0.8561\n",
      "Epoch 41/100\n",
      "132/132 [==============================] - 0s 163us/sample - loss: 0.8445 - accuracy: 0.8561\n",
      "Epoch 42/100\n",
      "132/132 [==============================] - 0s 131us/sample - loss: 0.8350 - accuracy: 0.8561\n",
      "Epoch 43/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 0.8254 - accuracy: 0.8636\n",
      "Epoch 44/100\n",
      "132/132 [==============================] - 0s 167us/sample - loss: 0.8153 - accuracy: 0.8636\n",
      "Epoch 45/100\n",
      "132/132 [==============================] - 0s 141us/sample - loss: 0.8047 - accuracy: 0.8636\n",
      "Epoch 46/100\n",
      "132/132 [==============================] - 0s 182us/sample - loss: 0.7964 - accuracy: 0.8636\n",
      "Epoch 47/100\n",
      "132/132 [==============================] - 0s 140us/sample - loss: 0.7876 - accuracy: 0.8561\n",
      "Epoch 48/100\n",
      "132/132 [==============================] - 0s 134us/sample - loss: 0.7788 - accuracy: 0.8561\n",
      "Epoch 49/100\n",
      "132/132 [==============================] - 0s 131us/sample - loss: 0.7713 - accuracy: 0.8636\n",
      "Epoch 50/100\n",
      "132/132 [==============================] - 0s 212us/sample - loss: 0.7626 - accuracy: 0.8636\n",
      "Epoch 51/100\n",
      "132/132 [==============================] - 0s 190us/sample - loss: 0.7545 - accuracy: 0.8636\n",
      "Epoch 52/100\n",
      "132/132 [==============================] - 0s 133us/sample - loss: 0.7473 - accuracy: 0.8636\n",
      "Epoch 53/100\n",
      "132/132 [==============================] - 0s 159us/sample - loss: 0.7394 - accuracy: 0.8712\n",
      "Epoch 54/100\n",
      "132/132 [==============================] - 0s 179us/sample - loss: 0.7322 - accuracy: 0.8788\n",
      "Epoch 55/100\n",
      "132/132 [==============================] - 0s 254us/sample - loss: 0.7245 - accuracy: 0.8788\n",
      "Epoch 56/100\n",
      "132/132 [==============================] - 0s 151us/sample - loss: 0.7175 - accuracy: 0.8788\n",
      "Epoch 57/100\n",
      "132/132 [==============================] - 0s 152us/sample - loss: 0.7109 - accuracy: 0.8636\n",
      "Epoch 58/100\n",
      "132/132 [==============================] - 0s 165us/sample - loss: 0.7039 - accuracy: 0.8636\n",
      "Epoch 59/100\n",
      "132/132 [==============================] - 0s 159us/sample - loss: 0.6974 - accuracy: 0.8636\n",
      "Epoch 60/100\n",
      "132/132 [==============================] - 0s 206us/sample - loss: 0.6906 - accuracy: 0.8712\n",
      "Epoch 61/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 0.6839 - accuracy: 0.8712\n",
      "Epoch 62/100\n",
      "132/132 [==============================] - 0s 141us/sample - loss: 0.6774 - accuracy: 0.8788\n",
      "Epoch 63/100\n",
      "132/132 [==============================] - 0s 141us/sample - loss: 0.6716 - accuracy: 0.8788\n",
      "Epoch 64/100\n",
      "132/132 [==============================] - 0s 124us/sample - loss: 0.6651 - accuracy: 0.8788\n",
      "Epoch 65/100\n",
      "132/132 [==============================] - 0s 174us/sample - loss: 0.6597 - accuracy: 0.8939\n",
      "Epoch 66/100\n",
      "132/132 [==============================] - 0s 192us/sample - loss: 0.6533 - accuracy: 0.8939\n",
      "Epoch 67/100\n",
      "132/132 [==============================] - 0s 147us/sample - loss: 0.6477 - accuracy: 0.8939\n",
      "Epoch 68/100\n",
      "132/132 [==============================] - 0s 150us/sample - loss: 0.6420 - accuracy: 0.8939\n",
      "Epoch 69/100\n",
      "132/132 [==============================] - 0s 158us/sample - loss: 0.6361 - accuracy: 0.8939\n",
      "Epoch 70/100\n",
      "132/132 [==============================] - 0s 151us/sample - loss: 0.6307 - accuracy: 0.9091\n",
      "Epoch 71/100\n",
      "132/132 [==============================] - 0s 150us/sample - loss: 0.6252 - accuracy: 0.9015\n",
      "Epoch 72/100\n",
      "132/132 [==============================] - 0s 205us/sample - loss: 0.6193 - accuracy: 0.9015\n",
      "Epoch 73/100\n",
      "132/132 [==============================] - 0s 161us/sample - loss: 0.6148 - accuracy: 0.8939\n",
      "Epoch 74/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 0.6107 - accuracy: 0.8712\n",
      "Epoch 75/100\n",
      "132/132 [==============================] - 0s 144us/sample - loss: 0.6059 - accuracy: 0.8712\n",
      "Epoch 76/100\n",
      "132/132 [==============================] - 0s 146us/sample - loss: 0.6018 - accuracy: 0.8788\n",
      "Epoch 77/100\n",
      "132/132 [==============================] - 0s 169us/sample - loss: 0.5959 - accuracy: 0.8864\n",
      "Epoch 78/100\n",
      "132/132 [==============================] - 0s 178us/sample - loss: 0.5906 - accuracy: 0.9015\n",
      "Epoch 79/100\n",
      "132/132 [==============================] - 0s 131us/sample - loss: 0.5854 - accuracy: 0.9091\n",
      "Epoch 80/100\n",
      "132/132 [==============================] - 0s 119us/sample - loss: 0.5805 - accuracy: 0.9091\n",
      "Epoch 81/100\n",
      "132/132 [==============================] - 0s 190us/sample - loss: 0.5753 - accuracy: 0.9091\n",
      "Epoch 82/100\n",
      "132/132 [==============================] - 0s 217us/sample - loss: 0.5711 - accuracy: 0.9091\n",
      "Epoch 83/100\n",
      "132/132 [==============================] - 0s 141us/sample - loss: 0.5667 - accuracy: 0.9091\n",
      "Epoch 84/100\n",
      "132/132 [==============================] - 0s 183us/sample - loss: 0.5619 - accuracy: 0.9091\n",
      "Epoch 85/100\n",
      "132/132 [==============================] - 0s 150us/sample - loss: 0.5582 - accuracy: 0.9091\n",
      "Epoch 86/100\n",
      "132/132 [==============================] - 0s 154us/sample - loss: 0.5529 - accuracy: 0.9167\n",
      "Epoch 87/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 0.5491 - accuracy: 0.9242\n",
      "Epoch 88/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 0.5458 - accuracy: 0.9318\n",
      "Epoch 89/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 0.5420 - accuracy: 0.9318\n",
      "Epoch 90/100\n",
      "132/132 [==============================] - 0s 224us/sample - loss: 0.5376 - accuracy: 0.9394\n",
      "Epoch 91/100\n",
      "132/132 [==============================] - 0s 149us/sample - loss: 0.5338 - accuracy: 0.9394\n",
      "Epoch 92/100\n",
      "132/132 [==============================] - 0s 182us/sample - loss: 0.5299 - accuracy: 0.9394\n",
      "Epoch 93/100\n",
      "132/132 [==============================] - 0s 163us/sample - loss: 0.5257 - accuracy: 0.9394\n",
      "Epoch 94/100\n",
      "132/132 [==============================] - 0s 137us/sample - loss: 0.5216 - accuracy: 0.9394\n",
      "Epoch 95/100\n",
      "132/132 [==============================] - 0s 144us/sample - loss: 0.5178 - accuracy: 0.9394\n",
      "Epoch 96/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 0.5146 - accuracy: 0.9394\n",
      "Epoch 97/100\n",
      "132/132 [==============================] - 0s 151us/sample - loss: 0.5110 - accuracy: 0.9394\n",
      "Epoch 98/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 0.5072 - accuracy: 0.9394\n",
      "Epoch 99/100\n",
      "132/132 [==============================] - 0s 122us/sample - loss: 0.5035 - accuracy: 0.9394\n",
      "Epoch 100/100\n",
      "132/132 [==============================] - 0s 165us/sample - loss: 0.5001 - accuracy: 0.9394\n",
      "56/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 1ms/sample - loss: 0.7253 - accuracy: 0.8393\n",
      "tf.Tensor(\n",
      "[[ 6  0  0  1  0]\n",
      " [ 0  7  0  1  0]\n",
      " [ 1  0 15  2  0]\n",
      " [ 1  0  2 13  0]\n",
      " [ 0  0  0  1  6]], shape=(5, 5), dtype=int32)\n",
      "----------------------------\n",
      "Incorrect Predictions\n",
      "i am upset\t 3.0 2\n",
      "congratulation for having a baby\t 2.0 0\n",
      "you brighten my day\t 2.0 3\n",
      "she is a bully\t 3.0 0\n",
      "i am upset\t 3.0 2\n",
      "will you be my valentine\t 2.0 3\n",
      "i will  run 1.0 3\n",
      "family is all i have\t 0.0 3\n",
      "i did not have breakfast  4.0 3\n",
      "-------------------------- \n",
      "Correct Predictions\n",
      "i want to eat\t 4.0 4\n",
      "he did not answer\t 3.0 3\n",
      "he got a very nice raise\t 2.0 2\n",
      "she got me a nice present\t 2.0 2\n",
      "ha ha ha it was so funny\t 2.0 2\n",
      "he is a good friend\t 2.0 2\n",
      "we had such a lovely dinner tonight\t 2.0 2\n",
      "where is the food\t 4.0 4\n",
      "stop making this joke ha ha ha\t 2.0 2\n",
      "where is the ball\t 1.0 1\n",
      "work is hard\t 3.0 3\n",
      "this girl is messing with me\t 3.0 3\n",
      "are you serious 3.0 3\n",
      "let us go play baseball\t 1.0 1\n",
      "this stupid grader is not working \t 3.0 3\n",
      "work is horrible\t 3.0 3\n",
      "stop pissing me off 3.0 3\n",
      "any suggestions for dinner\t 4.0 4\n",
      "i love taking breaks\t 0.0 0\n",
      "i boiled rice\t 4.0 4\n",
      "why are you feeling bad\t 3.0 3\n",
      "give me the ball 1.0 1\n",
      "my grandmother is the love of my life\t 0.0 0\n",
      "enjoy your game 1.0 1\n",
      "valentine day is near\t 2.0 2\n",
      "i miss you so much\t 0.0 0\n",
      "throw the ball\t 1.0 1\n",
      "my life is so boring\t 3.0 3\n",
      "she said yes\t 2.0 2\n",
      "he can pitch really well\t 1.0 1\n",
      "dance with me\t 2.0 2\n",
      "i am hungry 4.0 4\n",
      "see you at the restaurant\t 4.0 4\n",
      "i like to laugh\t 2.0 2\n",
      "i like your jacket \t 0.0 0\n",
      "i miss her\t 0.0 0\n",
      "what is your favorite baseball game\t 1.0 1\n",
      "good job\t 2.0 2\n",
      "i love you to the stars and back\t 0.0 0\n",
      "what you did was awesome\t 2.0 2\n",
      "ha ha ha lol\t 2.0 2\n",
      "i do not want to joke\t 3.0 3\n",
      "go away\t 3.0 3\n",
      "yesterday we lost again\t 3.0 3\n",
      "you are failing this exercise\t 3.0 3\n",
      "good joke\t 2.0 2\n",
      "you deserve this nice prize\t 2.0 2\n"
     ]
    }
   ],
   "source": [
    "# Glove vector of 300 Dimensions on Wikipedia using Avg\n",
    "X_train, X_test, word_to_vec_map = read_files_and_retrieve_wordtovec_map('glove.6B.300d.txt')\n",
    "X_train_lower, X_test_lower, X_train_Y, X_test_Y = process_training_and_test_values(X_train, X_test)\n",
    "tokenized_training_data = tokenize_values(X_train_lower)\n",
    "tokenized_test_labels = tokenize_values(X_test_lower)\n",
    "\n",
    "avg = sentence_to_avg(tokenized_training_data, word_to_vec_map, 300) \n",
    "test_avg = sentence_to_avg(tokenized_test_labels, word_to_vec_map, 300)\n",
    "\n",
    "\n",
    "training_dataset = convert_to_nparray(avg, 'float32')\n",
    "testing_dataset = convert_to_nparray(test_avg, 'float32')\n",
    "\n",
    "training_label_dataset = convert_to_nparray(X_train_Y, 'float32')\n",
    "test_label_dataset = convert_to_nparray(X_test_Y, 'float32')\n",
    "\n",
    "model = Model_to_Fit(512, 5)\n",
    "model.fit(x = training_dataset, y = training_label_dataset, epochs = 100)\n",
    "\n",
    "predictions = model.predict(testing_dataset)\n",
    "pred_val = np.amax(convert_to_nparray(predictions, 'float32'), axis = 1)\n",
    "model.get_weights()\n",
    "model.evaluate(testing_dataset, test_label_dataset)\n",
    "\n",
    "#tf.math.confusion_matrix(test_label_dataset, pred_val, num_classes = 5)\n",
    "pred_val = CreateConfusionMatrixandReturnPredictions(test_label_dataset, convert_to_nparray(predictions, 'float32'))\n",
    "ShowCorrectandIncorrectPredictions(test_label_dataset, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MU2ty9X0WeJs",
    "outputId": "d3cd83d5-3750-4d1f-a859-5d1505bf8914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 132 samples\n",
      "Epoch 1/100\n",
      "132/132 [==============================] - 0s 2ms/sample - loss: 1.8604 - accuracy: 0.1818\n",
      "Epoch 2/100\n",
      "132/132 [==============================] - 0s 116us/sample - loss: 1.8098 - accuracy: 0.1970\n",
      "Epoch 3/100\n",
      "132/132 [==============================] - 0s 129us/sample - loss: 1.7695 - accuracy: 0.1970\n",
      "Epoch 4/100\n",
      "132/132 [==============================] - 0s 140us/sample - loss: 1.7337 - accuracy: 0.1894\n",
      "Epoch 5/100\n",
      "132/132 [==============================] - 0s 133us/sample - loss: 1.7005 - accuracy: 0.1970\n",
      "Epoch 6/100\n",
      "132/132 [==============================] - 0s 108us/sample - loss: 1.6735 - accuracy: 0.2121\n",
      "Epoch 7/100\n",
      "132/132 [==============================] - 0s 116us/sample - loss: 1.6491 - accuracy: 0.2273\n",
      "Epoch 8/100\n",
      "132/132 [==============================] - 0s 106us/sample - loss: 1.6297 - accuracy: 0.2348\n",
      "Epoch 9/100\n",
      "132/132 [==============================] - 0s 120us/sample - loss: 1.6090 - accuracy: 0.2348\n",
      "Epoch 10/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 1.5919 - accuracy: 0.2348\n",
      "Epoch 11/100\n",
      "132/132 [==============================] - 0s 186us/sample - loss: 1.5741 - accuracy: 0.2348\n",
      "Epoch 12/100\n",
      "132/132 [==============================] - 0s 173us/sample - loss: 1.5595 - accuracy: 0.2500\n",
      "Epoch 13/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 1.5479 - accuracy: 0.2803\n",
      "Epoch 14/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 1.5338 - accuracy: 0.2955\n",
      "Epoch 15/100\n",
      "132/132 [==============================] - 0s 135us/sample - loss: 1.5227 - accuracy: 0.3258\n",
      "Epoch 16/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 1.5134 - accuracy: 0.3485\n",
      "Epoch 17/100\n",
      "132/132 [==============================] - 0s 174us/sample - loss: 1.5045 - accuracy: 0.3333\n",
      "Epoch 18/100\n",
      "132/132 [==============================] - 0s 157us/sample - loss: 1.4955 - accuracy: 0.3409\n",
      "Epoch 19/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 1.4861 - accuracy: 0.3409\n",
      "Epoch 20/100\n",
      "132/132 [==============================] - 0s 128us/sample - loss: 1.4774 - accuracy: 0.3561\n",
      "Epoch 21/100\n",
      "132/132 [==============================] - 0s 140us/sample - loss: 1.4683 - accuracy: 0.3636\n",
      "Epoch 22/100\n",
      "132/132 [==============================] - 0s 167us/sample - loss: 1.4596 - accuracy: 0.3712\n",
      "Epoch 23/100\n",
      "132/132 [==============================] - 0s 146us/sample - loss: 1.4506 - accuracy: 0.3712\n",
      "Epoch 24/100\n",
      "132/132 [==============================] - 0s 160us/sample - loss: 1.4424 - accuracy: 0.3712\n",
      "Epoch 25/100\n",
      "132/132 [==============================] - 0s 172us/sample - loss: 1.4350 - accuracy: 0.3788\n",
      "Epoch 26/100\n",
      "132/132 [==============================] - 0s 160us/sample - loss: 1.4280 - accuracy: 0.3712\n",
      "Epoch 27/100\n",
      "132/132 [==============================] - 0s 159us/sample - loss: 1.4210 - accuracy: 0.3712\n",
      "Epoch 28/100\n",
      "132/132 [==============================] - 0s 127us/sample - loss: 1.4138 - accuracy: 0.3712\n",
      "Epoch 29/100\n",
      "132/132 [==============================] - 0s 126us/sample - loss: 1.4070 - accuracy: 0.3788\n",
      "Epoch 30/100\n",
      "132/132 [==============================] - 0s 150us/sample - loss: 1.4019 - accuracy: 0.3788\n",
      "Epoch 31/100\n",
      "132/132 [==============================] - 0s 171us/sample - loss: 1.3954 - accuracy: 0.3864\n",
      "Epoch 32/100\n",
      "132/132 [==============================] - 0s 154us/sample - loss: 1.3897 - accuracy: 0.3864\n",
      "Epoch 33/100\n",
      "132/132 [==============================] - 0s 171us/sample - loss: 1.3834 - accuracy: 0.3939\n",
      "Epoch 34/100\n",
      "132/132 [==============================] - 0s 193us/sample - loss: 1.3780 - accuracy: 0.4015\n",
      "Epoch 35/100\n",
      "132/132 [==============================] - 0s 146us/sample - loss: 1.3716 - accuracy: 0.4015\n",
      "Epoch 36/100\n",
      "132/132 [==============================] - 0s 141us/sample - loss: 1.3646 - accuracy: 0.4015\n",
      "Epoch 37/100\n",
      "132/132 [==============================] - 0s 153us/sample - loss: 1.3588 - accuracy: 0.4091\n",
      "Epoch 38/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 1.3523 - accuracy: 0.4091\n",
      "Epoch 39/100\n",
      "132/132 [==============================] - 0s 122us/sample - loss: 1.3457 - accuracy: 0.4242\n",
      "Epoch 40/100\n",
      "132/132 [==============================] - 0s 146us/sample - loss: 1.3391 - accuracy: 0.4318\n",
      "Epoch 41/100\n",
      "132/132 [==============================] - 0s 140us/sample - loss: 1.3325 - accuracy: 0.4470\n",
      "Epoch 42/100\n",
      "132/132 [==============================] - 0s 158us/sample - loss: 1.3261 - accuracy: 0.4545\n",
      "Epoch 43/100\n",
      "132/132 [==============================] - 0s 132us/sample - loss: 1.3209 - accuracy: 0.4545\n",
      "Epoch 44/100\n",
      "132/132 [==============================] - 0s 108us/sample - loss: 1.3153 - accuracy: 0.4621\n",
      "Epoch 45/100\n",
      "132/132 [==============================] - 0s 93us/sample - loss: 1.3084 - accuracy: 0.4773\n",
      "Epoch 46/100\n",
      "132/132 [==============================] - 0s 118us/sample - loss: 1.3027 - accuracy: 0.4773\n",
      "Epoch 47/100\n",
      "132/132 [==============================] - 0s 120us/sample - loss: 1.2960 - accuracy: 0.4924\n",
      "Epoch 48/100\n",
      "132/132 [==============================] - 0s 109us/sample - loss: 1.2898 - accuracy: 0.5152\n",
      "Epoch 49/100\n",
      "132/132 [==============================] - 0s 105us/sample - loss: 1.2832 - accuracy: 0.5303\n",
      "Epoch 50/100\n",
      "132/132 [==============================] - 0s 102us/sample - loss: 1.2770 - accuracy: 0.5227\n",
      "Epoch 51/100\n",
      "132/132 [==============================] - 0s 131us/sample - loss: 1.2715 - accuracy: 0.5227\n",
      "Epoch 52/100\n",
      "132/132 [==============================] - 0s 134us/sample - loss: 1.2659 - accuracy: 0.5303\n",
      "Epoch 53/100\n",
      "132/132 [==============================] - 0s 166us/sample - loss: 1.2605 - accuracy: 0.5379\n",
      "Epoch 54/100\n",
      "132/132 [==============================] - 0s 121us/sample - loss: 1.2540 - accuracy: 0.5455\n",
      "Epoch 55/100\n",
      "132/132 [==============================] - 0s 151us/sample - loss: 1.2483 - accuracy: 0.5530\n",
      "Epoch 56/100\n",
      "132/132 [==============================] - 0s 112us/sample - loss: 1.2431 - accuracy: 0.5530\n",
      "Epoch 57/100\n",
      "132/132 [==============================] - 0s 107us/sample - loss: 1.2373 - accuracy: 0.5530\n",
      "Epoch 58/100\n",
      "132/132 [==============================] - 0s 128us/sample - loss: 1.2322 - accuracy: 0.5606\n",
      "Epoch 59/100\n",
      "132/132 [==============================] - 0s 118us/sample - loss: 1.2264 - accuracy: 0.5682\n",
      "Epoch 60/100\n",
      "132/132 [==============================] - 0s 124us/sample - loss: 1.2213 - accuracy: 0.5758\n",
      "Epoch 61/100\n",
      "132/132 [==============================] - 0s 115us/sample - loss: 1.2162 - accuracy: 0.5833\n",
      "Epoch 62/100\n",
      "132/132 [==============================] - 0s 110us/sample - loss: 1.2112 - accuracy: 0.5985\n",
      "Epoch 63/100\n",
      "132/132 [==============================] - 0s 116us/sample - loss: 1.2056 - accuracy: 0.5985\n",
      "Epoch 64/100\n",
      "132/132 [==============================] - 0s 133us/sample - loss: 1.2000 - accuracy: 0.5985\n",
      "Epoch 65/100\n",
      "132/132 [==============================] - 0s 137us/sample - loss: 1.1945 - accuracy: 0.6061\n",
      "Epoch 66/100\n",
      "132/132 [==============================] - 0s 133us/sample - loss: 1.1897 - accuracy: 0.6061\n",
      "Epoch 67/100\n",
      "132/132 [==============================] - 0s 257us/sample - loss: 1.1843 - accuracy: 0.6136\n",
      "Epoch 68/100\n",
      "132/132 [==============================] - 0s 183us/sample - loss: 1.1795 - accuracy: 0.6212\n",
      "Epoch 69/100\n",
      "132/132 [==============================] - 0s 147us/sample - loss: 1.1745 - accuracy: 0.6212\n",
      "Epoch 70/100\n",
      "132/132 [==============================] - 0s 162us/sample - loss: 1.1701 - accuracy: 0.6288\n",
      "Epoch 71/100\n",
      "132/132 [==============================] - 0s 140us/sample - loss: 1.1652 - accuracy: 0.6288\n",
      "Epoch 72/100\n",
      "132/132 [==============================] - 0s 134us/sample - loss: 1.1612 - accuracy: 0.6288\n",
      "Epoch 73/100\n",
      "132/132 [==============================] - 0s 121us/sample - loss: 1.1568 - accuracy: 0.6288\n",
      "Epoch 74/100\n",
      "132/132 [==============================] - 0s 114us/sample - loss: 1.1515 - accuracy: 0.6288\n",
      "Epoch 75/100\n",
      "132/132 [==============================] - 0s 112us/sample - loss: 1.1472 - accuracy: 0.6288\n",
      "Epoch 76/100\n",
      "132/132 [==============================] - 0s 112us/sample - loss: 1.1424 - accuracy: 0.6288\n",
      "Epoch 77/100\n",
      "132/132 [==============================] - 0s 104us/sample - loss: 1.1390 - accuracy: 0.6288\n",
      "Epoch 78/100\n",
      "132/132 [==============================] - 0s 97us/sample - loss: 1.1341 - accuracy: 0.6288\n",
      "Epoch 79/100\n",
      "132/132 [==============================] - 0s 95us/sample - loss: 1.1289 - accuracy: 0.6288\n",
      "Epoch 80/100\n",
      "132/132 [==============================] - 0s 99us/sample - loss: 1.1248 - accuracy: 0.6288\n",
      "Epoch 81/100\n",
      "132/132 [==============================] - 0s 143us/sample - loss: 1.1191 - accuracy: 0.6364\n",
      "Epoch 82/100\n",
      "132/132 [==============================] - 0s 150us/sample - loss: 1.1145 - accuracy: 0.6364\n",
      "Epoch 83/100\n",
      "132/132 [==============================] - 0s 102us/sample - loss: 1.1102 - accuracy: 0.6364\n",
      "Epoch 84/100\n",
      "132/132 [==============================] - 0s 166us/sample - loss: 1.1063 - accuracy: 0.6515\n",
      "Epoch 85/100\n",
      "132/132 [==============================] - 0s 124us/sample - loss: 1.1017 - accuracy: 0.6515\n",
      "Epoch 86/100\n",
      "132/132 [==============================] - 0s 128us/sample - loss: 1.0977 - accuracy: 0.6515\n",
      "Epoch 87/100\n",
      "132/132 [==============================] - 0s 99us/sample - loss: 1.0935 - accuracy: 0.6515\n",
      "Epoch 88/100\n",
      "132/132 [==============================] - 0s 109us/sample - loss: 1.0899 - accuracy: 0.6591\n",
      "Epoch 89/100\n",
      "132/132 [==============================] - 0s 109us/sample - loss: 1.0859 - accuracy: 0.6591\n",
      "Epoch 90/100\n",
      "132/132 [==============================] - 0s 120us/sample - loss: 1.0821 - accuracy: 0.6515\n",
      "Epoch 91/100\n",
      "132/132 [==============================] - 0s 128us/sample - loss: 1.0780 - accuracy: 0.6515\n",
      "Epoch 92/100\n",
      "132/132 [==============================] - 0s 102us/sample - loss: 1.0745 - accuracy: 0.6515\n",
      "Epoch 93/100\n",
      "132/132 [==============================] - 0s 127us/sample - loss: 1.0700 - accuracy: 0.6591\n",
      "Epoch 94/100\n",
      "132/132 [==============================] - 0s 159us/sample - loss: 1.0663 - accuracy: 0.6515\n",
      "Epoch 95/100\n",
      "132/132 [==============================] - 0s 124us/sample - loss: 1.0630 - accuracy: 0.6591\n",
      "Epoch 96/100\n",
      "132/132 [==============================] - 0s 103us/sample - loss: 1.0592 - accuracy: 0.6515\n",
      "Epoch 97/100\n",
      "132/132 [==============================] - 0s 97us/sample - loss: 1.0554 - accuracy: 0.6515\n",
      "Epoch 98/100\n",
      "132/132 [==============================] - 0s 134us/sample - loss: 1.0524 - accuracy: 0.6591\n",
      "Epoch 99/100\n",
      "132/132 [==============================] - 0s 143us/sample - loss: 1.0487 - accuracy: 0.6591\n",
      "Epoch 100/100\n",
      "132/132 [==============================] - 0s 115us/sample - loss: 1.0449 - accuracy: 0.6591\n",
      "56/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 1ms/sample - loss: 1.1060 - accuracy: 0.5714\n",
      "tf.Tensor(\n",
      "[[ 3  0  3  1  0]\n",
      " [ 0  4  1  3  0]\n",
      " [ 1  0 12  4  1]\n",
      " [ 2  0  3 11  0]\n",
      " [ 0  0  2  3  2]], shape=(5, 5), dtype=int32)\n",
      "----------------------------\n",
      "Incorrect Predictions\n",
      "i want to eat\t 4.0 3\n",
      "work is hard\t 3.0 2\n",
      "this girl is messing with me\t 3.0 0\n",
      "work is horrible\t 3.0 2\n",
      "congratulation for having a baby\t 2.0 0\n",
      "any suggestions for dinner\t 4.0 2\n",
      "i love taking breaks\t 0.0 2\n",
      "you brighten my day\t 2.0 3\n",
      "she is a bully\t 3.0 0\n",
      "give me the ball 1.0 3\n",
      "enjoy your game 1.0 2\n",
      "i miss you so much\t 0.0 3\n",
      "my life is so boring\t 3.0 2\n",
      "she said yes\t 2.0 3\n",
      "will you be my valentine\t 2.0 3\n",
      "he can pitch really well\t 1.0 3\n",
      "i am hungry 4.0 3\n",
      "see you at the restaurant\t 4.0 2\n",
      "i will  run 1.0 3\n",
      "i miss her\t 0.0 2\n",
      "i love you to the stars and back\t 0.0 2\n",
      "what you did was awesome\t 2.0 3\n",
      "ha ha ha lol\t 2.0 4\n",
      "i did not have breakfast  4.0 3\n",
      "-------------------------- \n",
      "Correct Predictions\n",
      "he did not answer\t 3.0 3\n",
      "he got a very nice raise\t 2.0 2\n",
      "she got me a nice present\t 2.0 2\n",
      "ha ha ha it was so funny\t 2.0 2\n",
      "he is a good friend\t 2.0 2\n",
      "i am upset\t 3.0 3\n",
      "we had such a lovely dinner tonight\t 2.0 2\n",
      "where is the food\t 4.0 4\n",
      "stop making this joke ha ha ha\t 2.0 2\n",
      "where is the ball\t 1.0 1\n",
      "are you serious 3.0 3\n",
      "let us go play baseball\t 1.0 1\n",
      "this stupid grader is not working \t 3.0 3\n",
      "stop pissing me off 3.0 3\n",
      "i boiled rice\t 4.0 4\n",
      "why are you feeling bad\t 3.0 3\n",
      "i am upset\t 3.0 3\n",
      "my grandmother is the love of my life\t 0.0 0\n",
      "valentine day is near\t 2.0 2\n",
      "throw the ball\t 1.0 1\n",
      "dance with me\t 2.0 2\n",
      "i like to laugh\t 2.0 2\n",
      "i like your jacket \t 0.0 0\n",
      "what is your favorite baseball game\t 1.0 1\n",
      "good job\t 2.0 2\n",
      "i do not want to joke\t 3.0 3\n",
      "go away\t 3.0 3\n",
      "yesterday we lost again\t 3.0 3\n",
      "family is all i have\t 0.0 0\n",
      "you are failing this exercise\t 3.0 3\n",
      "good joke\t 2.0 2\n",
      "you deserve this nice prize\t 2.0 2\n"
     ]
    }
   ],
   "source": [
    "# Glove vector of 50 Dimensions on Wikipedia using avg\n",
    "X_train, X_test, word_to_vec_map = read_files_and_retrieve_wordtovec_map('glove.6B.50d.txt')\n",
    "X_train_lower, X_test_lower, X_train_Y, X_test_Y = process_training_and_test_values(X_train, X_test)\n",
    "tokenized_training_data = tokenize_values(X_train_lower)\n",
    "tokenized_test_labels = tokenize_values(X_test_lower)\n",
    "\n",
    "avg = sentence_to_avg(tokenized_training_data, word_to_vec_map, 50) \n",
    "test_avg = sentence_to_avg(tokenized_test_labels, word_to_vec_map, 50)\n",
    "\n",
    "\n",
    "training_dataset = convert_to_nparray(avg, 'float32')\n",
    "testing_dataset = convert_to_nparray(test_avg, 'float32')\n",
    "\n",
    "training_label_dataset = convert_to_nparray(X_train_Y, 'float32')\n",
    "test_label_dataset = convert_to_nparray(X_test_Y, 'float32')\n",
    "\n",
    "model = Model_to_Fit(512, 5)\n",
    "model.fit(x = training_dataset, y = training_label_dataset, epochs = 100)\n",
    "\n",
    "predictions = model.predict(testing_dataset)\n",
    "pred_val = convert_to_nparray(predictions, 'float32')\n",
    "#model.get_weights()\n",
    "model.evaluate(testing_dataset, test_label_dataset)\n",
    "\n",
    "#tf.math.confusion_matrix(test_label_dataset, pred_val, num_classes = 5)\n",
    "pred_val = CreateConfusionMatrixandReturnPredictions(test_label_dataset, convert_to_nparray(predictions, 'float32'))\n",
    "ShowCorrectandIncorrectPredictions(test_label_dataset, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9aiB3aCk7jlL",
    "outputId": "848906c2-01ad-43b7-d5fa-9166ca38ca9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 132 samples\n",
      "Epoch 1/100\n",
      "132/132 [==============================] - 0s 2ms/sample - loss: 1.5761 - acc: 0.2727\n",
      "Epoch 2/100\n",
      "132/132 [==============================] - 0s 153us/sample - loss: 1.3252 - acc: 0.4015\n",
      "Epoch 3/100\n",
      "132/132 [==============================] - 0s 136us/sample - loss: 1.1413 - acc: 0.6515\n",
      "Epoch 4/100\n",
      "132/132 [==============================] - 0s 136us/sample - loss: 1.0171 - acc: 0.7652\n",
      "Epoch 5/100\n",
      "132/132 [==============================] - 0s 153us/sample - loss: 0.9345 - acc: 0.7121\n",
      "Epoch 6/100\n",
      "132/132 [==============================] - 0s 150us/sample - loss: 0.8599 - acc: 0.7652\n",
      "Epoch 7/100\n",
      "132/132 [==============================] - 0s 156us/sample - loss: 0.7894 - acc: 0.8030\n",
      "Epoch 8/100\n",
      "132/132 [==============================] - 0s 161us/sample - loss: 0.7696 - acc: 0.7348\n",
      "Epoch 9/100\n",
      "132/132 [==============================] - 0s 147us/sample - loss: 0.6960 - acc: 0.7879\n",
      "Epoch 10/100\n",
      "132/132 [==============================] - 0s 157us/sample - loss: 0.6451 - acc: 0.8106\n",
      "Epoch 11/100\n",
      "132/132 [==============================] - 0s 148us/sample - loss: 0.6091 - acc: 0.8636\n",
      "Epoch 12/100\n",
      "132/132 [==============================] - 0s 156us/sample - loss: 0.5672 - acc: 0.8636\n",
      "Epoch 13/100\n",
      "132/132 [==============================] - 0s 157us/sample - loss: 0.5444 - acc: 0.8712\n",
      "Epoch 14/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 0.5220 - acc: 0.8864\n",
      "Epoch 15/100\n",
      "132/132 [==============================] - 0s 152us/sample - loss: 0.4889 - acc: 0.8864\n",
      "Epoch 16/100\n",
      "132/132 [==============================] - 0s 143us/sample - loss: 0.4779 - acc: 0.8561\n",
      "Epoch 17/100\n",
      "132/132 [==============================] - 0s 163us/sample - loss: 0.4497 - acc: 0.8939\n",
      "Epoch 18/100\n",
      "132/132 [==============================] - 0s 178us/sample - loss: 0.4357 - acc: 0.8939\n",
      "Epoch 19/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 0.4238 - acc: 0.8939\n",
      "Epoch 20/100\n",
      "132/132 [==============================] - 0s 161us/sample - loss: 0.3814 - acc: 0.9318\n",
      "Epoch 21/100\n",
      "132/132 [==============================] - 0s 157us/sample - loss: 0.3866 - acc: 0.9167\n",
      "Epoch 22/100\n",
      "132/132 [==============================] - 0s 148us/sample - loss: 0.3637 - acc: 0.9242\n",
      "Epoch 23/100\n",
      "132/132 [==============================] - 0s 167us/sample - loss: 0.3360 - acc: 0.9470\n",
      "Epoch 24/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 0.3232 - acc: 0.9394\n",
      "Epoch 25/100\n",
      "132/132 [==============================] - 0s 151us/sample - loss: 0.3268 - acc: 0.9091\n",
      "Epoch 26/100\n",
      "132/132 [==============================] - 0s 148us/sample - loss: 0.3065 - acc: 0.9167\n",
      "Epoch 27/100\n",
      "132/132 [==============================] - 0s 179us/sample - loss: 0.3055 - acc: 0.9167\n",
      "Epoch 28/100\n",
      "132/132 [==============================] - 0s 168us/sample - loss: 0.2816 - acc: 0.9318\n",
      "Epoch 29/100\n",
      "132/132 [==============================] - 0s 165us/sample - loss: 0.2945 - acc: 0.9242\n",
      "Epoch 30/100\n",
      "132/132 [==============================] - 0s 149us/sample - loss: 0.2944 - acc: 0.9242\n",
      "Epoch 31/100\n",
      "132/132 [==============================] - 0s 147us/sample - loss: 0.2847 - acc: 0.9167\n",
      "Epoch 32/100\n",
      "132/132 [==============================] - 0s 161us/sample - loss: 0.2559 - acc: 0.9318\n",
      "Epoch 33/100\n",
      "132/132 [==============================] - 0s 154us/sample - loss: 0.2567 - acc: 0.9318\n",
      "Epoch 34/100\n",
      "132/132 [==============================] - 0s 157us/sample - loss: 0.2518 - acc: 0.9470\n",
      "Epoch 35/100\n",
      "132/132 [==============================] - 0s 148us/sample - loss: 0.2211 - acc: 0.9394\n",
      "Epoch 36/100\n",
      "132/132 [==============================] - 0s 162us/sample - loss: 0.2132 - acc: 0.9470\n",
      "Epoch 37/100\n",
      "132/132 [==============================] - 0s 177us/sample - loss: 0.2006 - acc: 0.9621\n",
      "Epoch 38/100\n",
      "132/132 [==============================] - 0s 159us/sample - loss: 0.2069 - acc: 0.9621\n",
      "Epoch 39/100\n",
      "132/132 [==============================] - 0s 157us/sample - loss: 0.1946 - acc: 0.9621\n",
      "Epoch 40/100\n",
      "132/132 [==============================] - 0s 189us/sample - loss: 0.1869 - acc: 0.9621\n",
      "Epoch 41/100\n",
      "132/132 [==============================] - 0s 164us/sample - loss: 0.1852 - acc: 0.9697\n",
      "Epoch 42/100\n",
      "132/132 [==============================] - 0s 153us/sample - loss: 0.1840 - acc: 0.9848\n",
      "Epoch 43/100\n",
      "132/132 [==============================] - 0s 162us/sample - loss: 0.1729 - acc: 0.9697\n",
      "Epoch 44/100\n",
      "132/132 [==============================] - 0s 157us/sample - loss: 0.1745 - acc: 0.9621\n",
      "Epoch 45/100\n",
      "132/132 [==============================] - 0s 140us/sample - loss: 0.1636 - acc: 0.9621\n",
      "Epoch 46/100\n",
      "132/132 [==============================] - 0s 162us/sample - loss: 0.1719 - acc: 0.9848\n",
      "Epoch 47/100\n",
      "132/132 [==============================] - 0s 177us/sample - loss: 0.1542 - acc: 0.9773\n",
      "Epoch 48/100\n",
      "132/132 [==============================] - 0s 176us/sample - loss: 0.1651 - acc: 0.9621\n",
      "Epoch 49/100\n",
      "132/132 [==============================] - 0s 170us/sample - loss: 0.1523 - acc: 0.9773\n",
      "Epoch 50/100\n",
      "132/132 [==============================] - 0s 175us/sample - loss: 0.1439 - acc: 0.9848\n",
      "Epoch 51/100\n",
      "132/132 [==============================] - 0s 158us/sample - loss: 0.1377 - acc: 0.9773\n",
      "Epoch 52/100\n",
      "132/132 [==============================] - 0s 148us/sample - loss: 0.1361 - acc: 0.9773\n",
      "Epoch 53/100\n",
      "132/132 [==============================] - 0s 162us/sample - loss: 0.1292 - acc: 0.9773\n",
      "Epoch 54/100\n",
      "132/132 [==============================] - 0s 207us/sample - loss: 0.1383 - acc: 0.9848\n",
      "Epoch 55/100\n",
      "132/132 [==============================] - 0s 171us/sample - loss: 0.1256 - acc: 0.9848\n",
      "Epoch 56/100\n",
      "132/132 [==============================] - 0s 249us/sample - loss: 0.1382 - acc: 0.9621\n",
      "Epoch 57/100\n",
      "132/132 [==============================] - 0s 143us/sample - loss: 0.1264 - acc: 0.9773\n",
      "Epoch 58/100\n",
      "132/132 [==============================] - 0s 153us/sample - loss: 0.1188 - acc: 0.9924\n",
      "Epoch 59/100\n",
      "132/132 [==============================] - 0s 260us/sample - loss: 0.1163 - acc: 0.9848\n",
      "Epoch 60/100\n",
      "132/132 [==============================] - 0s 242us/sample - loss: 0.1140 - acc: 0.9924\n",
      "Epoch 61/100\n",
      "132/132 [==============================] - 0s 191us/sample - loss: 0.1060 - acc: 0.9924\n",
      "Epoch 62/100\n",
      "132/132 [==============================] - 0s 218us/sample - loss: 0.1107 - acc: 0.9924\n",
      "Epoch 63/100\n",
      "132/132 [==============================] - 0s 188us/sample - loss: 0.1107 - acc: 0.9924\n",
      "Epoch 64/100\n",
      "132/132 [==============================] - 0s 172us/sample - loss: 0.1032 - acc: 0.9924\n",
      "Epoch 65/100\n",
      "132/132 [==============================] - 0s 228us/sample - loss: 0.1115 - acc: 0.9773\n",
      "Epoch 66/100\n",
      "132/132 [==============================] - 0s 166us/sample - loss: 0.1028 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "132/132 [==============================] - 0s 185us/sample - loss: 0.1287 - acc: 0.9621\n",
      "Epoch 68/100\n",
      "132/132 [==============================] - 0s 172us/sample - loss: 0.1232 - acc: 0.9697\n",
      "Epoch 69/100\n",
      "132/132 [==============================] - 0s 168us/sample - loss: 0.0946 - acc: 0.9924\n",
      "Epoch 70/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 0.1106 - acc: 0.9773\n",
      "Epoch 71/100\n",
      "132/132 [==============================] - 0s 166us/sample - loss: 0.0987 - acc: 0.9848\n",
      "Epoch 72/100\n",
      "132/132 [==============================] - 0s 173us/sample - loss: 0.1039 - acc: 0.9848\n",
      "Epoch 73/100\n",
      "132/132 [==============================] - 0s 147us/sample - loss: 0.0953 - acc: 0.9848\n",
      "Epoch 74/100\n",
      "132/132 [==============================] - 0s 196us/sample - loss: 0.0828 - acc: 0.9848\n",
      "Epoch 75/100\n",
      "132/132 [==============================] - 0s 177us/sample - loss: 0.0829 - acc: 0.9924\n",
      "Epoch 76/100\n",
      "132/132 [==============================] - 0s 183us/sample - loss: 0.0852 - acc: 0.9924\n",
      "Epoch 77/100\n",
      "132/132 [==============================] - 0s 160us/sample - loss: 0.0753 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "132/132 [==============================] - 0s 166us/sample - loss: 0.0773 - acc: 0.9848\n",
      "Epoch 79/100\n",
      "132/132 [==============================] - 0s 203us/sample - loss: 0.0849 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "132/132 [==============================] - 0s 182us/sample - loss: 0.0811 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 0.0708 - acc: 0.9924\n",
      "Epoch 82/100\n",
      "132/132 [==============================] - 0s 168us/sample - loss: 0.0731 - acc: 0.9924\n",
      "Epoch 83/100\n",
      "132/132 [==============================] - 0s 229us/sample - loss: 0.0655 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "132/132 [==============================] - 0s 178us/sample - loss: 0.0633 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "132/132 [==============================] - 0s 181us/sample - loss: 0.0672 - acc: 0.9924\n",
      "Epoch 86/100\n",
      "132/132 [==============================] - 0s 157us/sample - loss: 0.0782 - acc: 0.9924\n",
      "Epoch 87/100\n",
      "132/132 [==============================] - 0s 174us/sample - loss: 0.0583 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "132/132 [==============================] - 0s 170us/sample - loss: 0.0704 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "132/132 [==============================] - 0s 163us/sample - loss: 0.0594 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "132/132 [==============================] - 0s 163us/sample - loss: 0.0594 - acc: 0.9924\n",
      "Epoch 91/100\n",
      "132/132 [==============================] - 0s 180us/sample - loss: 0.0567 - acc: 0.9924\n",
      "Epoch 92/100\n",
      "132/132 [==============================] - 0s 195us/sample - loss: 0.0528 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "132/132 [==============================] - 0s 198us/sample - loss: 0.0533 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "132/132 [==============================] - 0s 164us/sample - loss: 0.0515 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "132/132 [==============================] - 0s 191us/sample - loss: 0.0505 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "132/132 [==============================] - 0s 157us/sample - loss: 0.0489 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "132/132 [==============================] - 0s 163us/sample - loss: 0.0496 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "132/132 [==============================] - 0s 166us/sample - loss: 0.0587 - acc: 0.9924\n",
      "Epoch 99/100\n",
      "132/132 [==============================] - 0s 167us/sample - loss: 0.0585 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "132/132 [==============================] - 0s 158us/sample - loss: 0.0467 - acc: 1.0000\n",
      "56/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 1ms/sample - loss: 0.3744 - acc: 0.8036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=54999, shape=(5, 5), dtype=int32, numpy=\n",
       "array([[ 2,  5,  0,  0,  0],\n",
       "       [ 0,  8,  0,  0,  0],\n",
       "       [ 0, 18,  0,  0,  0],\n",
       "       [ 0, 16,  0,  0,  0],\n",
       "       [ 0,  7,  0,  0,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Glove vector of 100 Dimensions on Wikipedia using avg\n",
    "X_train, X_test, word_to_vec_map = read_files_and_retrieve_wordtovec_map('glove.6B.100d.txt')\n",
    "X_train_lower, X_test_lower, X_train_Y, X_test_Y = process_training_and_test_values(X_train, X_test)\n",
    "tokenized_training_data = tokenize_values(X_train_lower)\n",
    "tokenized_test_labels = tokenize_values(X_test_lower)\n",
    "\n",
    "avg = sentence_to_avg(tokenized_training_data, word_to_vec_map, 100) \n",
    "test_avg = sentence_to_avg(tokenized_test_labels, word_to_vec_map, 100)\n",
    "\n",
    "\n",
    "training_dataset = convert_to_nparray(avg, 'float32')\n",
    "testing_dataset = convert_to_nparray(test_avg, 'float32')\n",
    "\n",
    "training_label_dataset = convert_to_nparray(X_train_Y, 'float32')\n",
    "test_label_dataset = convert_to_nparray(X_test_Y, 'float32')\n",
    "\n",
    "model = Model_to_Fit(512, 5)\n",
    "model.fit(x = training_dataset, y = training_label_dataset, epochs = 100)\n",
    "\n",
    "predictions = model.predict(testing_dataset)\n",
    "pred_val = np.max(convert_to_nparray(predictions, 'float32'), axis = 1)\n",
    "\n",
    "p = np.around(pred_val)\n",
    "model.get_weights()\n",
    "model.evaluate(testing_dataset, test_label_dataset)\n",
    "\n",
    "#tf.math.confusion_matrix(test_label_dataset, p, num_classes = 5)\n",
    "pred_val = CreateConfusionMatrixandReturnPredictions(test_label_dataset, convert_to_nparray(predictions, 'float32'))\n",
    "ShowCorrectandIncorrectPredictions(test_label_dataset, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hF9JhQ3pZ5Ny",
    "outputId": "fb6359fb-fb0d-43c8-bc5d-d6986248663a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 132 samples\n",
      "Epoch 1/100\n",
      "132/132 [==============================] - 0s 2ms/sample - loss: 2.0385 - accuracy: 0.1439\n",
      "Epoch 2/100\n",
      "132/132 [==============================] - 0s 147us/sample - loss: 1.9674 - accuracy: 0.1364\n",
      "Epoch 3/100\n",
      "132/132 [==============================] - 0s 185us/sample - loss: 1.9066 - accuracy: 0.1364\n",
      "Epoch 4/100\n",
      "132/132 [==============================] - 0s 132us/sample - loss: 1.8493 - accuracy: 0.1364\n",
      "Epoch 5/100\n",
      "132/132 [==============================] - 0s 308us/sample - loss: 1.7968 - accuracy: 0.1515\n",
      "Epoch 6/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 1.7491 - accuracy: 0.1667\n",
      "Epoch 7/100\n",
      "132/132 [==============================] - 0s 147us/sample - loss: 1.7090 - accuracy: 0.1591\n",
      "Epoch 8/100\n",
      "132/132 [==============================] - 0s 168us/sample - loss: 1.6686 - accuracy: 0.1667\n",
      "Epoch 9/100\n",
      "132/132 [==============================] - 0s 178us/sample - loss: 1.6326 - accuracy: 0.1818\n",
      "Epoch 10/100\n",
      "132/132 [==============================] - 0s 160us/sample - loss: 1.6003 - accuracy: 0.1742\n",
      "Epoch 11/100\n",
      "132/132 [==============================] - 0s 189us/sample - loss: 1.5695 - accuracy: 0.1970\n",
      "Epoch 12/100\n",
      "132/132 [==============================] - 0s 143us/sample - loss: 1.5428 - accuracy: 0.2803\n",
      "Epoch 13/100\n",
      "132/132 [==============================] - 0s 171us/sample - loss: 1.5170 - accuracy: 0.2879\n",
      "Epoch 14/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 1.4945 - accuracy: 0.3258\n",
      "Epoch 15/100\n",
      "132/132 [==============================] - 0s 137us/sample - loss: 1.4718 - accuracy: 0.3561\n",
      "Epoch 16/100\n",
      "132/132 [==============================] - 0s 211us/sample - loss: 1.4510 - accuracy: 0.4091\n",
      "Epoch 17/100\n",
      "132/132 [==============================] - 0s 153us/sample - loss: 1.4322 - accuracy: 0.4394\n",
      "Epoch 18/100\n",
      "132/132 [==============================] - 0s 156us/sample - loss: 1.4137 - accuracy: 0.4697\n",
      "Epoch 19/100\n",
      "132/132 [==============================] - 0s 319us/sample - loss: 1.3962 - accuracy: 0.4773\n",
      "Epoch 20/100\n",
      "132/132 [==============================] - 0s 141us/sample - loss: 1.3798 - accuracy: 0.5227\n",
      "Epoch 21/100\n",
      "132/132 [==============================] - 0s 177us/sample - loss: 1.3649 - accuracy: 0.5303\n",
      "Epoch 22/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 1.3494 - accuracy: 0.5303\n",
      "Epoch 23/100\n",
      "132/132 [==============================] - 0s 133us/sample - loss: 1.3338 - accuracy: 0.5606\n",
      "Epoch 24/100\n",
      "132/132 [==============================] - 0s 153us/sample - loss: 1.3193 - accuracy: 0.5606\n",
      "Epoch 25/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 1.3055 - accuracy: 0.5606\n",
      "Epoch 26/100\n",
      "132/132 [==============================] - 0s 160us/sample - loss: 1.2915 - accuracy: 0.5758\n",
      "Epoch 27/100\n",
      "132/132 [==============================] - 0s 136us/sample - loss: 1.2781 - accuracy: 0.5833\n",
      "Epoch 28/100\n",
      "132/132 [==============================] - 0s 135us/sample - loss: 1.2647 - accuracy: 0.5833\n",
      "Epoch 29/100\n",
      "132/132 [==============================] - 0s 161us/sample - loss: 1.2514 - accuracy: 0.5833\n",
      "Epoch 30/100\n",
      "132/132 [==============================] - 0s 147us/sample - loss: 1.2390 - accuracy: 0.6061\n",
      "Epoch 31/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 1.2260 - accuracy: 0.6212\n",
      "Epoch 32/100\n",
      "132/132 [==============================] - 0s 133us/sample - loss: 1.2139 - accuracy: 0.6288\n",
      "Epoch 33/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 1.2016 - accuracy: 0.6288\n",
      "Epoch 34/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 1.1899 - accuracy: 0.6212\n",
      "Epoch 35/100\n",
      "132/132 [==============================] - 0s 172us/sample - loss: 1.1784 - accuracy: 0.6288\n",
      "Epoch 36/100\n",
      "132/132 [==============================] - 0s 194us/sample - loss: 1.1671 - accuracy: 0.6288\n",
      "Epoch 37/100\n",
      "132/132 [==============================] - 0s 170us/sample - loss: 1.1564 - accuracy: 0.6364\n",
      "Epoch 38/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 1.1450 - accuracy: 0.6288\n",
      "Epoch 39/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 1.1342 - accuracy: 0.6667\n",
      "Epoch 40/100\n",
      "132/132 [==============================] - 0s 149us/sample - loss: 1.1234 - accuracy: 0.6818\n",
      "Epoch 41/100\n",
      "132/132 [==============================] - 0s 156us/sample - loss: 1.1132 - accuracy: 0.6970\n",
      "Epoch 42/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 1.1024 - accuracy: 0.7273\n",
      "Epoch 43/100\n",
      "132/132 [==============================] - 0s 151us/sample - loss: 1.0921 - accuracy: 0.7500\n",
      "Epoch 44/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 1.0824 - accuracy: 0.7500\n",
      "Epoch 45/100\n",
      "132/132 [==============================] - 0s 149us/sample - loss: 1.0728 - accuracy: 0.7424\n",
      "Epoch 46/100\n",
      "132/132 [==============================] - 0s 187us/sample - loss: 1.0630 - accuracy: 0.7576\n",
      "Epoch 47/100\n",
      "132/132 [==============================] - 0s 162us/sample - loss: 1.0532 - accuracy: 0.7576\n",
      "Epoch 48/100\n",
      "132/132 [==============================] - 0s 165us/sample - loss: 1.0437 - accuracy: 0.7576\n",
      "Epoch 49/100\n",
      "132/132 [==============================] - 0s 172us/sample - loss: 1.0344 - accuracy: 0.7500\n",
      "Epoch 50/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 1.0253 - accuracy: 0.7576\n",
      "Epoch 51/100\n",
      "132/132 [==============================] - 0s 150us/sample - loss: 1.0162 - accuracy: 0.7652\n",
      "Epoch 52/100\n",
      "132/132 [==============================] - 0s 134us/sample - loss: 1.0075 - accuracy: 0.7652\n",
      "Epoch 53/100\n",
      "132/132 [==============================] - 0s 168us/sample - loss: 0.9989 - accuracy: 0.7803\n",
      "Epoch 54/100\n",
      "132/132 [==============================] - 0s 156us/sample - loss: 0.9905 - accuracy: 0.7879\n",
      "Epoch 55/100\n",
      "132/132 [==============================] - 0s 133us/sample - loss: 0.9818 - accuracy: 0.8030\n",
      "Epoch 56/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 0.9738 - accuracy: 0.8030\n",
      "Epoch 57/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 0.9657 - accuracy: 0.8030\n",
      "Epoch 58/100\n",
      "132/132 [==============================] - 0s 133us/sample - loss: 0.9580 - accuracy: 0.8030\n",
      "Epoch 59/100\n",
      "132/132 [==============================] - 0s 154us/sample - loss: 0.9501 - accuracy: 0.7879\n",
      "Epoch 60/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 0.9422 - accuracy: 0.7879\n",
      "Epoch 61/100\n",
      "132/132 [==============================] - 0s 195us/sample - loss: 0.9346 - accuracy: 0.7955\n",
      "Epoch 62/100\n",
      "132/132 [==============================] - 0s 156us/sample - loss: 0.9265 - accuracy: 0.7955\n",
      "Epoch 63/100\n",
      "132/132 [==============================] - 0s 149us/sample - loss: 0.9196 - accuracy: 0.7879\n",
      "Epoch 64/100\n",
      "132/132 [==============================] - 0s 163us/sample - loss: 0.9126 - accuracy: 0.7803\n",
      "Epoch 65/100\n",
      "132/132 [==============================] - 0s 165us/sample - loss: 0.9053 - accuracy: 0.7879\n",
      "Epoch 66/100\n",
      "132/132 [==============================] - 0s 164us/sample - loss: 0.8985 - accuracy: 0.8106\n",
      "Epoch 67/100\n",
      "132/132 [==============================] - 0s 140us/sample - loss: 0.8919 - accuracy: 0.8258\n",
      "Epoch 68/100\n",
      "132/132 [==============================] - 0s 149us/sample - loss: 0.8852 - accuracy: 0.8182\n",
      "Epoch 69/100\n",
      "132/132 [==============================] - 0s 148us/sample - loss: 0.8784 - accuracy: 0.8258\n",
      "Epoch 70/100\n",
      "132/132 [==============================] - 0s 175us/sample - loss: 0.8720 - accuracy: 0.8333\n",
      "Epoch 71/100\n",
      "132/132 [==============================] - 0s 158us/sample - loss: 0.8652 - accuracy: 0.8258\n",
      "Epoch 72/100\n",
      "132/132 [==============================] - 0s 151us/sample - loss: 0.8584 - accuracy: 0.8333\n",
      "Epoch 73/100\n",
      "132/132 [==============================] - 0s 194us/sample - loss: 0.8515 - accuracy: 0.8333\n",
      "Epoch 74/100\n",
      "132/132 [==============================] - 0s 144us/sample - loss: 0.8455 - accuracy: 0.8333\n",
      "Epoch 75/100\n",
      "132/132 [==============================] - 0s 137us/sample - loss: 0.8393 - accuracy: 0.8333\n",
      "Epoch 76/100\n",
      "132/132 [==============================] - 0s 142us/sample - loss: 0.8332 - accuracy: 0.8258\n",
      "Epoch 77/100\n",
      "132/132 [==============================] - 0s 142us/sample - loss: 0.8274 - accuracy: 0.8258\n",
      "Epoch 78/100\n",
      "132/132 [==============================] - 0s 122us/sample - loss: 0.8216 - accuracy: 0.8333\n",
      "Epoch 79/100\n",
      "132/132 [==============================] - 0s 143us/sample - loss: 0.8157 - accuracy: 0.8333\n",
      "Epoch 80/100\n",
      "132/132 [==============================] - 0s 163us/sample - loss: 0.8103 - accuracy: 0.8333\n",
      "Epoch 81/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 0.8048 - accuracy: 0.8333\n",
      "Epoch 82/100\n",
      "132/132 [==============================] - 0s 163us/sample - loss: 0.7996 - accuracy: 0.8333\n",
      "Epoch 83/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 0.7940 - accuracy: 0.8333\n",
      "Epoch 84/100\n",
      "132/132 [==============================] - 0s 116us/sample - loss: 0.7882 - accuracy: 0.8333\n",
      "Epoch 85/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 0.7825 - accuracy: 0.8333\n",
      "Epoch 86/100\n",
      "132/132 [==============================] - 0s 117us/sample - loss: 0.7776 - accuracy: 0.8333\n",
      "Epoch 87/100\n",
      "132/132 [==============================] - 0s 161us/sample - loss: 0.7722 - accuracy: 0.8333\n",
      "Epoch 88/100\n",
      "132/132 [==============================] - 0s 183us/sample - loss: 0.7678 - accuracy: 0.8333\n",
      "Epoch 89/100\n",
      "132/132 [==============================] - 0s 162us/sample - loss: 0.7621 - accuracy: 0.8409\n",
      "Epoch 90/100\n",
      "132/132 [==============================] - 0s 176us/sample - loss: 0.7573 - accuracy: 0.8409\n",
      "Epoch 91/100\n",
      "132/132 [==============================] - 0s 188us/sample - loss: 0.7523 - accuracy: 0.8409\n",
      "Epoch 92/100\n",
      "132/132 [==============================] - 0s 183us/sample - loss: 0.7476 - accuracy: 0.8409\n",
      "Epoch 93/100\n",
      "132/132 [==============================] - 0s 153us/sample - loss: 0.7433 - accuracy: 0.8485\n",
      "Epoch 94/100\n",
      "132/132 [==============================] - 0s 169us/sample - loss: 0.7385 - accuracy: 0.8485\n",
      "Epoch 95/100\n",
      "132/132 [==============================] - 0s 198us/sample - loss: 0.7337 - accuracy: 0.8409\n",
      "Epoch 96/100\n",
      "132/132 [==============================] - 0s 171us/sample - loss: 0.7293 - accuracy: 0.8409\n",
      "Epoch 97/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 0.7249 - accuracy: 0.8485\n",
      "Epoch 98/100\n",
      "132/132 [==============================] - 0s 152us/sample - loss: 0.7207 - accuracy: 0.8561\n",
      "Epoch 99/100\n",
      "132/132 [==============================] - 0s 244us/sample - loss: 0.7165 - accuracy: 0.8485\n",
      "Epoch 100/100\n",
      "132/132 [==============================] - 0s 171us/sample - loss: 0.7118 - accuracy: 0.8485\n",
      "56/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 2ms/sample - loss: 0.8883 - accuracy: 0.7679\n",
      "[0.8323908277920314, 0.76785713]\n",
      "tf.Tensor(\n",
      "[[ 4  0  2  1  0]\n",
      " [ 0  5  1  2  0]\n",
      " [ 0  0 15  3  0]\n",
      " [ 0  0  2 14  0]\n",
      " [ 0  0  1  1  5]], shape=(5, 5), dtype=int32)\n",
      "----------------------------\n",
      "Incorrect Predictions\n",
      "you brighten my day\t 2.0 3\n",
      "she is a bully\t 3.0 2\n",
      "give me the ball 1.0 3\n",
      "will you be my valentine\t 2.0 3\n",
      "he can pitch really well\t 1.0 2\n",
      "i am hungry 4.0 3\n",
      "see you at the restaurant\t 4.0 2\n",
      "i like to laugh\t 2.0 3\n",
      "i will  run 1.0 3\n",
      "i like your jacket \t 0.0 3\n",
      "i love you to the stars and back\t 0.0 2\n",
      "yesterday we lost again\t 3.0 2\n",
      "family is all i have\t 0.0 2\n",
      "-------------------------- \n",
      "Correct Predictions\n",
      "i want to eat\t 4.0 4\n",
      "he did not answer\t 3.0 3\n",
      "he got a very nice raise\t 2.0 2\n",
      "she got me a nice present\t 2.0 2\n",
      "ha ha ha it was so funny\t 2.0 2\n",
      "he is a good friend\t 2.0 2\n",
      "i am upset\t 3.0 3\n",
      "we had such a lovely dinner tonight\t 2.0 2\n",
      "where is the food\t 4.0 4\n",
      "stop making this joke ha ha ha\t 2.0 2\n",
      "where is the ball\t 1.0 1\n",
      "work is hard\t 3.0 3\n",
      "this girl is messing with me\t 3.0 3\n",
      "are you serious 3.0 3\n",
      "let us go play baseball\t 1.0 1\n",
      "this stupid grader is not working \t 3.0 3\n",
      "work is horrible\t 3.0 3\n",
      "congratulation for having a baby\t 2.0 2\n",
      "stop pissing me off 3.0 3\n",
      "any suggestions for dinner\t 4.0 4\n",
      "i love taking breaks\t 0.0 0\n",
      "i boiled rice\t 4.0 4\n",
      "why are you feeling bad\t 3.0 3\n",
      "i am upset\t 3.0 3\n",
      "my grandmother is the love of my life\t 0.0 0\n",
      "enjoy your game 1.0 1\n",
      "valentine day is near\t 2.0 2\n",
      "i miss you so much\t 0.0 0\n",
      "throw the ball\t 1.0 1\n",
      "my life is so boring\t 3.0 3\n",
      "she said yes\t 2.0 2\n",
      "dance with me\t 2.0 2\n",
      "i miss her\t 0.0 0\n",
      "what is your favorite baseball game\t 1.0 1\n",
      "good job\t 2.0 2\n",
      "what you did was awesome\t 2.0 2\n",
      "ha ha ha lol\t 2.0 2\n",
      "i do not want to joke\t 3.0 3\n",
      "go away\t 3.0 3\n",
      "you are failing this exercise\t 3.0 3\n",
      "good joke\t 2.0 2\n",
      "you deserve this nice prize\t 2.0 2\n",
      "i did not have breakfast  4.0 4\n"
     ]
    }
   ],
   "source": [
    "# Glove vector of 100 Dimensions for Twitter using Avg\n",
    "X_train, X_test, word_to_vec_map = read_files_and_retrieve_wordtovec_map('glove.twitter.27B.100d.txt')\n",
    "X_train_lower, X_test_lower, X_train_Y, X_test_Y = process_training_and_test_values(X_train, X_test)\n",
    "tokenized_training_data = tokenize_values(X_train_lower)\n",
    "tokenized_test_labels = tokenize_values(X_test_lower)\n",
    "\n",
    "avg = sentence_to_avg(tokenized_training_data, word_to_vec_map, 100) \n",
    "test_avg = sentence_to_avg(tokenized_test_labels, word_to_vec_map, 100)\n",
    "\n",
    "\n",
    "training_dataset = convert_to_nparray(avg, 'float32')\n",
    "testing_dataset = convert_to_nparray(test_avg, 'float32')\n",
    "\n",
    "training_label_dataset = convert_to_nparray(X_train_Y, 'float32')\n",
    "test_label_dataset = convert_to_nparray(X_test_Y, 'float32')\n",
    "\n",
    "model = Model_to_Fit(512, 5)\n",
    "model.fit(x = training_dataset, y = training_label_dataset, epochs = 100)\n",
    "\n",
    "predictions = model.predict(testing_dataset)\n",
    "model.get_weights()\n",
    "print(model.evaluate(testing_dataset, test_label_dataset))\n",
    "\n",
    "pred_val = CreateConfusionMatrixandReturnPredictions(test_label_dataset, convert_to_nparray(predictions, 'float32'))\n",
    "ShowCorrectandIncorrectPredictions(test_label_dataset, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "seugH3V8CtKm",
    "outputId": "aafb17f9-d806-4ad8-9888-dfcf542f3889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 132 samples\n",
      "Epoch 1/100\n",
      "132/132 [==============================] - 0s 2ms/sample - loss: 1.6287 - acc: 0.2197\n",
      "Epoch 2/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 1.5736 - acc: 0.2121\n",
      "Epoch 3/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 1.5316 - acc: 0.2955\n",
      "Epoch 4/100\n",
      "132/132 [==============================] - 0s 130us/sample - loss: 1.5004 - acc: 0.3409\n",
      "Epoch 5/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 1.4733 - acc: 0.3561\n",
      "Epoch 6/100\n",
      "132/132 [==============================] - 0s 129us/sample - loss: 1.4492 - acc: 0.3788\n",
      "Epoch 7/100\n",
      "132/132 [==============================] - 0s 127us/sample - loss: 1.4270 - acc: 0.3939\n",
      "Epoch 8/100\n",
      "132/132 [==============================] - 0s 140us/sample - loss: 1.4064 - acc: 0.4091\n",
      "Epoch 9/100\n",
      "132/132 [==============================] - 0s 131us/sample - loss: 1.3849 - acc: 0.4318\n",
      "Epoch 10/100\n",
      "132/132 [==============================] - 0s 126us/sample - loss: 1.3675 - acc: 0.4545\n",
      "Epoch 11/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 1.3491 - acc: 0.4621\n",
      "Epoch 12/100\n",
      "132/132 [==============================] - 0s 137us/sample - loss: 1.3319 - acc: 0.4773\n",
      "Epoch 13/100\n",
      "132/132 [==============================] - 0s 175us/sample - loss: 1.3144 - acc: 0.5000\n",
      "Epoch 14/100\n",
      "132/132 [==============================] - 0s 161us/sample - loss: 1.2974 - acc: 0.5076\n",
      "Epoch 15/100\n",
      "132/132 [==============================] - 0s 172us/sample - loss: 1.2805 - acc: 0.5455\n",
      "Epoch 16/100\n",
      "132/132 [==============================] - 0s 146us/sample - loss: 1.2634 - acc: 0.5455\n",
      "Epoch 17/100\n",
      "132/132 [==============================] - 0s 136us/sample - loss: 1.2470 - acc: 0.5530\n",
      "Epoch 18/100\n",
      "132/132 [==============================] - 0s 147us/sample - loss: 1.2338 - acc: 0.5758\n",
      "Epoch 19/100\n",
      "132/132 [==============================] - 0s 146us/sample - loss: 1.2184 - acc: 0.5758\n",
      "Epoch 20/100\n",
      "132/132 [==============================] - 0s 162us/sample - loss: 1.2027 - acc: 0.5833\n",
      "Epoch 21/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 1.1879 - acc: 0.6061\n",
      "Epoch 22/100\n",
      "132/132 [==============================] - 0s 163us/sample - loss: 1.1738 - acc: 0.6364\n",
      "Epoch 23/100\n",
      "132/132 [==============================] - 0s 182us/sample - loss: 1.1587 - acc: 0.6515\n",
      "Epoch 24/100\n",
      "132/132 [==============================] - 0s 157us/sample - loss: 1.1455 - acc: 0.6742\n",
      "Epoch 25/100\n",
      "132/132 [==============================] - 0s 169us/sample - loss: 1.1314 - acc: 0.6818\n",
      "Epoch 26/100\n",
      "132/132 [==============================] - 0s 120us/sample - loss: 1.1180 - acc: 0.7045\n",
      "Epoch 27/100\n",
      "132/132 [==============================] - 0s 182us/sample - loss: 1.1049 - acc: 0.7197\n",
      "Epoch 28/100\n",
      "132/132 [==============================] - 0s 165us/sample - loss: 1.0931 - acc: 0.7348\n",
      "Epoch 29/100\n",
      "132/132 [==============================] - 0s 255us/sample - loss: 1.0811 - acc: 0.7424\n",
      "Epoch 30/100\n",
      "132/132 [==============================] - 0s 171us/sample - loss: 1.0701 - acc: 0.7424\n",
      "Epoch 31/100\n",
      "132/132 [==============================] - 0s 121us/sample - loss: 1.0592 - acc: 0.7424\n",
      "Epoch 32/100\n",
      "132/132 [==============================] - 0s 166us/sample - loss: 1.0466 - acc: 0.7500\n",
      "Epoch 33/100\n",
      "132/132 [==============================] - 0s 180us/sample - loss: 1.0344 - acc: 0.7576\n",
      "Epoch 34/100\n",
      "132/132 [==============================] - 0s 159us/sample - loss: 1.0239 - acc: 0.7576\n",
      "Epoch 35/100\n",
      "132/132 [==============================] - 0s 167us/sample - loss: 1.0136 - acc: 0.7576\n",
      "Epoch 36/100\n",
      "132/132 [==============================] - 0s 194us/sample - loss: 1.0027 - acc: 0.7576\n",
      "Epoch 37/100\n",
      "132/132 [==============================] - 0s 129us/sample - loss: 0.9917 - acc: 0.7576\n",
      "Epoch 38/100\n",
      "132/132 [==============================] - 0s 152us/sample - loss: 0.9813 - acc: 0.7500\n",
      "Epoch 39/100\n",
      "132/132 [==============================] - 0s 130us/sample - loss: 0.9717 - acc: 0.7576\n",
      "Epoch 40/100\n",
      "132/132 [==============================] - 0s 167us/sample - loss: 0.9621 - acc: 0.7727\n",
      "Epoch 41/100\n",
      "132/132 [==============================] - 0s 157us/sample - loss: 0.9528 - acc: 0.7652\n",
      "Epoch 42/100\n",
      "132/132 [==============================] - 0s 132us/sample - loss: 0.9431 - acc: 0.7727\n",
      "Epoch 43/100\n",
      "132/132 [==============================] - 0s 150us/sample - loss: 0.9334 - acc: 0.7955\n",
      "Epoch 44/100\n",
      "132/132 [==============================] - 0s 153us/sample - loss: 0.9247 - acc: 0.7955\n",
      "Epoch 45/100\n",
      "132/132 [==============================] - 0s 124us/sample - loss: 0.9155 - acc: 0.7955\n",
      "Epoch 46/100\n",
      "132/132 [==============================] - 0s 131us/sample - loss: 0.9074 - acc: 0.8030\n",
      "Epoch 47/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 0.9002 - acc: 0.8030\n",
      "Epoch 48/100\n",
      "132/132 [==============================] - 0s 141us/sample - loss: 0.8916 - acc: 0.8030\n",
      "Epoch 49/100\n",
      "132/132 [==============================] - 0s 161us/sample - loss: 0.8834 - acc: 0.8030\n",
      "Epoch 50/100\n",
      "132/132 [==============================] - 0s 156us/sample - loss: 0.8765 - acc: 0.8106\n",
      "Epoch 51/100\n",
      "132/132 [==============================] - 0s 131us/sample - loss: 0.8687 - acc: 0.8182\n",
      "Epoch 52/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 0.8619 - acc: 0.8182\n",
      "Epoch 53/100\n",
      "132/132 [==============================] - 0s 150us/sample - loss: 0.8535 - acc: 0.8182\n",
      "Epoch 54/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 0.8467 - acc: 0.8258\n",
      "Epoch 55/100\n",
      "132/132 [==============================] - 0s 143us/sample - loss: 0.8406 - acc: 0.8182\n",
      "Epoch 56/100\n",
      "132/132 [==============================] - 0s 143us/sample - loss: 0.8345 - acc: 0.8182\n",
      "Epoch 57/100\n",
      "132/132 [==============================] - 0s 133us/sample - loss: 0.8286 - acc: 0.8258\n",
      "Epoch 58/100\n",
      "132/132 [==============================] - 0s 121us/sample - loss: 0.8223 - acc: 0.8258\n",
      "Epoch 59/100\n",
      "132/132 [==============================] - 0s 117us/sample - loss: 0.8156 - acc: 0.8333\n",
      "Epoch 60/100\n",
      "132/132 [==============================] - 0s 166us/sample - loss: 0.8094 - acc: 0.8258\n",
      "Epoch 61/100\n",
      "132/132 [==============================] - 0s 135us/sample - loss: 0.8024 - acc: 0.8258\n",
      "Epoch 62/100\n",
      "132/132 [==============================] - 0s 144us/sample - loss: 0.7959 - acc: 0.8182\n",
      "Epoch 63/100\n",
      "132/132 [==============================] - 0s 158us/sample - loss: 0.7886 - acc: 0.8182\n",
      "Epoch 64/100\n",
      "132/132 [==============================] - 0s 127us/sample - loss: 0.7823 - acc: 0.8106\n",
      "Epoch 65/100\n",
      "132/132 [==============================] - 0s 128us/sample - loss: 0.7772 - acc: 0.8106\n",
      "Epoch 66/100\n",
      "132/132 [==============================] - 0s 149us/sample - loss: 0.7722 - acc: 0.8106\n",
      "Epoch 67/100\n",
      "132/132 [==============================] - 0s 140us/sample - loss: 0.7656 - acc: 0.8182\n",
      "Epoch 68/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 0.7598 - acc: 0.8182\n",
      "Epoch 69/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 0.7551 - acc: 0.8258\n",
      "Epoch 70/100\n",
      "132/132 [==============================] - 0s 140us/sample - loss: 0.7491 - acc: 0.8333\n",
      "Epoch 71/100\n",
      "132/132 [==============================] - 0s 158us/sample - loss: 0.7422 - acc: 0.8409\n",
      "Epoch 72/100\n",
      "132/132 [==============================] - 0s 125us/sample - loss: 0.7366 - acc: 0.8561\n",
      "Epoch 73/100\n",
      "132/132 [==============================] - 0s 154us/sample - loss: 0.7314 - acc: 0.8712\n",
      "Epoch 74/100\n",
      "132/132 [==============================] - 0s 134us/sample - loss: 0.7262 - acc: 0.8788\n",
      "Epoch 75/100\n",
      "132/132 [==============================] - 0s 141us/sample - loss: 0.7221 - acc: 0.8258\n",
      "Epoch 76/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 0.7176 - acc: 0.8333\n",
      "Epoch 77/100\n",
      "132/132 [==============================] - 0s 142us/sample - loss: 0.7125 - acc: 0.8333\n",
      "Epoch 78/100\n",
      "132/132 [==============================] - 0s 148us/sample - loss: 0.7074 - acc: 0.8409\n",
      "Epoch 79/100\n",
      "132/132 [==============================] - 0s 201us/sample - loss: 0.7021 - acc: 0.8485\n",
      "Epoch 80/100\n",
      "132/132 [==============================] - 0s 156us/sample - loss: 0.6972 - acc: 0.8485\n",
      "Epoch 81/100\n",
      "132/132 [==============================] - 0s 157us/sample - loss: 0.6925 - acc: 0.8485\n",
      "Epoch 82/100\n",
      "132/132 [==============================] - 0s 167us/sample - loss: 0.6872 - acc: 0.8561\n",
      "Epoch 83/100\n",
      "132/132 [==============================] - 0s 161us/sample - loss: 0.6827 - acc: 0.8636\n",
      "Epoch 84/100\n",
      "132/132 [==============================] - 0s 157us/sample - loss: 0.6784 - acc: 0.8636\n",
      "Epoch 85/100\n",
      "132/132 [==============================] - 0s 154us/sample - loss: 0.6736 - acc: 0.8864\n",
      "Epoch 86/100\n",
      "132/132 [==============================] - 0s 150us/sample - loss: 0.6701 - acc: 0.8939\n",
      "Epoch 87/100\n",
      "132/132 [==============================] - 0s 141us/sample - loss: 0.6654 - acc: 0.9015\n",
      "Epoch 88/100\n",
      "132/132 [==============================] - 0s 156us/sample - loss: 0.6611 - acc: 0.8939\n",
      "Epoch 89/100\n",
      "132/132 [==============================] - 0s 190us/sample - loss: 0.6583 - acc: 0.8864\n",
      "Epoch 90/100\n",
      "132/132 [==============================] - 0s 172us/sample - loss: 0.6540 - acc: 0.8864\n",
      "Epoch 91/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 0.6503 - acc: 0.8712\n",
      "Epoch 92/100\n",
      "132/132 [==============================] - 0s 158us/sample - loss: 0.6466 - acc: 0.8485\n",
      "Epoch 93/100\n",
      "132/132 [==============================] - 0s 162us/sample - loss: 0.6424 - acc: 0.8561\n",
      "Epoch 94/100\n",
      "132/132 [==============================] - 0s 141us/sample - loss: 0.6376 - acc: 0.8712\n",
      "Epoch 95/100\n",
      "132/132 [==============================] - 0s 170us/sample - loss: 0.6340 - acc: 0.8712\n",
      "Epoch 96/100\n",
      "132/132 [==============================] - 0s 134us/sample - loss: 0.6300 - acc: 0.8788\n",
      "Epoch 97/100\n",
      "132/132 [==============================] - 0s 162us/sample - loss: 0.6253 - acc: 0.8864\n",
      "Epoch 98/100\n",
      "132/132 [==============================] - 0s 175us/sample - loss: 0.6217 - acc: 0.8939\n",
      "Epoch 99/100\n",
      "132/132 [==============================] - 0s 166us/sample - loss: 0.6183 - acc: 0.9015\n",
      "Epoch 100/100\n",
      "132/132 [==============================] - 0s 169us/sample - loss: 0.6140 - acc: 0.9091\n",
      "56/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 1ms/sample - loss: 0.8192 - acc: 0.7857\n",
      "[0.7775702050754002, 0.78571427]\n",
      "tf.Tensor(\n",
      "[[ 4  1  0  2  0]\n",
      " [ 0  7  0  1  0]\n",
      " [ 0  0 15  3  0]\n",
      " [ 2  1  1 12  0]\n",
      " [ 0  0  0  1  6]], shape=(5, 5), dtype=int32)\n",
      "----------------------------\n",
      "Incorrect Predictions\n",
      "work is hard\t 3.0 2\n",
      "this girl is messing with me\t 3.0 0\n",
      "you brighten my day\t 2.0 3\n",
      "she is a bully\t 3.0 0\n",
      "i miss you so much\t 0.0 3\n",
      "will you be my valentine\t 2.0 3\n",
      "i will  run 1.0 3\n",
      "i love you to the stars and back\t 0.0 1\n",
      "what you did was awesome\t 2.0 3\n",
      "go away\t 3.0 1\n",
      "family is all i have\t 0.0 3\n",
      "i did not have breakfast  4.0 3\n",
      "-------------------------- \n",
      "Correct Predictions\n",
      "i want to eat\t 4.0 4\n",
      "he did not answer\t 3.0 3\n",
      "he got a very nice raise\t 2.0 2\n",
      "she got me a nice present\t 2.0 2\n",
      "ha ha ha it was so funny\t 2.0 2\n",
      "he is a good friend\t 2.0 2\n",
      "i am upset\t 3.0 3\n",
      "we had such a lovely dinner tonight\t 2.0 2\n",
      "where is the food\t 4.0 4\n",
      "stop making this joke ha ha ha\t 2.0 2\n",
      "where is the ball\t 1.0 1\n",
      "are you serious 3.0 3\n",
      "let us go play baseball\t 1.0 1\n",
      "this stupid grader is not working \t 3.0 3\n",
      "work is horrible\t 3.0 3\n",
      "congratulation for having a baby\t 2.0 2\n",
      "stop pissing me off 3.0 3\n",
      "any suggestions for dinner\t 4.0 4\n",
      "i love taking breaks\t 0.0 0\n",
      "i boiled rice\t 4.0 4\n",
      "why are you feeling bad\t 3.0 3\n",
      "i am upset\t 3.0 3\n",
      "give me the ball 1.0 1\n",
      "my grandmother is the love of my life\t 0.0 0\n",
      "enjoy your game 1.0 1\n",
      "valentine day is near\t 2.0 2\n",
      "throw the ball\t 1.0 1\n",
      "my life is so boring\t 3.0 3\n",
      "she said yes\t 2.0 2\n",
      "he can pitch really well\t 1.0 1\n",
      "dance with me\t 2.0 2\n",
      "i am hungry 4.0 4\n",
      "see you at the restaurant\t 4.0 4\n",
      "i like to laugh\t 2.0 2\n",
      "i like your jacket \t 0.0 0\n",
      "i miss her\t 0.0 0\n",
      "what is your favorite baseball game\t 1.0 1\n",
      "good job\t 2.0 2\n",
      "ha ha ha lol\t 2.0 2\n",
      "i do not want to joke\t 3.0 3\n",
      "yesterday we lost again\t 3.0 3\n",
      "you are failing this exercise\t 3.0 3\n",
      "good joke\t 2.0 2\n",
      "you deserve this nice prize\t 2.0 2\n"
     ]
    }
   ],
   "source": [
    "# Glove vector of 200 Dimensions on Wikipedia using avg\n",
    "X_train, X_test, word_to_vec_map = read_files_and_retrieve_wordtovec_map('glove.6B.200d.txt')\n",
    "X_train_lower, X_test_lower, X_train_Y, X_test_Y = process_training_and_test_values(X_train, X_test)\n",
    "tokenized_training_data = tokenize_values(X_train_lower)\n",
    "tokenized_test_labels = tokenize_values(X_test_lower)\n",
    "\n",
    "avg = sentence_to_avg(tokenized_training_data, word_to_vec_map, 200) \n",
    "test_avg = sentence_to_avg(tokenized_test_labels, word_to_vec_map, 200)\n",
    "\n",
    "training_dataset = convert_to_nparray(avg, 'float32')\n",
    "testing_dataset = convert_to_nparray(test_avg, 'float32')\n",
    "\n",
    "training_label_dataset = convert_to_nparray(X_train_Y, 'float32')\n",
    "test_label_dataset = convert_to_nparray(X_test_Y, 'float32')\n",
    "\n",
    "model = Model_to_Fit(512, 5)\n",
    "model.fit(x = training_dataset, y = training_label_dataset, epochs = 100)\n",
    "\n",
    "predictions = model.predict(testing_dataset)\n",
    "model.get_weights()\n",
    "print(model.evaluate(testing_dataset, test_label_dataset))\n",
    "\n",
    "pred_val = CreateConfusionMatrixandReturnPredictions(test_label_dataset, convert_to_nparray(predictions, 'float32'))\n",
    "ShowCorrectandIncorrectPredictions(test_label_dataset, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "R4P1itiBOSDl",
    "outputId": "f0719eb0-ebc3-4bac-e0f1-95aee227f56d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 132 samples\n",
      "Epoch 1/100\n",
      "132/132 [==============================] - 0s 2ms/sample - loss: 1.7316 - accuracy: 0.2045\n",
      "Epoch 2/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 1.6696 - accuracy: 0.2045\n",
      "Epoch 3/100\n",
      "132/132 [==============================] - 0s 135us/sample - loss: 1.6199 - accuracy: 0.2197\n",
      "Epoch 4/100\n",
      "132/132 [==============================] - 0s 116us/sample - loss: 1.5729 - accuracy: 0.2273\n",
      "Epoch 5/100\n",
      "132/132 [==============================] - 0s 125us/sample - loss: 1.5349 - accuracy: 0.2424\n",
      "Epoch 6/100\n",
      "132/132 [==============================] - 0s 121us/sample - loss: 1.4966 - accuracy: 0.3030\n",
      "Epoch 7/100\n",
      "132/132 [==============================] - 0s 126us/sample - loss: 1.4634 - accuracy: 0.3409\n",
      "Epoch 8/100\n",
      "132/132 [==============================] - 0s 111us/sample - loss: 1.4335 - accuracy: 0.3788\n",
      "Epoch 9/100\n",
      "132/132 [==============================] - 0s 116us/sample - loss: 1.4063 - accuracy: 0.4318\n",
      "Epoch 10/100\n",
      "132/132 [==============================] - 0s 145us/sample - loss: 1.3785 - accuracy: 0.4545\n",
      "Epoch 11/100\n",
      "132/132 [==============================] - 0s 127us/sample - loss: 1.3527 - accuracy: 0.4848\n",
      "Epoch 12/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 1.3281 - accuracy: 0.5227\n",
      "Epoch 13/100\n",
      "132/132 [==============================] - 0s 154us/sample - loss: 1.3059 - accuracy: 0.5682\n",
      "Epoch 14/100\n",
      "132/132 [==============================] - 0s 169us/sample - loss: 1.2828 - accuracy: 0.5985\n",
      "Epoch 15/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 1.2625 - accuracy: 0.6212\n",
      "Epoch 16/100\n",
      "132/132 [==============================] - 0s 154us/sample - loss: 1.2397 - accuracy: 0.6667\n",
      "Epoch 17/100\n",
      "132/132 [==============================] - 0s 167us/sample - loss: 1.2185 - accuracy: 0.6894\n",
      "Epoch 18/100\n",
      "132/132 [==============================] - 0s 136us/sample - loss: 1.1987 - accuracy: 0.7045\n",
      "Epoch 19/100\n",
      "132/132 [==============================] - 0s 158us/sample - loss: 1.1796 - accuracy: 0.7197\n",
      "Epoch 20/100\n",
      "132/132 [==============================] - 0s 114us/sample - loss: 1.1601 - accuracy: 0.7424\n",
      "Epoch 21/100\n",
      "132/132 [==============================] - 0s 126us/sample - loss: 1.1428 - accuracy: 0.7576\n",
      "Epoch 22/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 1.1249 - accuracy: 0.7576\n",
      "Epoch 23/100\n",
      "132/132 [==============================] - 0s 153us/sample - loss: 1.1070 - accuracy: 0.7652\n",
      "Epoch 24/100\n",
      "132/132 [==============================] - 0s 124us/sample - loss: 1.0900 - accuracy: 0.7727\n",
      "Epoch 25/100\n",
      "132/132 [==============================] - 0s 113us/sample - loss: 1.0743 - accuracy: 0.7727\n",
      "Epoch 26/100\n",
      "132/132 [==============================] - 0s 114us/sample - loss: 1.0595 - accuracy: 0.7652\n",
      "Epoch 27/100\n",
      "132/132 [==============================] - 0s 151us/sample - loss: 1.0444 - accuracy: 0.7576\n",
      "Epoch 28/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 1.0302 - accuracy: 0.7652\n",
      "Epoch 29/100\n",
      "132/132 [==============================] - 0s 164us/sample - loss: 1.0161 - accuracy: 0.7652\n",
      "Epoch 30/100\n",
      "132/132 [==============================] - 0s 130us/sample - loss: 1.0021 - accuracy: 0.7803\n",
      "Epoch 31/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 0.9881 - accuracy: 0.7803\n",
      "Epoch 32/100\n",
      "132/132 [==============================] - 0s 149us/sample - loss: 0.9751 - accuracy: 0.7879\n",
      "Epoch 33/100\n",
      "132/132 [==============================] - 0s 149us/sample - loss: 0.9624 - accuracy: 0.7955\n",
      "Epoch 34/100\n",
      "132/132 [==============================] - 0s 163us/sample - loss: 0.9511 - accuracy: 0.7803\n",
      "Epoch 35/100\n",
      "132/132 [==============================] - 0s 132us/sample - loss: 0.9388 - accuracy: 0.8106\n",
      "Epoch 36/100\n",
      "132/132 [==============================] - 0s 122us/sample - loss: 0.9275 - accuracy: 0.8182\n",
      "Epoch 37/100\n",
      "132/132 [==============================] - 0s 119us/sample - loss: 0.9163 - accuracy: 0.8182\n",
      "Epoch 38/100\n",
      "132/132 [==============================] - 0s 134us/sample - loss: 0.9054 - accuracy: 0.8333\n",
      "Epoch 39/100\n",
      "132/132 [==============================] - 0s 113us/sample - loss: 0.8943 - accuracy: 0.8409\n",
      "Epoch 40/100\n",
      "132/132 [==============================] - 0s 127us/sample - loss: 0.8838 - accuracy: 0.8712\n",
      "Epoch 41/100\n",
      "132/132 [==============================] - 0s 126us/sample - loss: 0.8728 - accuracy: 0.8712\n",
      "Epoch 42/100\n",
      "132/132 [==============================] - 0s 149us/sample - loss: 0.8631 - accuracy: 0.8712\n",
      "Epoch 43/100\n",
      "132/132 [==============================] - 0s 148us/sample - loss: 0.8529 - accuracy: 0.8712\n",
      "Epoch 44/100\n",
      "132/132 [==============================] - 0s 187us/sample - loss: 0.8425 - accuracy: 0.8712\n",
      "Epoch 45/100\n",
      "132/132 [==============================] - 0s 163us/sample - loss: 0.8333 - accuracy: 0.8712\n",
      "Epoch 46/100\n",
      "132/132 [==============================] - 0s 148us/sample - loss: 0.8237 - accuracy: 0.8561\n",
      "Epoch 47/100\n",
      "132/132 [==============================] - 0s 131us/sample - loss: 0.8151 - accuracy: 0.8485\n",
      "Epoch 48/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 0.8058 - accuracy: 0.8485\n",
      "Epoch 49/100\n",
      "132/132 [==============================] - 0s 153us/sample - loss: 0.7971 - accuracy: 0.8561\n",
      "Epoch 50/100\n",
      "132/132 [==============================] - 0s 135us/sample - loss: 0.7885 - accuracy: 0.8864\n",
      "Epoch 51/100\n",
      "132/132 [==============================] - 0s 142us/sample - loss: 0.7802 - accuracy: 0.8864\n",
      "Epoch 52/100\n",
      "132/132 [==============================] - 0s 135us/sample - loss: 0.7721 - accuracy: 0.9015\n",
      "Epoch 53/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 0.7641 - accuracy: 0.9091\n",
      "Epoch 54/100\n",
      "132/132 [==============================] - 0s 116us/sample - loss: 0.7564 - accuracy: 0.9091\n",
      "Epoch 55/100\n",
      "132/132 [==============================] - 0s 133us/sample - loss: 0.7486 - accuracy: 0.9015\n",
      "Epoch 56/100\n",
      "132/132 [==============================] - 0s 132us/sample - loss: 0.7412 - accuracy: 0.8864\n",
      "Epoch 57/100\n",
      "132/132 [==============================] - 0s 147us/sample - loss: 0.7338 - accuracy: 0.8939\n",
      "Epoch 58/100\n",
      "132/132 [==============================] - 0s 171us/sample - loss: 0.7262 - accuracy: 0.8939\n",
      "Epoch 59/100\n",
      "132/132 [==============================] - 0s 132us/sample - loss: 0.7191 - accuracy: 0.9091\n",
      "Epoch 60/100\n",
      "132/132 [==============================] - 0s 137us/sample - loss: 0.7118 - accuracy: 0.9091\n",
      "Epoch 61/100\n",
      "132/132 [==============================] - 0s 135us/sample - loss: 0.7051 - accuracy: 0.9091\n",
      "Epoch 62/100\n",
      "132/132 [==============================] - 0s 119us/sample - loss: 0.6985 - accuracy: 0.9167\n",
      "Epoch 63/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 0.6917 - accuracy: 0.9167\n",
      "Epoch 64/100\n",
      "132/132 [==============================] - 0s 143us/sample - loss: 0.6851 - accuracy: 0.9167\n",
      "Epoch 65/100\n",
      "132/132 [==============================] - 0s 116us/sample - loss: 0.6791 - accuracy: 0.9091\n",
      "Epoch 66/100\n",
      "132/132 [==============================] - 0s 142us/sample - loss: 0.6727 - accuracy: 0.9167\n",
      "Epoch 67/100\n",
      "132/132 [==============================] - 0s 136us/sample - loss: 0.6670 - accuracy: 0.9015\n",
      "Epoch 68/100\n",
      "132/132 [==============================] - 0s 179us/sample - loss: 0.6620 - accuracy: 0.8788\n",
      "Epoch 69/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 0.6561 - accuracy: 0.8788\n",
      "Epoch 70/100\n",
      "132/132 [==============================] - 0s 151us/sample - loss: 0.6504 - accuracy: 0.8788\n",
      "Epoch 71/100\n",
      "132/132 [==============================] - 0s 125us/sample - loss: 0.6444 - accuracy: 0.8864\n",
      "Epoch 72/100\n",
      "132/132 [==============================] - 0s 124us/sample - loss: 0.6394 - accuracy: 0.9015\n",
      "Epoch 73/100\n",
      "132/132 [==============================] - 0s 151us/sample - loss: 0.6338 - accuracy: 0.8939\n",
      "Epoch 74/100\n",
      "132/132 [==============================] - 0s 135us/sample - loss: 0.6285 - accuracy: 0.9167\n",
      "Epoch 75/100\n",
      "132/132 [==============================] - 0s 130us/sample - loss: 0.6227 - accuracy: 0.9167\n",
      "Epoch 76/100\n",
      "132/132 [==============================] - 0s 162us/sample - loss: 0.6175 - accuracy: 0.9242\n",
      "Epoch 77/100\n",
      "132/132 [==============================] - 0s 190us/sample - loss: 0.6128 - accuracy: 0.9242\n",
      "Epoch 78/100\n",
      "132/132 [==============================] - 0s 165us/sample - loss: 0.6080 - accuracy: 0.9167\n",
      "Epoch 79/100\n",
      "132/132 [==============================] - 0s 148us/sample - loss: 0.6033 - accuracy: 0.9091\n",
      "Epoch 80/100\n",
      "132/132 [==============================] - 0s 151us/sample - loss: 0.5983 - accuracy: 0.9242\n",
      "Epoch 81/100\n",
      "132/132 [==============================] - 0s 138us/sample - loss: 0.5930 - accuracy: 0.9318\n",
      "Epoch 82/100\n",
      "132/132 [==============================] - 0s 212us/sample - loss: 0.5883 - accuracy: 0.9318\n",
      "Epoch 83/100\n",
      "132/132 [==============================] - 0s 143us/sample - loss: 0.5834 - accuracy: 0.9318\n",
      "Epoch 84/100\n",
      "132/132 [==============================] - 0s 140us/sample - loss: 0.5789 - accuracy: 0.9394\n",
      "Epoch 85/100\n",
      "132/132 [==============================] - 0s 124us/sample - loss: 0.5740 - accuracy: 0.9394\n",
      "Epoch 86/100\n",
      "132/132 [==============================] - 0s 137us/sample - loss: 0.5700 - accuracy: 0.9394\n",
      "Epoch 87/100\n",
      "132/132 [==============================] - 0s 139us/sample - loss: 0.5658 - accuracy: 0.9470\n",
      "Epoch 88/100\n",
      "132/132 [==============================] - 0s 155us/sample - loss: 0.5618 - accuracy: 0.9470\n",
      "Epoch 89/100\n",
      "132/132 [==============================] - 0s 149us/sample - loss: 0.5577 - accuracy: 0.9470\n",
      "Epoch 90/100\n",
      "132/132 [==============================] - 0s 140us/sample - loss: 0.5537 - accuracy: 0.9470\n",
      "Epoch 91/100\n",
      "132/132 [==============================] - 0s 192us/sample - loss: 0.5497 - accuracy: 0.9470\n",
      "Epoch 92/100\n",
      "132/132 [==============================] - 0s 157us/sample - loss: 0.5450 - accuracy: 0.9470\n",
      "Epoch 93/100\n",
      "132/132 [==============================] - 0s 132us/sample - loss: 0.5414 - accuracy: 0.9470\n",
      "Epoch 94/100\n",
      "132/132 [==============================] - 0s 136us/sample - loss: 0.5373 - accuracy: 0.9470\n",
      "Epoch 95/100\n",
      "132/132 [==============================] - 0s 123us/sample - loss: 0.5333 - accuracy: 0.9470\n",
      "Epoch 96/100\n",
      "132/132 [==============================] - 0s 146us/sample - loss: 0.5291 - accuracy: 0.9470\n",
      "Epoch 97/100\n",
      "132/132 [==============================] - 0s 169us/sample - loss: 0.5249 - accuracy: 0.9470\n",
      "Epoch 98/100\n",
      "132/132 [==============================] - 0s 140us/sample - loss: 0.5214 - accuracy: 0.9470\n",
      "Epoch 99/100\n",
      "132/132 [==============================] - 0s 142us/sample - loss: 0.5176 - accuracy: 0.9470\n",
      "Epoch 100/100\n",
      "132/132 [==============================] - 0s 153us/sample - loss: 0.5139 - accuracy: 0.9470\n",
      "56/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 1ms/sample - loss: 0.7295 - accuracy: 0.8750\n",
      "[0.6505708779607501, 0.875]\n",
      "tf.Tensor(\n",
      "[[ 6  0  1  0  0]\n",
      " [ 0  7  1  0  0]\n",
      " [ 1  0 14  3  0]\n",
      " [ 0  1  0 15  0]\n",
      " [ 0  0  0  0  7]], shape=(5, 5), dtype=int32)\n",
      "----------------------------\n",
      "Incorrect Predictions\n",
      "you brighten my day\t 2.0 0\n",
      "will you be my valentine\t 2.0 3\n",
      "he can pitch really well\t 1.0 2\n",
      "dance with me\t 2.0 3\n",
      "i like to laugh\t 2.0 3\n",
      "go away\t 3.0 1\n",
      "family is all i have\t 0.0 2\n",
      "-------------------------- \n",
      "Correct Predictions\n",
      "i want to eat\t 4.0 4\n",
      "he did not answer\t 3.0 3\n",
      "he got a very nice raise\t 2.0 2\n",
      "she got me a nice present\t 2.0 2\n",
      "ha ha ha it was so funny\t 2.0 2\n",
      "he is a good friend\t 2.0 2\n",
      "i am upset\t 3.0 3\n",
      "we had such a lovely dinner tonight\t 2.0 2\n",
      "where is the food\t 4.0 4\n",
      "stop making this joke ha ha ha\t 2.0 2\n",
      "where is the ball\t 1.0 1\n",
      "work is hard\t 3.0 3\n",
      "this girl is messing with me\t 3.0 3\n",
      "are you serious 3.0 3\n",
      "let us go play baseball\t 1.0 1\n",
      "this stupid grader is not working \t 3.0 3\n",
      "work is horrible\t 3.0 3\n",
      "congratulation for having a baby\t 2.0 2\n",
      "stop pissing me off 3.0 3\n",
      "any suggestions for dinner\t 4.0 4\n",
      "i love taking breaks\t 0.0 0\n",
      "i boiled rice\t 4.0 4\n",
      "she is a bully\t 3.0 3\n",
      "why are you feeling bad\t 3.0 3\n",
      "i am upset\t 3.0 3\n",
      "give me the ball 1.0 1\n",
      "my grandmother is the love of my life\t 0.0 0\n",
      "enjoy your game 1.0 1\n",
      "valentine day is near\t 2.0 2\n",
      "i miss you so much\t 0.0 0\n",
      "throw the ball\t 1.0 1\n",
      "my life is so boring\t 3.0 3\n",
      "she said yes\t 2.0 2\n",
      "i am hungry 4.0 4\n",
      "see you at the restaurant\t 4.0 4\n",
      "i will  run 1.0 1\n",
      "i like your jacket \t 0.0 0\n",
      "i miss her\t 0.0 0\n",
      "what is your favorite baseball game\t 1.0 1\n",
      "good job\t 2.0 2\n",
      "i love you to the stars and back\t 0.0 0\n",
      "what you did was awesome\t 2.0 2\n",
      "ha ha ha lol\t 2.0 2\n",
      "i do not want to joke\t 3.0 3\n",
      "yesterday we lost again\t 3.0 3\n",
      "you are failing this exercise\t 3.0 3\n",
      "good joke\t 2.0 2\n",
      "you deserve this nice prize\t 2.0 2\n",
      "i did not have breakfast  4.0 4\n"
     ]
    }
   ],
   "source": [
    "# Glove vector of 200 Dimensions for Twitter using Avg\n",
    "X_train, X_test, word_to_vec_map = read_files_and_retrieve_wordtovec_map('glove.twitter.27B.200d.txt')\n",
    "X_train_lower, X_test_lower, X_train_Y, X_test_Y = process_training_and_test_values(X_train, X_test)\n",
    "tokenized_training_data = tokenize_values(X_train_lower)\n",
    "tokenized_test_labels = tokenize_values(X_test_lower)\n",
    "\n",
    "avg = sentence_to_avg(tokenized_training_data, word_to_vec_map, 200) \n",
    "test_avg = sentence_to_avg(tokenized_test_labels, word_to_vec_map, 200)\n",
    "\n",
    "\n",
    "training_dataset = convert_to_nparray(avg, 'float32')\n",
    "testing_dataset = convert_to_nparray(test_avg, 'float32')\n",
    "\n",
    "training_label_dataset = convert_to_nparray(X_train_Y, 'float32')\n",
    "test_label_dataset = convert_to_nparray(X_test_Y, 'float32')\n",
    "\n",
    "model = Model_to_Fit(512, 5)\n",
    "model.fit(x = training_dataset, y = training_label_dataset, epochs = 100)\n",
    "\n",
    "predictions = model.predict(testing_dataset)\n",
    "model.get_weights()\n",
    "print(model.evaluate(testing_dataset, test_label_dataset))\n",
    "\n",
    "pred_val = CreateConfusionMatrixandReturnPredictions(test_label_dataset, convert_to_nparray(predictions, 'float32'))\n",
    "ShowCorrectandIncorrectPredictions(test_label_dataset, pred_val)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
