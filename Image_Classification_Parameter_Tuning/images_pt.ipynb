{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RH5WFRmzbjZ2",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Image Classification Using CIFAR-10 dataset\n",
    "\n",
    "The CIFAR-10 (Canadian Institute For Advanced Research) dataset contains 60,000 32x32 color images. Each image is labeled with one of the following 10 categories: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. There are 50000 training images and 10000 test images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pq3RpHMbjZ4",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "This notebook has 5 parts.  You will practice TensorFlow on three different levels of abstraction.\n",
    "\n",
    "1. Part I, Preparation: load the CIFAR-10 dataset.\n",
    "2. Part II, Barebone TensorFlow: **Abstraction Level 1**, we will work directly with low-level TensorFlow graphs. \n",
    "3. Part III, Keras Model API: **Abstraction Level 2**, we will use `tf.keras.Model` to define arbitrary neural network architecture. \n",
    "4. Part IV, Keras Sequential + Functional API: **Abstraction Level 3**, we will use `tf.keras.Sequential` to define a linear feed-forward network very conveniently, and then explore the functional libraries for building unique and uncommon models that require more flexibility.\n",
    "5. Part V, Tuning: Experiment with different architectures, activation functions, weight initializations, optimizers, hyperparameters, regularizations or other advanced features. Your goal is to get accuracy as high as possible on CIFAR-10 (without using convolutional layers).\n",
    "\n",
    "We will discuss Keras in more detail later in the notebook.\n",
    "\n",
    "Here is a table of comparison:\n",
    "\n",
    "| API           | Flexibility | Convenience |\n",
    "|---------------|-------------|-------------|\n",
    "| Barebone      | High        | Low         |\n",
    "| `tf.keras.Model`     | High        | Medium      |\n",
    "| `tf.keras.Sequential` | Low         | High        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvuw4VPWbjZ6"
   },
   "source": [
    "# Part I: Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. The downloading might take a couple minutes the first time you do it, but the files should stay cached after that. \n",
    "\n",
    "The `tf.keras.datasets` package in TensorFlow provides prebuilt utility functions for loading many common datasets. For the purposes of this assignment we will write our own code to preprocess the data and iterate through it in minibatches. The `tf.data` package in TensorFlow provides tools for automating this process, but working with this package adds extra complication and is beyond the scope of this notebook. However using `tf.data` can be much more efficient than the simple approach used in this notebook, so you should consider using it for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "-rgaN0G_bjZ7",
    "outputId": "4e762649-2073-4e15-fad8-7e91fea1e2b6",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#import tensorflow as tf\n",
    "#importing the required project dependencies\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "q5OGAB5objaB",
    "outputId": "542d5ad7-35d1-4082-d41d-87496b528a1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-rc2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "xvs3OYaEbjaK",
    "outputId": "096f0049-0a09-4c59-c30c-a5d65071795f",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 4s 0us/step\n",
      "X_trian shape (50000, 32, 32, 3)\n",
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,) int32\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "def load_cifar10(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    print(\"X_trian shape\", X_train.shape)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "  \n",
    "def load_datagen(x_train, y_train):\n",
    "  train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "  x = 0;\n",
    "  for x_batch, y_batch in train_datagen.flow(x_train, y_train, batch_size=32):\n",
    "      x_new_train =np.concatenate((x_train, x_batch))\n",
    "      y_new_train = np.concatenate((y_train, y_batch))\n",
    "      x = x + 1     \n",
    "      if(x > 200):\n",
    "        break\n",
    "  x_new_train = np.asarray(x_new_train, dtype = np.int32)\n",
    "  y_new_train = np.asarray(y_new_train, dtype = np.int32).flatten()\n",
    "  return x_new_train, y_new_train\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "NHW = (0, 1, 2)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rL41ZEtbjaP",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "\n",
    "\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "9ViK7HpAbjaT",
    "outputId": "0b503c46-dc21-40eb-c213-5c784ef92392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (64, 32, 32, 3) (64,)\n",
      "1 (64, 32, 32, 3) (64,)\n",
      "2 (64, 32, 32, 3) (64,)\n",
      "3 (64, 32, 32, 3) (64,)\n",
      "4 (64, 32, 32, 3) (64,)\n",
      "5 (64, 32, 32, 3) (64,)\n",
      "6 (64, 32, 32, 3) (64,)\n"
     ]
    }
   ],
   "source": [
    "# We can iterate through a dataset like this:\n",
    "for t, (x, y) in enumerate(train_dset):\n",
    "    print(t, x.shape, y.shape)\n",
    "    if t > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x0HAyvzobjaX"
   },
   "source": [
    "You can optionally **use GPU by setting the flag to True below**. It's not neccessary to use a GPU for this assignment; if you are working on Google Cloud then we recommend that you do not use a GPU, as it will be significantly more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yNLQ9JhGbjaZ",
    "outputId": "afdc9d22-6634-41af-95b5-dad10e55d721",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  /cpu:0\n"
     ]
    }
   ],
   "source": [
    "# Set up some global variables\n",
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "# Constant to control how often we print when training models\n",
    "print_every = 100\n",
    "\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8J3OGxtbjbn",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# Part II: Barebones TensorFlow\n",
    "TensorFlow comes with various high-level APIs which make it very convenient to define and train neural networks; we will cover some of these constructs in Part III and Part IV of this notebook. In this section, we will start by building a model with basic TensorFlow constructs to help you better understand what's going on under the hood of the higher-level APIs.\n",
    "\n",
    "**\"Barebones Tensorflow\" is important to understanding the building blocks of TensorFlow, but much of it involves concepts from TensorFlow 1.x.** We will be working with legacy modules such as `tf.Variable`.\n",
    "\n",
    "Therefore, please read and understand the differences between legacy (1.x) TF and the new (2.0) TF.\n",
    "\n",
    "### Historical background on TensorFlow 1.x\n",
    "\n",
    "TensorFlow 1.x is primarily a framework for working with **static computational graphs**. Nodes in the computational graph are Tensors which will hold n-dimensional arrays when the graph is run; edges in the graph represent functions that will operate on Tensors when the graph is run to actually perform useful computation.\n",
    "\n",
    "Before Tensorflow 2.0, we had to configure the graph into two phases. There are plenty of tutorials online that explain this two-step process. The process generally looks like the following for TF 1.x:\n",
    "1. **Build a computational graph that describes the computation that you want to perform**. This stage doesn't actually perform any computation; it just builds up a symbolic representation of your computation. This stage will typically define one or more `placeholder` objects that represent inputs to the computational graph.\n",
    "2. **Run the computational graph many times.** Each time the graph is run (e.g. for one gradient descent step) you will specify which parts of the graph you want to compute, and pass a `feed_dict` dictionary that will give concrete values to any `placeholder`s in the graph.\n",
    "\n",
    "### The new paradigm in Tensorflow 2.0\n",
    "Now, with Tensorflow 2.0, we can simply adopt a functional form that is more Pythonic and similar in spirit to PyTorch and direct Numpy operation. Instead of the 2-step paradigm with computation graphs, making it (among other things) easier to debug TF code. You can read more details at https://www.tensorflow.org/guide/eager.\n",
    "\n",
    "The main difference between the TF 1.x and 2.0 approach is that the 2.0 approach doesn't make use of `tf.Session`, `tf.run`, `placeholder`, `feed_dict`. To get more details of what's different between the two version and how to convert between the two, check out the official migration guide: https://www.tensorflow.org/alpha/guide/migration_guide\n",
    "\n",
    "Later, in the rest of this notebook we'll focus on this new, simpler approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uRHCjok6bjbp",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### TensorFlow warmup: Flatten Function\n",
    "\n",
    "We can see this in action by defining a simple `flatten` function that will reshape image data for use in a fully-connected network.\n",
    "\n",
    "In TensorFlow, data for convolutional feature maps is typically stored in a Tensor of shape N x H x W x C where:\n",
    "\n",
    "- N is the number of datapoints (minibatch size)\n",
    "- H is the height of the feature map\n",
    "- W is the width of the feature map\n",
    "- C is the number of channels in the feature map\n",
    "\n",
    "This is the right way to represent the data when using convolutional neural networks (we will explore CNNs in a future assignment). When we use fully connected linear/affine layers to process the image, however, we want each datapoint to be represented by a single vector -- it's no longer useful to segregate the different channels, rows, and columns of the data. So, we use a \"flatten\" operation to collapse the `H x W x C` values per representation into a single long vector. \n",
    "\n",
    "Notice the `tf.reshape` call has the target shape as `(N, -1)`, meaning it will reshape/keep the first dimension to be N, and then infer as necessary what the second dimension is in the output, so we can collapse the remaining dimensions from the input properly.\n",
    "\n",
    "**NOTE**: TensorFlow and PyTorch differ on the default Tensor layout; TensorFlow uses N x H x W x C but PyTorch uses N x C x H x W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJcQM_y7bjbr",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    - TensorFlow Tensor of shape (N, D1, ..., DM)\n",
    "    \n",
    "    Output:\n",
    "    - TensorFlow Tensor of shape (N, D1 * ... * DM)\n",
    "    \"\"\"\n",
    "    N = tf.shape(x)[0]\n",
    "    return tf.reshape(x, (N, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "0EwjmX68bjbv",
    "outputId": "2a120822-59f5-45c8-9e18-215f7f640db6",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_np:\n",
      " [[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]] \n",
      "\n",
      "x_flat_np:\n",
      " tf.Tensor(\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      " [12 13 14 15 16 17 18 19 20 21 22 23]], shape=(2, 12), dtype=int64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_flatten():\n",
    "    # Construct concrete values of the input data x using numpy\n",
    "    x_np = np.arange(24).reshape((2, 3, 4))\n",
    "    print('x_np:\\n', x_np, '\\n')\n",
    "    # Compute a concrete output value.\n",
    "    x_flat_np = flatten(x_np)\n",
    "    print('x_flat_np:\\n', x_flat_np, '\\n')\n",
    "\n",
    "test_flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "482-htB0bjb2"
   },
   "source": [
    "### Barebones TensorFlow: Define a Two-Layer Network\n",
    "We will now implement our first neural network with TensorFlow: a fully-connected ReLU network with two hidden layers and no biases on the CIFAR10 dataset. For now we will use only low-level TensorFlow operators to define the network; later we will see how to use the higher-level abstractions provided by `tf.keras` to simplify the process.\n",
    "\n",
    "We will define the forward pass of the network in the function `two_layer_fc`; this will accept TensorFlow Tensors for the inputs and weights of the network, and return a TensorFlow Tensor for the scores. \n",
    "\n",
    "After defining the network architecture in the `two_layer_fc` function, we will test the implementation by checking the shape of the output.\n",
    "\n",
    "**It's important that you read and understand this implementation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GG5NJkBabjb-",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def two_layer_fc(x, params):\n",
    "    \"\"\"\n",
    "    A fully-connected neural network; the architecture is:\n",
    "    fully-connected layer -> ReLU -> fully connected layer.\n",
    "    Note that we only need to define the forward pass here; TensorFlow will take\n",
    "    care of computing the gradients for us.\n",
    "    \n",
    "    The input to the network will be a minibatch of data, of shape\n",
    "    (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units,\n",
    "    and the output layer will produce scores for C classes.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A TensorFlow Tensor of shape (N, d1, ..., dM) giving a minibatch of\n",
    "      input data.\n",
    "    - params: A list [w1, w2] of TensorFlow Tensors giving weights for the\n",
    "      network, where w1 has shape (D, H) and w2 has shape (H, C).\n",
    "    \n",
    "    Returns:\n",
    "    - scores: A TensorFlow Tensor of shape (N, C) giving classification scores\n",
    "      for the input data x.\n",
    "    \"\"\"\n",
    "    w1, w2 = params                   # Unpack the parameters\n",
    "    x = flatten(x)                    # Flatten the input; now x has shape (N, D)\n",
    "    h = tf.nn.relu(tf.matmul(x, w1))  # Hidden layer: h has shape (N, H)\n",
    "    scores = tf.matmul(h, w2)         # Compute scores of shape (N, C)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "NCJkmyDLbjcC",
    "outputId": "1a41baa5-b04c-4fd5-db48-aaf51962930c",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 42)\n",
      "(42, 10)\n",
      "(64, 32, 32, 3)\n",
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_test():\n",
    "    hidden_layer_size = 42\n",
    "\n",
    "    # Scoping our TF operations under a tf.device context manager \n",
    "    # lets us tell TensorFlow where we want these Tensors to be\n",
    "    # multiplied and/or operated on, e.g. on a CPU or a GPU.\n",
    "    with tf.device(device):        \n",
    "        x = tf.zeros((64, 32, 32, 3))\n",
    "        w1 = tf.zeros((32 * 32 * 3, hidden_layer_size))\n",
    "        w2 = tf.zeros((hidden_layer_size, 10))\n",
    "        print(w1.shape)\n",
    "        print(w2.shape)\n",
    "        print(x.shape)\n",
    "\n",
    "        # Call our two_layer_fc function for the forward pass of the network.\n",
    "        scores = two_layer_fc(x, [w1, w2])\n",
    "\n",
    "    print(scores.shape)\n",
    "\n",
    "two_layer_fc_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gy9ngW0-9_02"
   },
   "outputs": [],
   "source": [
    "def three_layer_fc(x, params):\n",
    "    \"\"\"\n",
    "    A fully-connected neural network; the architecture is:\n",
    "    fully-connected layer -> ReLU -> fully connected layer.\n",
    "    Note that we only need to define the forward pass here; TensorFlow will take\n",
    "    care of computing the gradients for us.\n",
    "    \n",
    "    The input to the network will be a minibatch of data, of shape\n",
    "    (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units,\n",
    "    and the output layer will produce scores for C classes.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A TensorFlow Tensor of shape (N, d1, ..., dM) giving a minibatch of\n",
    "      input data.\n",
    "    - params: A list [w1, w2] of TensorFlow Tensors giving weights for the\n",
    "      network, where w1 has shape (D, H) and w2 has shape (H, C).\n",
    "    \n",
    "    Returns:\n",
    "    - scores: A TensorFlow Tensor of shape (N, C) giving classification scores\n",
    "      for the input data x.\n",
    "    \"\"\"\n",
    "    w1, w2, w3 = params                   # Unpack the parameters\n",
    "    x = flatten(x)                    # Flatten the input; now x has shape (N, D)\n",
    "    h1 = tf.nn.relu(tf.matmul(x, w1))  # Hidden layer: h has shape (N, H)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2))\n",
    "    scores = tf.matmul(h2, w3)         # Compute scores of shape (N, C)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RMs3IrG5-c74",
    "outputId": "24e7a301-05e1-495b-ca7f-21811de67067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def three_layer_fc_test():\n",
    "    hidden_layer_size = 42\n",
    "\n",
    "    # Scoping our TF operations under a tf.device context manager \n",
    "    # lets us tell TensorFlow where we want these Tensors to be\n",
    "    # multiplied and/or operated on, e.g. on a CPU or a GPU.\n",
    "    with tf.device(device):        \n",
    "        x = tf.zeros((64, 32, 32, 3))\n",
    "        w1 = tf.zeros((32 * 32 * 3, hidden_layer_size))        \n",
    "        w2 = tf.zeros((hidden_layer_size, hidden_layer_size))\n",
    "        w3 = tf.zeros((hidden_layer_size, 10))\n",
    "        \n",
    "        # Call our two_layer_fc function for the forward pass of the network.\n",
    "        scores = three_layer_fc(x, [w1, w2, w3])\n",
    "\n",
    "    print(scores.shape)\n",
    "\n",
    "three_layer_fc_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3m0i_ZFZaz8g"
   },
   "outputs": [],
   "source": [
    "def three_layer_convnet(x, params):\n",
    "    \"\"\"\n",
    "    A three-layer convolutional network with the architecture described above.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: A TensorFlow Tensor of shape (N, H, W, 3) giving a minibatch of images\n",
    "    - params: A list of TensorFlow Tensors giving the weights and biases for the\n",
    "      network; should contain the following:\n",
    "      - conv_w1: TensorFlow Tensor of shape (KH1, KW1, 3, channel_1) giving\n",
    "        weights for the first convolutional layer.\n",
    "      - conv_b1: TensorFlow Tensor of shape (channel_1,) giving biases for the\n",
    "        first convolutional layer.\n",
    "      - conv_w2: TensorFlow Tensor of shape (KH2, KW2, channel_1, channel_2)\n",
    "        giving weights for the second convolutional layer\n",
    "      - conv_b2: TensorFlow Tensor of shape (channel_2,) giving biases for the\n",
    "        second convolutional layer.\n",
    "      - fc_w: TensorFlow Tensor giving weights for the fully-connected layer.\n",
    "        Can you figure out what the shape should be?\n",
    "      - fc_b: TensorFlow Tensor giving biases for the fully-connected layer.\n",
    "        Can you figure out what the shape should be?\n",
    "    \"\"\"\n",
    "    conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params\n",
    "    scores = None\n",
    "    ############################################################################\n",
    "    # TODO: Implement the forward pass for the three-layer ConvNet.            #\n",
    "    ############################################################################\n",
    "    paddings = tf.constant([[0,0], [2,2], [2,2], [0,0]])\n",
    "    x = tf.pad(x, paddings, 'CONSTANT')\n",
    "    conv1 = tf.nn.conv2d(x, conv_w1, strides=[1,1,1,1], padding=\"VALID\")+conv_b1\n",
    "\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    paddings = tf.constant([[0,0], [1,1], [1,1], [0,0]])\n",
    "    conv1 = tf.pad(conv1, paddings, 'CONSTANT')\n",
    "    conv2 = tf.nn.conv2d(conv1, conv_w2, strides=[1,1,1,1], padding=\"VALID\")+conv_b2\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    \n",
    "    relu2 = flatten(relu2)\n",
    "    scores = tf.matmul(relu2, fc_w) + fc_b\n",
    "    ############################################################################\n",
    "    #                              END OF YOUR CODE                            #\n",
    "    ############################################################################\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5Xcy8zFEcmO5",
    "outputId": "56e013c6-e624-4e77-85c8-647dcdabe4e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 10)\n"
     ]
    }
   ],
   "source": [
    "def three_layer_convnet_test():\n",
    "   # ops.reset_default_graph()   \n",
    "\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((74, 32, 32, 3))\n",
    "        conv_w1 = tf.zeros((5, 5, 3, 6))\n",
    "        conv_b1 = tf.zeros((6,))\n",
    "        conv_w2 = tf.zeros((3, 3, 6, 9))\n",
    "        conv_b2 = tf.zeros((9,))\n",
    "        fc_w = tf.zeros((32 * 32 * 9, 10))\n",
    "        fc_b = tf.zeros((10,))\n",
    "        params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n",
    "        scores = three_layer_convnet(x, params)\n",
    "        print(scores.shape)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    three_layer_convnet_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KLfyfEBGbjcH"
   },
   "source": [
    "### Barebones TensorFlow: Training Step\n",
    "\n",
    "We now define the `training_step` function performs a single training step. This will take three basic steps:\n",
    "\n",
    "1. Compute the loss\n",
    "2. Compute the gradient of the loss with respect to all network weights\n",
    "3. Make a weight update step using (stochastic) gradient descent.\n",
    "\n",
    "\n",
    "We need to use a few new TensorFlow functions to do all of this:\n",
    "- For computing the cross-entropy loss we'll use `tf.nn.sparse_softmax_cross_entropy_with_logits`: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\n",
    "\n",
    "- For averaging the loss across a minibatch of data we'll use `tf.reduce_mean`:\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/reduce_mean\n",
    "\n",
    "- For computing gradients of the loss with respect to the weights we'll use `tf.GradientTape` (useful for Eager execution):  https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape\n",
    "\n",
    "- We'll mutate the weight values stored in a TensorFlow Tensor using `tf.assign_sub` (\"sub\" is for subtraction): https://www.tensorflow.org/api_docs/python/tf/assign_sub \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hHy8VqUmbjc1",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def training_step(model_fn, x, y, params, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        scores = model_fn(x, params) # Forward pass of the model\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "        total_loss = tf.reduce_mean(loss)\n",
    "        grad_params = tape.gradient(total_loss, params)\n",
    "\n",
    "        # Make a vanilla gradient descent step on all of the model parameters\n",
    "        # Manually update the weights using assign_sub()\n",
    "        for w, grad_w in zip(params, grad_params):\n",
    "            w.assign_sub(learning_rate * grad_w)\n",
    "                        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGbhKkvkbjc6",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def train_part2(model_fn, init_fn, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_fn: A Python function that performs the forward pass of the model\n",
    "      using TensorFlow; it should have the following signature:\n",
    "      scores = model_fn(x, params) where x is a TensorFlow Tensor giving a\n",
    "      minibatch of image data, params is a list of TensorFlow Tensors holding\n",
    "      the model weights, and scores is a TensorFlow Tensor of shape (N, C)\n",
    "      giving scores for all elements of x.\n",
    "    - init_fn: A Python function that initializes the parameters of the model.\n",
    "      It should have the signature params = init_fn() where params is a list\n",
    "      of TensorFlow Tensors holding the (randomly initialized) weights of the\n",
    "      model.\n",
    "    - learning_rate: Python float giving the learning rate to use for SGD.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    params = init_fn()  # Initialize the model parameters            \n",
    "        \n",
    "    for t, (x_np, y_np) in enumerate(train_dset):\n",
    "        # Run the graph on a batch of training data.\n",
    "        loss = training_step(model_fn, x_np, y_np, params, learning_rate)\n",
    "        \n",
    "        # Periodically print the loss and check accuracy on the val set.\n",
    "        if t % print_every == 0:\n",
    "            print('Iteration %d, loss = %.4f' % (t, loss))\n",
    "            check_accuracy(val_dset, x_np, model_fn, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4VVXwNobjc-",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def check_accuracy(dset, x, model_fn, params):\n",
    "    \"\"\"\n",
    "    Check accuracy on a classification model, e.g. for validation.\n",
    "    \n",
    "    Inputs:\n",
    "    - dset: A Dataset object against which to check accuracy\n",
    "    - x: A TensorFlow placeholder Tensor where input images should be fed\n",
    "    - model_fn: the Model we will be calling to make predictions on x\n",
    "    - params: parameters for the model_fn to work with\n",
    "      \n",
    "    Returns: Nothing, but prints the accuracy of the model\n",
    "    \"\"\"\n",
    "    num_correct, num_samples = 0, 0\n",
    "    for x_batch, y_batch in dset:\n",
    "        scores_np = model_fn(x_batch, params).numpy()\n",
    "        y_pred = scores_np.argmax(axis=1)\n",
    "        num_samples += x_batch.shape[0]\n",
    "        num_correct += (y_pred == y_batch).sum()\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PeFSlSW_bjdB"
   },
   "source": [
    "### Barebones TensorFlow: Initialization\n",
    "We'll use the following utility method to initialize the weight matrices for our models using Kaiming's normalization method.\n",
    "\n",
    "[1] He et al, *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n",
    "*, ICCV 2015, https://arxiv.org/abs/1502.01852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CsX6AIdvbjdD"
   },
   "outputs": [],
   "source": [
    "def create_matrix_with_kaiming_normal(shape):\n",
    "    if len(shape) == 2:\n",
    "        fan_in, fan_out = shape[0], shape[1]\n",
    "    elif len(shape) == 4:\n",
    "        fan_in, fan_out = np.prod(shape[:3]), shape[3]\n",
    "    return tf.keras.backend.random_normal(shape) * np.sqrt(2.0 / fan_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkwcKnpBbjdJ"
   },
   "source": [
    "### Barebones TensorFlow: Train a Two-Layer Network\n",
    "We are finally ready to use all of the pieces defined above to train a two-layer fully-connected network on CIFAR-10.\n",
    "\n",
    "We just need to define a function to initialize the weights of the model, and call `train_part2`.\n",
    "\n",
    "Defining the weights of the network introduces another important piece of TensorFlow API: `tf.Variable`. A TensorFlow Variable is a Tensor whose value is stored in the graph and persists across runs of the computational graph; however unlike constants defined with `tf.zeros` or `tf.random_normal`, the values of a Variable can be mutated as the graph runs; these mutations will persist across graph runs. Learnable parameters of the network are usually stored in Variables.\n",
    "\n",
    "Without any hyperparameter tuning, you should achieve validation accuracies above 40% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "N_xzPfLubjdL",
    "outputId": "a1ab91dc-9d19-481e-cbeb-bfdc6730e0ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.7569\n",
      "Got 126 / 1000 correct (12.60%)\n",
      "Iteration 100, loss = 1.8546\n",
      "Got 392 / 1000 correct (39.20%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6dea6d50b979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrain_part2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwo_layer_fc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwo_layer_fc_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-62fec118ede2>\u001b[0m in \u001b[0;36mtrain_part2\u001b[0;34m(model_fn, init_fn, learning_rate)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_np\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Run the graph on a batch of training data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Periodically print the loss and check accuracy on the val set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-5d6c4cba53d1>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(model_fn, x, y, params, learning_rate)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mgrad_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Make a vanilla gradient descent step on all of the model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1573\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_MatMulGradAgainstFirstOnly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1574\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1575\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_MatMulGradAgainstSecondOnly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1576\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m     \u001b[0;31m# No gradient skipping, so do the full gradient computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGradAgainstSecondOnly\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1554\u001b[0m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1556\u001b[0;31m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1557\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6110\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6111\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6112\u001b[0;31m         transpose_a, \"transpose_b\", transpose_b)\n\u001b[0m\u001b[1;32m   6113\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6114\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def two_layer_fc_init():\n",
    "    \"\"\"\n",
    "    Initialize the weights of a two-layer network, for use with the\n",
    "    two_layer_network function defined above. \n",
    "    You can use the `create_matrix_with_kaiming_normal` helper!\n",
    "    \n",
    "    Inputs: None\n",
    "    \n",
    "    Returns: A list of:\n",
    "    - w1: TensorFlow tf.Variable giving the weights for the first layer\n",
    "    - w2: TensorFlow tf.Variable giving the weights for the second layer\n",
    "    \"\"\"\n",
    "    \n",
    "    hidden_layer_size = 4000\n",
    "    w1 = tf.Variable(create_matrix_with_kaiming_normal((3 * 32 * 32, 4000)))\n",
    "    w2 = tf.Variable(create_matrix_with_kaiming_normal((4000, 10)))\n",
    "    return [w1, w2]\n",
    "\n",
    "learning_rate = 1e-2\n",
    "train_part2(two_layer_fc, two_layer_fc_init, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "ysTKDRPEA9TO",
    "outputId": "504d3560-85df-4cc7-c348-8056f253a2ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.9402\n",
      "Got 140 / 1000 correct (14.00%)\n",
      "Iteration 100, loss = 1.8079\n",
      "Got 420 / 1000 correct (42.00%)\n",
      "Iteration 200, loss = 1.4307\n",
      "Got 425 / 1000 correct (42.50%)\n",
      "Iteration 300, loss = 1.7175\n",
      "Got 402 / 1000 correct (40.20%)\n",
      "Iteration 400, loss = 1.6054\n",
      "Got 453 / 1000 correct (45.30%)\n",
      "Iteration 500, loss = 1.7504\n",
      "Got 458 / 1000 correct (45.80%)\n",
      "Iteration 600, loss = 1.7125\n",
      "Got 472 / 1000 correct (47.20%)\n",
      "Iteration 700, loss = 1.6645\n",
      "Got 443 / 1000 correct (44.30%)\n"
     ]
    }
   ],
   "source": [
    "def three_layer_fc_init():\n",
    "    \"\"\"\n",
    "    Initialize the weights of a three-layer network, for use with the\n",
    "    two_layer_network function defined above. \n",
    "    You can use the `create_matrix_with_kaiming_normal` helper!\n",
    "    \n",
    "    Inputs: None\n",
    "    \n",
    "    Returns: A list of:\n",
    "    - w1: TensorFlow tf.Variable giving the weights for the first layer\n",
    "    - w2: TensorFlow tf.Variable giving the weights for the second layer\n",
    "    \"\"\"\n",
    "    \n",
    "    hidden_layer_size = 4000\n",
    "    w1 = tf.Variable(create_matrix_with_kaiming_normal((3 * 32 * 32, 4000)))\n",
    "    w2 = tf.Variable(create_matrix_with_kaiming_normal((4000, 4000)))\n",
    "    w3 = tf.Variable(create_matrix_with_kaiming_normal((4000, 10)))\n",
    "    return [w1, w2, w3]\n",
    "\n",
    "learning_rate = 1e-2\n",
    "train_part2(three_layer_fc, three_layer_fc_init, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "MymPQ1QKvEQl",
    "outputId": "91b1d73c-2cd0-456e-d36e-aa4553675e53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 3.5214\n",
      "Got 119 / 1000 correct (11.90%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-2738604986e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrain_part2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthree_layer_convnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthree_layer_conv_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-62fec118ede2>\u001b[0m in \u001b[0;36mtrain_part2\u001b[0;34m(model_fn, init_fn, learning_rate)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_np\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Run the graph on a batch of training data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Periodically print the loss and check accuracy on the val set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-5d6c4cba53d1>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(model_fn, x, y, params, learning_rate)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mgrad_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Make a vanilla gradient descent step on all of the model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    604\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m           data_format=data_format)\n\u001b[0m\u001b[1;32m    607\u001b[0m   ]\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[0;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m         explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1188\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def three_layer_conv_init():\n",
    "    \"\"\"\n",
    "    Initialize the weights of a three-layer network, for use with the\n",
    "    two_layer_network function defined above. \n",
    "    You can use the `create_matrix_with_kaiming_normal` helper!\n",
    "    \n",
    "    Inputs: None\n",
    "    \n",
    "    Returns: A list of:\n",
    "    - w1: TensorFlow tf.Variable giving the weights for the first layer\n",
    "    - w2: TensorFlow tf.Variable giving the weights for the second layer\n",
    "    \"\"\"\n",
    "    conv_w1 = tf.Variable(create_matrix_with_kaiming_normal([5, 5, 3, 32]))\n",
    "    conv_b1 = tf.Variable(np.zeros([32]), dtype=tf.float32)\n",
    "    conv_w2 = tf.Variable(create_matrix_with_kaiming_normal([3, 3, 32, 16]))\n",
    "    conv_b2 = tf.Variable(np.zeros([16]), dtype=tf.float32)\n",
    "    fc_w = tf.Variable(create_matrix_with_kaiming_normal([32*32*16,10]))\n",
    "    fc_b = tf.Variable(np.zeros([10]), dtype=tf.float32)\n",
    "    params = (conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b)\n",
    "    return params\n",
    "\n",
    "learning_rate = 3e-3\n",
    "train_part2(three_layer_convnet, three_layer_conv_init, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdFs0Zs1bjdQ",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# Part III: Keras Model Subclassing API\n",
    "\n",
    "Implementing a neural network using the low-level TensorFlow API is a good way to understand how TensorFlow works, but it's a little inconvenient - we had to manually keep track of all Tensors holding learnable parameters. This was fine for a small network, but could quickly become unweildy for a large complex model.\n",
    "\n",
    "Fortunately TensorFlow 2.0 provides higher-level APIs such as `tf.keras` which make it easy to build models out of modular, object-oriented layers. Further, TensorFlow 2.0 uses eager execution that evaluates operations immediately, without explicitly constructing any computational graphs. This makes it easy to write and debug models, and reduces the boilerplate code.\n",
    "\n",
    "In this part of the notebook we will define neural network models using the `tf.keras.Model` API. To implement your own model, you need to do the following:\n",
    "\n",
    "1. Define a new class which subclasses `tf.keras.Model`. Give your class an intuitive name that describes it, like `TwoLayerFC` or `ThreeLayerConvNet`.\n",
    "2. In the initializer `__init__()` for your new class, define all the layers you need as class attributes. The `tf.keras.layers` package provides many common neural-network layers, like `tf.keras.layers.Dense` for fully-connected layers and `tf.keras.layers.Conv2D` for convolutional layers. Under the hood, these layers will construct `Variable` Tensors for any learnable parameters. **Warning**: Don't forget to call `super(YourModelName, self).__init__()` as the first line in your initializer!\n",
    "3. Implement the `call()` method for your class; this implements the forward pass of your model, and defines the *connectivity* of your network. Layers defined in `__init__()` implement `__call__()` so they can be used as function objects that transform input Tensors into output Tensors. Don't define any new layers in `call()`; any layers you want to use in the forward pass should be defined in `__init__()`.\n",
    "\n",
    "After you define your `tf.keras.Model` subclass, you can instantiate it and use it like the model functions from Part II.\n",
    "\n",
    "### Keras Model Subclassing API: Two-Layer Network\n",
    "\n",
    "Here is a concrete example of using the `tf.keras.Model` API to define a two-layer network. There are a few new bits of API to be aware of here:\n",
    "\n",
    "We use an `Initializer` object to set up the initial values of the learnable parameters of the layers; in particular `tf.initializers.VarianceScaling` gives behavior similar to the Kaiming initialization method we used in Part II. You can read more about it here: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/initializers/VarianceScaling\n",
    "\n",
    "We construct `tf.keras.layers.Dense` objects to represent the two fully-connected layers of the model. In addition to multiplying their input by a weight matrix and adding a bias vector, these layer can also apply a nonlinearity for you. For the first layer we specify a ReLU activation function by passing `activation='relu'` to the constructor; the second layer uses softmax activation function. Finally, we use `tf.keras.layers.Flatten` to flatten the output from the previous fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VDGgwSqKbjdR",
    "outputId": "0d155eed-0d8b-4f2f-da39-f26487332ad0",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerFC(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(TwoLayerFC, self).__init__()        \n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def test_TwoLayerFC():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = TwoLayerFC(hidden_size, num_classes)\n",
    "    with tf.device(device):\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "        \n",
    "test_TwoLayerFC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MobV_SJcFJQx",
    "outputId": "179d10bf-fe5c-42c9-cf2a-9765030aa6f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]\n",
      " [0.49180257 0.5180142  0.48530337 0.5062271  0.53295815 0.54027903\n",
      "  0.5014939  0.48024526 0.5195698  0.4236802 ]], shape=(64, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class ThreeLayerFC(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(ThreeLayerFC, self).__init__()        \n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(num_classes, activation='sigmoid',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def test_ThreeLayerFC():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = ThreeLayerFC(hidden_size, num_classes)\n",
    "    with tf.device(device):\n",
    "        scores = model(x)\n",
    "        print(scores)\n",
    "        \n",
    "test_ThreeLayerFC()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GLMc6w2EUdhy",
    "outputId": "cb75d921-b868-4232-cca0-997459ee253c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(64, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class ThreeLayerConv(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2, num_classes):\n",
    "        super(ThreeLayerConv, self).__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Implement the __init__ method for a three-layer ConvNet. You   #\n",
    "        # should instantiate layer objects to be used in the forward pass.     #\n",
    "        ########################################################################   \n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)       \n",
    "        self.conv1 = tf.keras.layers.Conv2D(channel_1, [5,5], [1,1], padding='valid',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      activation=tf.nn.relu)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(channel_2, [3,3], [1,1], padding='valid',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      activation=tf.nn.relu)\n",
    "        self.fc = tf.keras.layers.Dense(num_classes, kernel_initializer=initializer)\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward pass for a three-layer ConvNet. You      #\n",
    "        # should use the layer objects defined in the __init__ method.         #\n",
    "        ########################################################################\n",
    "        padding = tf.constant([[0,0],[2,2],[2,2],[0,0]])\n",
    "        x = tf.pad(x, padding, 'CONSTANT')\n",
    "        x = self.conv1(x)\n",
    "        padding = tf.constant([[0,0],[1,1],[1,1],[0,0]])\n",
    "        x = tf.pad(x, padding, 'CONSTANT')\n",
    "        x = self.conv2(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        scores = self.fc(x)\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################        \n",
    "        return scores\n",
    "      \n",
    "def test_ThreeLayerConv():    \n",
    "    channel_1, channel_2, num_classes = 12, 8, 10\n",
    "    model = ThreeLayerConv(channel_1, channel_2, num_classes)\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, 3, 32, 32))\n",
    "        scores = model(x) \n",
    "        print(scores)\n",
    "\n",
    "test_ThreeLayerConv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XBYW3dpWbjdW"
   },
   "source": [
    "### Keras Model Subclassing API: Eager Training\n",
    "\n",
    "While keras models have a builtin training loop (using the `model.fit`), sometimes you need more customization. Here's an example, of a training loop implemented with eager execution.\n",
    "\n",
    "In particular, notice `tf.GradientTape`. Automatic differentiation is used in the backend for implementing backpropagation in frameworks like TensorFlow. During eager execution, `tf.GradientTape` is used to trace operations for computing gradients later. A particular `tf.GradientTape` can only compute one gradient; subsequent calls to tape will throw a runtime error. \n",
    "\n",
    "TensorFlow 2.0 ships with easy-to-use built-in metrics under `tf.keras.metrics` module. Each metric is an object, and we can use `update_state()` to add observations and `reset_state()` to clear all observations. We can get the current result of a metric by calling `result()` on the metric object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WvDKsyKbjer",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during trainingn\n",
    "    \"\"\"    \n",
    "    with tf.device(device):\n",
    "        training_start_time = datetime.datetime.now()\n",
    "\n",
    "        # Compute the loss like we did in Part II\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        \n",
    "        model = model_init_fn()\n",
    "        optimizer = optimizer_init_fn()\n",
    "        \n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "    \n",
    "        val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "        \n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\n",
    "            train_loss.reset_states()\n",
    "            train_accuracy.reset_states()\n",
    "            \n",
    "            for x_np, y_np in train_dset:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    \n",
    "                    # Use the model function to build the forward pass.\n",
    "                    scores = model(x_np, training=is_training)\n",
    "                    loss = loss_fn(y_np, scores)\n",
    "      \n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                    \n",
    "                    # Update the metrics\n",
    "                    train_loss.update_state(loss)\n",
    "                    train_accuracy.update_state(y_np, scores)\n",
    "                    \n",
    "                    if t % print_every == 0:\n",
    "                        val_loss.reset_states()\n",
    "                        val_accuracy.reset_states()\n",
    "                        for test_x, test_y in val_dset:\n",
    "                            # During validation at end of epoch, training set to False\n",
    "                            prediction = model(test_x, training=False)\n",
    "                            t_loss = loss_fn(test_y, prediction)\n",
    "\n",
    "                            val_loss.update_state(t_loss)\n",
    "                            val_accuracy.update_state(test_y, prediction)\n",
    "                        \n",
    "                        template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'\n",
    "                        print (template.format(t, epoch+1,\n",
    "                                             train_loss.result(),\n",
    "                                             train_accuracy.result()*100,\n",
    "                                             val_loss.result(),\n",
    "                                             val_accuracy.result()*100))\n",
    "                    t += 1\n",
    "    training_end_time = datetime.datetime.now()\n",
    "\n",
    "    time_to_train = training_end_time - training_start_time\n",
    "  \n",
    "    dateTimeDifferenceInHours = time_to_train.total_seconds()\n",
    "    print('Training took {} seconds'.format(dateTimeDifferenceInHours))                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLCfx5Hwbje4"
   },
   "source": [
    "### Keras Model Subclassing API: Train a Two-Layer Network\n",
    "We can now use the tools defined above to train a two-layer network on CIFAR-10. We define the `model_init_fn` and `optimizer_init_fn` that construct the model and optimizer respectively when called. Here we want to train the model using stochastic gradient descent with no momentum, so we construct a `tf.keras.optimizers.SGD` function; you can [read about it here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/SGD).\n",
    "\n",
    "Without any hyperparameter tuning, you should achieve validation accuracies above 40% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "colab_type": "code",
    "id": "I-JrzO23bje5",
    "outputId": "4ffe2bdb-4a3d-4457-c0eb-b21bb74e557a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8611b3b77b39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_part34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_init_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_init_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-59086aff6717>\u001b[0m in \u001b[0;36mtrain_part34\u001b[0;34m(model_init_fn, optimizer_init_fn, num_epochs, is_training)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_init_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_init_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-8611b3b77b39>\u001b[0m in \u001b[0;36mmodel_init_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_init_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTwoLayerFC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moptimizer_init_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TwoLayerFC' is not defined"
     ]
    }
   ],
   "source": [
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    return TwoLayerFC(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "rIPOTlWpF6hb",
    "outputId": "cee565ae-7329-4700-edb9-53f47c4de2d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.28182315826416, Accuracy: 14.0625, Val Loss: 2.3065717220306396, Val Accuracy: 10.800000190734863\n",
      "Iteration 100, Epoch 1, Loss: 2.268281936645508, Accuracy: 15.362005233764648, Val Loss: 2.251209020614624, Val Accuracy: 17.5\n",
      "Iteration 200, Epoch 1, Loss: 2.2586984634399414, Accuracy: 16.81436538696289, Val Loss: 2.2399985790252686, Val Accuracy: 19.400001525878906\n",
      "Iteration 300, Epoch 1, Loss: 2.2524163722991943, Accuracy: 17.67026710510254, Val Loss: 2.231448173522949, Val Accuracy: 21.19999885559082\n",
      "Iteration 400, Epoch 1, Loss: 2.2464656829833984, Accuracy: 18.566864013671875, Val Loss: 2.2268855571746826, Val Accuracy: 22.799999237060547\n",
      "Iteration 500, Epoch 1, Loss: 2.242558002471924, Accuracy: 19.158557891845703, Val Loss: 2.221726894378662, Val Accuracy: 24.200000762939453\n",
      "Iteration 600, Epoch 1, Loss: 2.2391881942749023, Accuracy: 19.724937438964844, Val Loss: 2.216858148574829, Val Accuracy: 24.69999885559082\n",
      "Iteration 700, Epoch 1, Loss: 2.235640525817871, Accuracy: 20.18990707397461, Val Loss: 2.2141005992889404, Val Accuracy: 25.299999237060547\n"
     ]
    }
   ],
   "source": [
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    return ThreeLayerFC(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "1xlKghRhZgX7",
    "outputId": "551e8352-64f3-41b9-cf40-8d6e66ae7e93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 10.008524894714355, Accuracy: 10.9375, Val Loss: 8.58033561706543, Val Accuracy: 7.90000057220459\n",
      "Iteration 100, Epoch 1, Loss: 9.202701568603516, Accuracy: 10.210396766662598, Val Loss: 10.24263858795166, Val Accuracy: 7.90000057220459\n",
      "Iteration 200, Epoch 1, Loss: 9.125349044799805, Accuracy: 10.1445894241333, Val Loss: 8.654484748840332, Val Accuracy: 7.90000057220459\n",
      "Iteration 300, Epoch 1, Loss: 8.839177131652832, Accuracy: 10.210755348205566, Val Loss: 8.654186248779297, Val Accuracy: 7.90000057220459\n",
      "Iteration 400, Epoch 1, Loss: 8.813014030456543, Accuracy: 10.080267906188965, Val Loss: 9.82066822052002, Val Accuracy: 7.90000057220459\n",
      "Iteration 500, Epoch 1, Loss: 8.96052360534668, Accuracy: 10.079840660095215, Val Loss: 9.845467567443848, Val Accuracy: 7.90000057220459\n",
      "Iteration 600, Epoch 1, Loss: 9.072073936462402, Accuracy: 10.134151458740234, Val Loss: 9.845132827758789, Val Accuracy: 7.90000057220459\n",
      "Iteration 700, Epoch 1, Loss: 9.158257484436035, Accuracy: 10.04368782043457, Val Loss: 9.845132827758789, Val Accuracy: 7.90000057220459\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "channel_1, channel_2, num_classes = 32, 16, 10\n",
    "\n",
    "\n",
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    return ThreeLayerFC(channel_1, channel_2, num_classes)\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    #return model(inputs)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-eM1-Eabje_"
   },
   "source": [
    "# Part IV: Keras Sequential API\n",
    "In Part III we introduced the `tf.keras.Model` API, which allows you to define models with any number of learnable layers and with arbitrary connectivity between layers.\n",
    "\n",
    "However for many models you don't need such flexibility - a lot of models can be expressed as a sequential stack of layers, with the output of each layer fed to the next layer as input. If your model fits this pattern, then there is an even easier way to define your model: using `tf.keras.Sequential`. You don't need to write any custom classes; you simply call the `tf.keras.Sequential` constructor with a list containing a sequence of layer objects.\n",
    "\n",
    "One complication with `tf.keras.Sequential` is that you must define the shape of the input to the model by passing a value to the `input_shape` of the first layer in your model.\n",
    "\n",
    "### Keras Sequential API: Two-Layer Network\n",
    "In this subsection, we will rewrite the two-layer fully-connected network using `tf.keras.Sequential`, and train it using the training loop defined above.\n",
    "\n",
    "Without any hyperparameter tuning, you should see validation accuracies above 40% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "Oa7BkXxMbjfB",
    "outputId": "8949a72f-10b5-48f0-f533-740b566e952f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 3.283254861831665, Accuracy: 4.6875, Val Loss: 3.265125036239624, Val Accuracy: 10.699999809265137\n",
      "Iteration 100, Epoch 1, Loss: 2.2577285766601562, Accuracy: 28.449874877929688, Val Loss: 1.907272219657898, Val Accuracy: 39.0\n",
      "Iteration 200, Epoch 1, Loss: 2.085240125656128, Accuracy: 32.45491409301758, Val Loss: 1.8180067539215088, Val Accuracy: 39.599998474121094\n",
      "Iteration 300, Epoch 1, Loss: 2.0084543228149414, Accuracy: 34.13621139526367, Val Loss: 1.8622419834136963, Val Accuracy: 37.5\n",
      "Iteration 400, Epoch 1, Loss: 1.940671682357788, Accuracy: 35.89853286743164, Val Loss: 1.7754285335540771, Val Accuracy: 41.60000228881836\n",
      "Iteration 500, Epoch 1, Loss: 1.8946212530136108, Accuracy: 36.938621520996094, Val Loss: 1.6755846738815308, Val Accuracy: 42.89999771118164\n",
      "Iteration 600, Epoch 1, Loss: 1.8617912530899048, Accuracy: 37.92377471923828, Val Loss: 1.7067407369613647, Val Accuracy: 42.599998474121094\n",
      "Iteration 700, Epoch 1, Loss: 1.8350392580032349, Accuracy: 38.5654411315918, Val Loss: 1.6428592205047607, Val Accuracy: 44.80000305175781\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    input_shape = (32, 32, 3)\n",
    "    hidden_layer_size, num_classes = 4000, 10\n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    layers = [\n",
    "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu',\n",
    "                              kernel_initializer=initializer),\n",
    "        tf.keras.layers.BatchNormalization(axis=1),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', \n",
    "                              kernel_initializer=initializer),\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "Z4WIbMYHHab2",
    "outputId": "bedd3fab-cd1e-4219-8fb9-505ac53d1494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.290614128112793, Accuracy: 12.5, Val Loss: 2.305014133453369, Val Accuracy: 9.5\n",
      "Iteration 100, Epoch 1, Loss: 2.274724006652832, Accuracy: 11.215965270996094, Val Loss: 2.265923023223877, Val Accuracy: 13.40000057220459\n",
      "Iteration 200, Epoch 1, Loss: 2.2671642303466797, Accuracy: 12.041356086730957, Val Loss: 2.253378391265869, Val Accuracy: 15.399999618530273\n",
      "Iteration 300, Epoch 1, Loss: 2.2602460384368896, Accuracy: 13.102158546447754, Val Loss: 2.2413699626922607, Val Accuracy: 18.700000762939453\n",
      "Iteration 400, Epoch 1, Loss: 2.2535767555236816, Accuracy: 14.685940742492676, Val Loss: 2.2352731227874756, Val Accuracy: 21.799999237060547\n",
      "Iteration 500, Epoch 1, Loss: 2.248988628387451, Accuracy: 16.136476516723633, Val Loss: 2.2290380001068115, Val Accuracy: 22.30000114440918\n",
      "Iteration 600, Epoch 1, Loss: 2.244640350341797, Accuracy: 17.22389793395996, Val Loss: 2.225301504135132, Val Accuracy: 22.799999237060547\n",
      "Iteration 700, Epoch 1, Loss: 2.2410507202148438, Accuracy: 18.025588989257812, Val Loss: 2.2205655574798584, Val Accuracy: 22.600000381469727\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "def three_layer_model_init_fn():\n",
    "    input_shape = (32, 32, 3)\n",
    "    hidden_layer_size, num_classes = 4000, 10\n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    layers = [\n",
    "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu',\n",
    "                              kernel_initializer=initializer),\n",
    "        tf.keras.layers.BatchNormalization(axis=1),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', \n",
    "                              kernel_initializer=initializer),\n",
    "        tf.keras.layers.BatchNormalization(axis=1),        \n",
    "        tf.keras.layers.Dense(num_classes, activation='sigmoid', \n",
    "                              kernel_initializer=initializer),\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate) \n",
    "\n",
    "train_part34(three_layer_model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "98Tj_wt0fXCa",
    "outputId": "c28a6f97-9c86-452b-de6f-a4185ea1c1b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Iteration 0, Epoch 1, Loss: 9.314998626708984, Accuracy: 6.25, Val Loss: 6.736607551574707, Val Accuracy: 11.200000762939453\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-315a203ccbd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mtrain_part34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_init_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_init_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-2e0df7329c12>\u001b[0m in \u001b[0;36mtrain_part34\u001b[0;34m(model_init_fn, optimizer_init_fn, num_epochs, is_training)\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    594\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m           data_format=data_format),\n\u001b[0m\u001b[1;32m    597\u001b[0m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[1;32m    598\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a three-layer ConvNet using tf.keras.Sequential.         #\n",
    "    ############################################################################\n",
    "    input_shape = (32,32,3)\n",
    "    channel_1, channel_2, num_classes = 32, 16, 10\n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)   \n",
    "    layers = [\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        tf.keras.layers.Conv2D(channel_1, [5,5], [1,1], padding='same',\n",
    "                               kernel_initializer=initializer,\n",
    "                               activation=tf.nn.relu),\n",
    "        tf.keras.layers.Conv2D(channel_2, [3,3], [1,1], padding='same',\n",
    "                               kernel_initializer=initializer,\n",
    "                               activation=tf.nn.relu),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(num_classes, kernel_initializer=initializer)\n",
    "    ]\n",
    "    return tf.keras.Sequential(layers)\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "   # return model(inputs)\n",
    "\n",
    "learning_rate = 5e-4\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    #optimizer = tf.kerastrain.MomentumOptimizer(learning_rate, momentum=0.9, \n",
    "     #                                      use_nesterov=True)\n",
    "    optimizer =  tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0_GSFoz3bjfE"
   },
   "source": [
    "### Abstracting Away the Training Loop\n",
    "In the previous examples, we used a customised training loop to train models (e.g. `train_part34`). Writing your own training loop is only required if you need more flexibility and control during training your model. Alternately, you can also use  built-in APIs like `tf.keras.Model.fit()` and `tf.keras.Model.evaluate` to train and evaluate a model. Also remember to configure your model for training by calling `tf.keras.Model.compile.\n",
    "\n",
    "Without any hyperparameter tuning, you should see validation and test accuracies above 42% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GLFEC5ZzbjfF"
   },
   "outputs": [],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8-a2hCpbjfK"
   },
   "source": [
    "##  Part IV: Functional API\n",
    "### Demonstration with a Two-Layer Network \n",
    "\n",
    "In the previous section, we saw how we can use `tf.keras.Sequential` to stack layers to quickly build simple models. But this comes at the cost of losing flexibility.\n",
    "\n",
    "Often we will have to write complex models that have non-sequential data flows: a layer can have **multiple inputs and/or outputs**, such as stacking the output of 2 previous layers together to feed as input to a third! (Some examples are residual connections and dense blocks.)\n",
    "\n",
    "In such cases, we can use Keras functional API to write models with complex topologies such as:\n",
    "\n",
    " 1. Multi-input models\n",
    " 2. Multi-output models\n",
    " 3. Models with shared layers (the same layer called several times)\n",
    " 4. Models with non-sequential data flows (e.g. residual connections)\n",
    "\n",
    "Writing a model with Functional API requires us to create a `tf.keras.Model` instance and explicitly write input tensors and output tensors for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yRFq7w2objfL",
    "outputId": "e19ccda9-8ffc-436c-c693-da067a49f669",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_functional(input_shape, hidden_size, num_classes):  \n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    flattened_inputs = tf.keras.layers.Flatten()(inputs)\n",
    "    fc1_output = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                 kernel_initializer=initializer)(flattened_inputs)\n",
    "    scores = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                             kernel_initializer=initializer)(fc1_output)\n",
    "\n",
    "    # Instantiate the model given inputs and outputs.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=scores)\n",
    "    return model\n",
    "\n",
    "def test_two_layer_fc_functional():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "    input_shape = (50,)\n",
    "    \n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = two_layer_fc_functional(input_shape, hidden_size, num_classes)\n",
    "    \n",
    "    with tf.device(device):\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "        \n",
    "test_two_layer_fc_functional()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l1BXQGtPH_7c",
    "outputId": "9c03d25a-c778-47e3-8b82-29c9577d4e5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def three_layer_fc_functional(input_shape, hidden_size, num_classes):  \n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    flattened_inputs = tf.keras.layers.Flatten()(inputs)\n",
    "    fc1_output = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                 kernel_initializer=initializer)(flattened_inputs)\n",
    "    fc2_output = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                             kernel_initializer=initializer)(fc1_output)\n",
    "    scores = tf.keras.layers.Dense(num_classes, activation='sigmoid',\n",
    "                             kernel_initializer=initializer)(fc2_output)\n",
    "\n",
    "    # Instantiate the model given inputs and outputs.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=scores)\n",
    "    return model\n",
    "\n",
    "def test_three_layer_fc_functional():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "    input_shape = (50,)\n",
    "    \n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = three_layer_fc_functional(input_shape, hidden_size, num_classes)\n",
    "    \n",
    "    with tf.device(device):\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "        \n",
    "test_three_layer_fc_functional()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJlAezFybjfO"
   },
   "source": [
    "### Keras Functional API: Train a Two-Layer Network\n",
    "You can now train this two-layer network constructed using the functional API.\n",
    "\n",
    "Without any hyperparameter tuning, but you should see validation accuracies above 40% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "v24JdxRhbjfQ",
    "outputId": "7feda6db-0e98-438c-b5bd-cd392363c395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 3.084482431411743, Accuracy: 6.25, Val Loss: 2.7636258602142334, Val Accuracy: 14.0\n",
      "Iteration 700, Epoch 1, Loss: 1.8224191665649414, Accuracy: 38.69026184082031, Val Loss: 1.6211074590682983, Val Accuracy: 43.599998474121094\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    return two_layer_fc_functional(input_shape, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "bxUBlekLIklm",
    "outputId": "c7a79c8b-0bd8-48e0-9388-f90a4ae8cede"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.308790922164917, Accuracy: 9.375, Val Loss: 2.3053598403930664, Val Accuracy: 10.699999809265137\n",
      "Iteration 100, Epoch 1, Loss: 2.2764015197753906, Accuracy: 12.12871265411377, Val Loss: 2.2551395893096924, Val Accuracy: 16.599998474121094\n",
      "Iteration 200, Epoch 1, Loss: 2.260119676589966, Accuracy: 14.692163467407227, Val Loss: 2.236701011657715, Val Accuracy: 18.399999618530273\n",
      "Iteration 300, Epoch 1, Loss: 2.250509023666382, Accuracy: 15.993563652038574, Val Loss: 2.2268240451812744, Val Accuracy: 20.0\n",
      "Iteration 400, Epoch 1, Loss: 2.2429234981536865, Accuracy: 17.101776123046875, Val Loss: 2.2217795848846436, Val Accuracy: 21.0\n",
      "Iteration 500, Epoch 1, Loss: 2.2383415699005127, Accuracy: 17.521207809448242, Val Loss: 2.2170510292053223, Val Accuracy: 21.5\n",
      "Iteration 600, Epoch 1, Loss: 2.2346527576446533, Accuracy: 17.95185089111328, Val Loss: 2.211547374725342, Val Accuracy: 21.799999237060547\n",
      "Iteration 700, Epoch 1, Loss: 2.231525182723999, Accuracy: 18.275230407714844, Val Loss: 2.208641767501831, Val Accuracy: 21.899999618530273\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def three_layer_model_init_fn():\n",
    "    return three_layer_fc_functional(input_shape, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(three_layer_model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hxt1lTrcbjf4"
   },
   "source": [
    "# Part V: Tuning\n",
    "In this section, you are asked to experiment with different dense/fully connnected architectures, activation functions, weight initializations, hyperparameters, optimizers, and regularization approaches to train models on the CIFAR-10 dataset. You can use the built-in train function, the `train_part34` function from above, or implement your own training loop.\n",
    "\n",
    "Describe what you did at the end of the notebook.\n",
    "\n",
    "### Things to experiment with:\n",
    "- **Network architectures**: The network above has two layers of trainable parameters. Can you do better with a deeper network? Or maybe with a wider network? Try five different architectures and observe the performance on the validation data. Use the architectures in combinations with other hyperparameters, as outlines below. Discuss your findings.\n",
    "- **Activation functions**: In your networks, use five different activation functions, such as ReLU, leaky ReLU, parametric ReLU, ELU, MaxOut, or tanh to gain practical insights into their ability to improve accuracy. \n",
    "- **Weight initialization**: Corresponding to your activation functions, use different weight initialization schemes. Discuss your findings. What happens if you use the zero_weight initialization? \n",
    "- **Batch normalization**: Try adding batch normalization. Do your networks train faster? Does the accuracy improve?\n",
    "- **Optimizers**: Use different optimizers, including SGD, SGD with momentum, RMSprop and Adam. Use the optimizers with and without batch normalization to observe what optimizers benefit more from batch normalization, or different weight initializations schemes and what optimizers are more robust to initialization/normalization. \n",
    "- **Regularization**: Compare L2 weight regularization, with dropout, batch normalization, and data augmentation. Discuss your findings.  \n",
    "- **Model Ensemble**: Construct a model ensemble using some of your best hyperparameters as identified before, and compare the accuracy of the model assemble with the accuracy of your best individual model (based on the validation dataset). \n",
    "\n",
    "\n",
    "### NOTE: Batch Normalization / Dropout\n",
    "When you are using Batch Normalization and Dropout, remember to pass `is_training=True` if you use the `train_part34()` function. BatchNorm and Dropout layers have different behaviors at training and inference time. `training` is a specific keyword argument reserved for this purpose in any `tf.keras.Model`'s `call()` function. Read more about this here : https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization#methods\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dropout#methods\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and other hyperparameters. When doing this there are a couple important things to keep in mind: \n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eyor8-RsFvOJ"
   },
   "outputs": [],
   "source": [
    "# Fully connected 3 layer network#1 starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "vhi1zKPabjf6",
    "outputId": "4176d7ae-0fa7-4cf8-9f56-c405f6ae8f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.3004648685455322, Accuracy: 9.375, Val Loss: 2.3054885864257812, Val Accuracy: 10.0\n",
      "Iteration 700, Epoch 1, Loss: 2.2931909561157227, Accuracy: 12.73849868774414, Val Loss: 2.28623104095459, Val Accuracy: 12.800000190734863\n",
      "Iteration 1400, Epoch 2, Loss: 2.278656244277954, Accuracy: 14.62352466583252, Val Loss: 2.2751588821411133, Val Accuracy: 12.600000381469727\n",
      "Iteration 2100, Epoch 3, Loss: 2.26861834526062, Accuracy: 14.927504539489746, Val Loss: 2.2670528888702393, Val Accuracy: 13.40000057220459\n",
      "Iteration 2800, Epoch 4, Loss: 2.2591516971588135, Accuracy: 15.308150291442871, Val Loss: 2.2568225860595703, Val Accuracy: 14.0\n",
      "Iteration 3500, Epoch 5, Loss: 2.2487266063690186, Accuracy: 17.176774978637695, Val Loss: 2.2472569942474365, Val Accuracy: 16.100000381469727\n",
      "Iteration 4200, Epoch 6, Loss: 2.241701364517212, Accuracy: 17.516002655029297, Val Loss: 2.240373134613037, Val Accuracy: 17.5\n",
      "Iteration 4900, Epoch 7, Loss: 2.2363786697387695, Accuracy: 17.540983200073242, Val Loss: 2.2347183227539062, Val Accuracy: 17.700000762939453\n",
      "Iteration 5600, Epoch 8, Loss: 2.231245279312134, Accuracy: 17.54707145690918, Val Loss: 2.229764938354492, Val Accuracy: 18.19999885559082\n",
      "Iteration 6300, Epoch 9, Loss: 2.226249933242798, Accuracy: 17.548770904541016, Val Loss: 2.225375175476074, Val Accuracy: 18.099998474121094\n",
      "Iteration 7000, Epoch 10, Loss: 2.219916820526123, Accuracy: 18.238903045654297, Val Loss: 2.221649408340454, Val Accuracy: 18.099998474121094\n",
      "Training took 77.04043 seconds\n"
     ]
    }
   ],
   "source": [
    " \"\"\" Custom network with three fully connected layers. \n",
    "    Activation Functions Used: Relu, Softmax, Sigmoid\n",
    "    Optimizers Used: SGD\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = he_uniform\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.he_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(num_classes, activation='sigmoid',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.SGD(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "iNEq5PPNxHUj",
    "outputId": "88f84d2b-9478-4d7d-d6ff-b4019be66ab4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 11.420839309692383, Accuracy: 1.5625, Val Loss: 7.956015586853027, Val Accuracy: 9.09999942779541\n",
      "Iteration 700, Epoch 1, Loss: 4.972878456115723, Accuracy: 16.56785011291504, Val Loss: 5.257494926452637, Val Accuracy: 8.699999809265137\n",
      "Iteration 1400, Epoch 2, Loss: 5.2580695152282715, Accuracy: 9.97047233581543, Val Loss: 5.257494926452637, Val Accuracy: 8.699999809265137\n",
      "Iteration 2100, Epoch 3, Loss: 5.257570743560791, Accuracy: 9.943431854248047, Val Loss: 5.257494926452637, Val Accuracy: 8.699999809265137\n",
      "Iteration 2800, Epoch 4, Loss: 5.257495880126953, Accuracy: 9.872017860412598, Val Loss: 5.257494926452637, Val Accuracy: 8.699999809265137\n",
      "Iteration 3500, Epoch 5, Loss: 5.2574849128723145, Accuracy: 9.950657844543457, Val Loss: 5.257494926452637, Val Accuracy: 8.699999809265137\n",
      "Iteration 4200, Epoch 6, Loss: 5.257476329803467, Accuracy: 9.880391120910645, Val Loss: 5.257494926452637, Val Accuracy: 8.699999809265137\n",
      "Iteration 4900, Epoch 7, Loss: 5.257482528686523, Accuracy: 9.959016799926758, Val Loss: 5.257494926452637, Val Accuracy: 8.699999809265137\n",
      "Iteration 5600, Epoch 8, Loss: 5.257492542266846, Accuracy: 9.969926834106445, Val Loss: 5.257494926452637, Val Accuracy: 8.699999809265137\n",
      "Iteration 6300, Epoch 9, Loss: 5.257503032684326, Accuracy: 10.097542762756348, Val Loss: 5.257494926452637, Val Accuracy: 8.699999809265137\n",
      "Iteration 7000, Epoch 10, Loss: 5.257500648498535, Accuracy: 9.929906845092773, Val Loss: 5.257494926452637, Val Accuracy: 8.699999809265137\n",
      "Training took 219.325358 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with three fully connected layers. \n",
    "    Activation Functions Used: Relu, LeakyReLu, PreLU\n",
    "    Optimizers Used: RMSProp\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = glorot_normal (Xavier)\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_normal(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(192, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(192, activation=tf.keras.layers.LeakyReLU(),\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(192, activation=tf.keras.layers.PReLU(),\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.RMSprop(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "cogx9swt2KCF",
    "outputId": "f2e59803-86f7-4004-eb04-2cc4ddf23c49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 5.074756622314453, Accuracy: 0.0, Val Loss: 5.029781818389893, Val Accuracy: 0.7000000476837158\n",
      "Iteration 700, Epoch 1, Loss: 3.0958364009857178, Accuracy: 20.564373016357422, Val Loss: 2.169178009033203, Val Accuracy: 29.600000381469727\n",
      "Iteration 1400, Epoch 2, Loss: 2.0062339305877686, Accuracy: 32.46309280395508, Val Loss: 1.902441382408142, Val Accuracy: 33.89999771118164\n",
      "Iteration 2100, Epoch 3, Loss: 1.8462144136428833, Accuracy: 36.06930923461914, Val Loss: 1.8116933107376099, Val Accuracy: 36.89999771118164\n",
      "Iteration 2800, Epoch 4, Loss: 1.7679942846298218, Accuracy: 38.54373550415039, Val Loss: 1.7625174522399902, Val Accuracy: 38.400001525878906\n",
      "Iteration 3500, Epoch 5, Loss: 1.7198361158370972, Accuracy: 40.224544525146484, Val Loss: 1.723604440689087, Val Accuracy: 39.39999771118164\n",
      "Iteration 4200, Epoch 6, Loss: 1.6853551864624023, Accuracy: 41.45468521118164, Val Loss: 1.6874781847000122, Val Accuracy: 41.60000228881836\n",
      "Iteration 4900, Epoch 7, Loss: 1.6617162227630615, Accuracy: 42.28995895385742, Val Loss: 1.6713218688964844, Val Accuracy: 41.900001525878906\n",
      "Iteration 5600, Epoch 8, Loss: 1.6323914527893066, Accuracy: 43.325050354003906, Val Loss: 1.6538234949111938, Val Accuracy: 42.79999923706055\n",
      "Iteration 6300, Epoch 9, Loss: 1.601416826248169, Accuracy: 44.156429290771484, Val Loss: 1.62807035446167, Val Accuracy: 43.599998474121094\n",
      "Iteration 7000, Epoch 10, Loss: 1.5728274583816528, Accuracy: 45.09346008300781, Val Loss: 1.611651062965393, Val Accuracy: 44.10000228881836\n",
      "Training took 112.937228 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with three fully connected layers. \n",
    "    Activation Functions Used: Relu, ReLU, Softmax\n",
    "    Optimizers Used: SGD with momentum\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = glorot_uniform\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.SGD(learning_rate, momentum = 0.1) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "c-WECh0O7uxc",
    "outputId": "c67ac952-3b8b-434b-ea03-48d3bec96a98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 6.019235610961914, Accuracy: 0.0, Val Loss: 4.85754919052124, Val Accuracy: 5.600000381469727\n",
      "Iteration 700, Epoch 1, Loss: 1.869362473487854, Accuracy: 37.905670166015625, Val Loss: 1.6481701135635376, Val Accuracy: 46.0\n",
      "Iteration 1400, Epoch 2, Loss: 1.5243102312088013, Accuracy: 46.66584777832031, Val Loss: 1.5237269401550293, Val Accuracy: 48.79999923706055\n",
      "Iteration 2100, Epoch 3, Loss: 1.403150200843811, Accuracy: 50.917179107666016, Val Loss: 1.5113362073898315, Val Accuracy: 48.400001525878906\n",
      "Iteration 2800, Epoch 4, Loss: 1.3204530477523804, Accuracy: 53.66550827026367, Val Loss: 1.4689565896987915, Val Accuracy: 48.599998474121094\n",
      "Iteration 3500, Epoch 5, Loss: 1.254278302192688, Accuracy: 56.078372955322266, Val Loss: 1.4901604652404785, Val Accuracy: 50.599998474121094\n",
      "Iteration 4200, Epoch 6, Loss: 1.205810785293579, Accuracy: 57.5850715637207, Val Loss: 1.4509872198104858, Val Accuracy: 51.5\n",
      "Iteration 4900, Epoch 7, Loss: 1.1536744832992554, Accuracy: 59.139347076416016, Val Loss: 1.5072531700134277, Val Accuracy: 51.89999771118164\n",
      "Iteration 5600, Epoch 8, Loss: 1.108548641204834, Accuracy: 61.23822784423828, Val Loss: 1.462546706199646, Val Accuracy: 52.79999923706055\n",
      "Iteration 6300, Epoch 9, Loss: 1.0607478618621826, Accuracy: 62.85224151611328, Val Loss: 1.448974370956421, Val Accuracy: 53.79999923706055\n",
      "Iteration 7000, Epoch 10, Loss: 1.0136469602584839, Accuracy: 64.23773193359375, Val Loss: 1.5559532642364502, Val Accuracy: 50.400001525878906\n",
      "Training took 223.408527 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with three fully connected layers. \n",
    "    Activation Functions Used: ELU, ReLU, Softmax\n",
    "    Optimizers Used: Nadam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = he_uniform\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.he_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Nadam(learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "C_oqZ6o9voch",
    "outputId": "213f907b-c4fa-4852-8f9a-121514991304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 5.934971809387207, Accuracy: 0.0, Val Loss: 5.023471832275391, Val Accuracy: 4.400000095367432\n",
      "Iteration 700, Epoch 1, Loss: 1.9073517322540283, Accuracy: 37.776390075683594, Val Loss: 1.7194112539291382, Val Accuracy: 41.5\n",
      "Iteration 1400, Epoch 2, Loss: 1.5460330247879028, Accuracy: 46.14419174194336, Val Loss: 1.5238200426101685, Val Accuracy: 47.400001525878906\n",
      "Iteration 2100, Epoch 3, Loss: 1.4272615909576416, Accuracy: 50.08787536621094, Val Loss: 1.5060251951217651, Val Accuracy: 46.39999771118164\n",
      "Iteration 2800, Epoch 4, Loss: 1.3508049249649048, Accuracy: 52.75844955444336, Val Loss: 1.4731191396713257, Val Accuracy: 48.5\n",
      "Iteration 3500, Epoch 5, Loss: 1.2877105474472046, Accuracy: 54.83409881591797, Val Loss: 1.4372961521148682, Val Accuracy: 50.5\n",
      "Iteration 4200, Epoch 6, Loss: 1.2307240962982178, Accuracy: 57.10916519165039, Val Loss: 1.523315668106079, Val Accuracy: 50.0\n",
      "Iteration 4900, Epoch 7, Loss: 1.1784148216247559, Accuracy: 58.91393280029297, Val Loss: 1.538468599319458, Val Accuracy: 48.599998474121094\n",
      "Iteration 5600, Epoch 8, Loss: 1.1349284648895264, Accuracy: 60.64984130859375, Val Loss: 1.5154330730438232, Val Accuracy: 47.60000228881836\n",
      "Iteration 6300, Epoch 9, Loss: 1.0921419858932495, Accuracy: 61.68714141845703, Val Loss: 1.5105879306793213, Val Accuracy: 51.599998474121094\n",
      "Iteration 7000, Epoch 10, Loss: 1.04017972946167, Accuracy: 63.06951141357422, Val Loss: 1.5560413599014282, Val Accuracy: 50.19999694824219\n",
      "Training took 123.343701 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with three fully connected layers. \n",
    "    Activation Functions Used: Relu, ReLU, Softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = he_uniform\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.he_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 128, 128, 10\n",
    "\n",
    "model1 = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "colab_type": "code",
    "id": "8ZM_feVG94bq",
    "outputId": "82edcd14-2e0c-432b-ddc5-576ac78c1a4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CustomNet object at 0x7fba568814e0>\n",
      "Iteration 0, Epoch 1, Loss: 6.017521858215332, Accuracy: 0.0, Val Loss: 5.75267219543457, Val Accuracy: 8.299999237060547\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-a9e890297025>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mtrain_part34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_init_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_init_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-59086aff6717>\u001b[0m in \u001b[0;36mtrain_part34\u001b[0;34m(model_init_fn, optimizer_init_fn, num_epochs, is_training)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0;31m# Use the model function to build the forward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m     if (base_layer_utils.is_in_eager_or_tf_function() and\n\u001b[1;32m    801\u001b[0m         not call_context.in_call):\n\u001b[0;32m--> 802\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcall_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_clear_losses\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_layers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       for layer in trackable_layer_utils.filter_empty_layer_containers(\n\u001b[0;32m-> 1143\u001b[0;31m           self._layers):\n\u001b[0m\u001b[1;32m   1144\u001b[0m         \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/training/tracking/layer_utils.py\u001b[0m in \u001b[0;36mfilter_empty_layer_containers\u001b[0;34m(layer_list)\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mto_visit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_visit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mexisting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/util/object_identity.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with three fully connected layers. \n",
    "    Activation Functions Used: Relu, ReLU, Softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = he_uniform\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.he_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(256, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(256, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 256, 256, 10\n",
    "\n",
    "model2 = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "jMySSxkNPJVa",
    "outputId": "37d2372f-4592-42cf-c9f0-b8429ef221e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 5.545177459716797, Accuracy: 6.25, Val Loss: 5.543257713317871, Val Accuracy: 11.90000057220459\n",
      "Iteration 700, Epoch 1, Loss: 4.91896915435791, Accuracy: 10.103869438171387, Val Loss: 4.322484016418457, Val Accuracy: 9.800000190734863\n",
      "Iteration 1400, Epoch 2, Loss: 3.7813146114349365, Accuracy: 9.84990119934082, Val Loss: 3.394547939300537, Val Accuracy: 9.800000190734863\n",
      "Iteration 2100, Epoch 3, Loss: 3.0459656715393066, Accuracy: 9.902240753173828, Val Loss: 2.863523244857788, Val Accuracy: 9.800000190734863\n",
      "Iteration 2800, Epoch 4, Loss: 2.683000326156616, Accuracy: 9.862698554992676, Val Loss: 2.608543634414673, Val Accuracy: 9.800000190734863\n",
      "Iteration 3500, Epoch 5, Loss: 2.5164992809295654, Accuracy: 9.564502716064453, Val Loss: 2.485023021697998, Val Accuracy: 10.699999809265137\n",
      "Iteration 4200, Epoch 6, Loss: 2.434187889099121, Accuracy: 9.775101661682129, Val Loss: 2.419881582260132, Val Accuracy: 7.90000057220459\n",
      "Iteration 4900, Epoch 7, Loss: 2.3891916275024414, Accuracy: 9.759221076965332, Val Loss: 2.3826663494110107, Val Accuracy: 7.90000057220459\n",
      "Iteration 5600, Epoch 8, Loss: 2.36234712600708, Accuracy: 9.701883316040039, Val Loss: 2.359304666519165, Val Accuracy: 7.90000057220459\n",
      "Iteration 6300, Epoch 9, Loss: 2.3451406955718994, Accuracy: 9.636921882629395, Val Loss: 2.3437912464141846, Val Accuracy: 8.699999809265137\n",
      "Iteration 7000, Epoch 10, Loss: 2.3334999084472656, Accuracy: 9.404205322265625, Val Loss: 2.3338429927825928, Val Accuracy: 8.699999809265137\n",
      "Training took 169.0591 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with three fully connected layers and weights initialized to zero. \n",
    "    Activation Functions Used: Relu, ReLU, Softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = zero\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.Zeros()\n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(256, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(256, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AAPGvjH3GHF3"
   },
   "outputs": [],
   "source": [
    "# Fully connected 3 layer network#1 ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qo5mb7LdGMVp"
   },
   "outputs": [],
   "source": [
    "# Fully connected 4 layer network#2 starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "URFhyYEHP1qw",
    "outputId": "03f0a860-575e-4d30-f2fe-2b5e35f9c6db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 4.15327262878418, Accuracy: 3.125, Val Loss: 4.136752605438232, Val Accuracy: 5.200000286102295\n",
      "Iteration 700, Epoch 1, Loss: 3.532177448272705, Accuracy: 10.995452880859375, Val Loss: 3.063830614089966, Val Accuracy: 7.800000190734863\n",
      "Iteration 1400, Epoch 2, Loss: 2.780595541000366, Accuracy: 9.858044624328613, Val Loss: 2.616070508956909, Val Accuracy: 7.800000190734863\n",
      "Iteration 2100, Epoch 3, Loss: 2.507383346557617, Accuracy: 9.942680358886719, Val Loss: 2.4549179077148438, Val Accuracy: 7.800000190734863\n",
      "Iteration 2800, Epoch 4, Loss: 2.4077513217926025, Accuracy: 9.759374618530273, Val Loss: 2.387640953063965, Val Accuracy: 7.800000190734863\n",
      "Iteration 3500, Epoch 5, Loss: 2.3634157180786133, Accuracy: 9.847719192504883, Val Loss: 2.3544113636016846, Val Accuracy: 11.200000762939453\n",
      "Iteration 4200, Epoch 6, Loss: 2.340364456176758, Accuracy: 9.951332092285156, Val Loss: 2.335966110229492, Val Accuracy: 7.800000190734863\n",
      "Iteration 4900, Epoch 7, Loss: 2.327115297317505, Accuracy: 10.017767906188965, Val Loss: 2.3248956203460693, Val Accuracy: 7.800000190734863\n",
      "Iteration 5600, Epoch 8, Loss: 2.318979024887085, Accuracy: 9.974407196044922, Val Loss: 2.31784725189209, Val Accuracy: 7.800000190734863\n",
      "Iteration 6300, Epoch 9, Loss: 2.313720941543579, Accuracy: 10.03787899017334, Val Loss: 2.3132336139678955, Val Accuracy: 7.800000190734863\n",
      "Iteration 7000, Epoch 10, Loss: 2.310206413269043, Accuracy: 10.251913070678711, Val Loss: 2.3101911544799805, Val Accuracy: 7.800000190734863\n",
      "Training took 103.841262 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with four fully connected layers. \n",
    "    Activation Functions Used: Relu, tanh, Softmax, Sigmoid\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = he_uniform\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.he_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(32, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(32, activation='tanh',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(32, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.Dense(64, activation='sigmoid',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GG0y54IpqFjj"
   },
   "outputs": [],
   "source": [
    "\"\"\" Custom network with four fully connected layers and weights initialized to zero. \n",
    "    Activation Functions Used: Relu, tanh, Softmax, Sigmoid\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = he_uniform\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.zeros()\n",
    "        self.fc1 = tf.keras.layers.Dense(32, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(32, activation='tanh',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(32, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.Dense(64, activation='sigmoid',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "ErEs4_77RIWb",
    "outputId": "d2dfc3a3-2dd5-4b7d-a102-43f23d6d0aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 4.138031959533691, Accuracy: 1.5625, Val Loss: 4.124605178833008, Val Accuracy: 1.3000000715255737\n",
      "Iteration 700, Epoch 1, Loss: 1.9835296869277954, Accuracy: 32.883827209472656, Val Loss: 1.7953952550888062, Val Accuracy: 40.400001525878906\n",
      "Iteration 1400, Epoch 2, Loss: 1.69623863697052, Accuracy: 40.27996826171875, Val Loss: 1.6975334882736206, Val Accuracy: 43.20000076293945\n",
      "Iteration 2100, Epoch 3, Loss: 1.6118003129959106, Accuracy: 43.15751647949219, Val Loss: 1.750809907913208, Val Accuracy: 42.599998474121094\n",
      "Iteration 2800, Epoch 4, Loss: 1.5469318628311157, Accuracy: 45.39687728881836, Val Loss: 1.7856608629226685, Val Accuracy: 43.20000076293945\n",
      "Iteration 3500, Epoch 5, Loss: 1.4930534362792969, Accuracy: 47.513710021972656, Val Loss: 1.7507803440093994, Val Accuracy: 44.20000076293945\n",
      "Iteration 4200, Epoch 6, Loss: 1.4522528648376465, Accuracy: 48.98394775390625, Val Loss: 1.760956883430481, Val Accuracy: 45.89999771118164\n",
      "Iteration 4900, Epoch 7, Loss: 1.4170196056365967, Accuracy: 50.16722869873047, Val Loss: 1.8049118518829346, Val Accuracy: 45.79999923706055\n",
      "Iteration 5600, Epoch 8, Loss: 1.3791534900665283, Accuracy: 51.71066665649414, Val Loss: 1.7613943815231323, Val Accuracy: 45.400001525878906\n",
      "Iteration 6300, Epoch 9, Loss: 1.3379493951797485, Accuracy: 53.532196044921875, Val Loss: 1.8627774715423584, Val Accuracy: 44.900001525878906\n",
      "Iteration 7000, Epoch 10, Loss: 1.2953866720199585, Accuracy: 54.8469352722168, Val Loss: 1.9110000133514404, Val Accuracy: 45.10000228881836\n",
      "Training took 95.509374 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with four fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, relu, softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 32, 32, 10\n",
    "\n",
    "model3 = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "DhkYqv1tSSc5",
    "outputId": "92a0235b-0f07-4ae6-e613-345347b24086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 4.629169940948486, Accuracy: 0.0, Val Loss: 4.784705638885498, Val Accuracy: 1.3000000715255737\n",
      "Iteration 700, Epoch 1, Loss: 3.175558090209961, Accuracy: 17.981008529663086, Val Loss: 2.185238838195801, Val Accuracy: 28.5\n",
      "Iteration 1400, Epoch 2, Loss: 2.0409128665924072, Accuracy: 29.781003952026367, Val Loss: 1.9500231742858887, Val Accuracy: 33.39999771118164\n",
      "Iteration 2100, Epoch 3, Loss: 1.89655339717865, Accuracy: 33.460567474365234, Val Loss: 1.8785152435302734, Val Accuracy: 35.10000228881836\n",
      "Iteration 2800, Epoch 4, Loss: 1.8248779773712158, Accuracy: 35.744903564453125, Val Loss: 1.837938666343689, Val Accuracy: 36.599998474121094\n",
      "Iteration 3500, Epoch 5, Loss: 1.7790374755859375, Accuracy: 37.39273452758789, Val Loss: 1.8019938468933105, Val Accuracy: 37.400001525878906\n",
      "Iteration 4200, Epoch 6, Loss: 1.7467286586761475, Accuracy: 38.53605270385742, Val Loss: 1.7713929414749146, Val Accuracy: 39.5\n",
      "Iteration 4900, Epoch 7, Loss: 1.7236790657043457, Accuracy: 39.39549255371094, Val Loss: 1.7547498941421509, Val Accuracy: 39.599998474121094\n",
      "Iteration 5600, Epoch 8, Loss: 1.6969155073165894, Accuracy: 40.39617919921875, Val Loss: 1.7369425296783447, Val Accuracy: 40.29999923706055\n",
      "Iteration 6300, Epoch 9, Loss: 1.6691744327545166, Accuracy: 40.89595413208008, Val Loss: 1.7161520719528198, Val Accuracy: 42.0\n",
      "Iteration 7000, Epoch 10, Loss: 1.6436539888381958, Accuracy: 41.26752471923828, Val Loss: 1.6968117952346802, Val Accuracy: 42.599998474121094\n",
      "Training took 100.010334 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with four fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, relu, softmax\n",
    "    Optimizers Used: SGD\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = glorot_uniform(xavier)\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(num_classes, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 96, 96, 10\n",
    "\n",
    "model4 = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.SGD(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "tkmPvWijS0ZB",
    "outputId": "3f2018c3-322a-4b48-c084-4fd54640d250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 4.622900009155273, Accuracy: 0.0, Val Loss: 4.652817249298096, Val Accuracy: 0.7000000476837158\n",
      "Iteration 700, Epoch 1, Loss: 3.3350508213043213, Accuracy: 16.17778205871582, Val Loss: 2.2686514854431152, Val Accuracy: 26.399999618530273\n",
      "Iteration 1400, Epoch 2, Loss: 2.081228256225586, Accuracy: 28.67618179321289, Val Loss: 1.9813309907913208, Val Accuracy: 31.799999237060547\n",
      "Iteration 2100, Epoch 3, Loss: 1.9220768213272095, Accuracy: 32.74934005737305, Val Loss: 1.8951126337051392, Val Accuracy: 33.29999923706055\n",
      "Iteration 2800, Epoch 4, Loss: 1.8471931219100952, Accuracy: 35.08635711669922, Val Loss: 1.8455675840377808, Val Accuracy: 36.19999694824219\n",
      "Iteration 3500, Epoch 5, Loss: 1.7960790395736694, Accuracy: 36.813499450683594, Val Loss: 1.8083993196487427, Val Accuracy: 36.19999694824219\n",
      "Iteration 4200, Epoch 6, Loss: 1.7603392601013184, Accuracy: 38.072776794433594, Val Loss: 1.7709484100341797, Val Accuracy: 38.5\n",
      "Iteration 4900, Epoch 7, Loss: 1.7325605154037476, Accuracy: 39.12910079956055, Val Loss: 1.7516084909439087, Val Accuracy: 39.099998474121094\n",
      "Iteration 5600, Epoch 8, Loss: 1.7012038230895996, Accuracy: 40.415794372558594, Val Loss: 1.7307347059249878, Val Accuracy: 39.89999771118164\n",
      "Iteration 6300, Epoch 9, Loss: 1.6707054376602173, Accuracy: 41.06755828857422, Val Loss: 1.7065685987472534, Val Accuracy: 41.0\n",
      "Iteration 7000, Epoch 10, Loss: 1.644197940826416, Accuracy: 41.70560836791992, Val Loss: 1.6857908964157104, Val Accuracy: 43.29999923706055\n",
      "Training took 107.664071 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with four fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, PRrelu, softmax\n",
    "    Optimizers Used: SGD\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = glorot_uniform(xavier)\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(96, activation=tf.keras.layers.PReLU(),\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.Dense(96, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.SGD(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "k2gyP2a3Uqgm",
    "outputId": "6764d4db-c0df-4660-d240-5fa9a78551c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 4.953373432159424, Accuracy: 0.0, Val Loss: 4.910431385040283, Val Accuracy: 0.30000001192092896\n",
      "Iteration 700, Epoch 1, Loss: 3.648277759552002, Accuracy: 16.79074478149414, Val Loss: 2.5129098892211914, Val Accuracy: 26.0\n",
      "Iteration 1400, Epoch 2, Loss: 2.1723668575286865, Accuracy: 29.19537353515625, Val Loss: 2.046302318572998, Val Accuracy: 30.599998474121094\n",
      "Iteration 2100, Epoch 3, Loss: 1.9551620483398438, Accuracy: 32.58457946777344, Val Loss: 1.9382052421569824, Val Accuracy: 33.19999694824219\n",
      "Iteration 2800, Epoch 4, Loss: 1.8666714429855347, Accuracy: 34.9931640625, Val Loss: 1.872639536857605, Val Accuracy: 34.79999923706055\n",
      "Iteration 3500, Epoch 5, Loss: 1.8077040910720825, Accuracy: 36.72411346435547, Val Loss: 1.825153112411499, Val Accuracy: 35.29999923706055\n",
      "Iteration 4200, Epoch 6, Loss: 1.7668704986572266, Accuracy: 38.00960159301758, Val Loss: 1.7848596572875977, Val Accuracy: 37.5\n",
      "Iteration 4900, Epoch 7, Loss: 1.7364652156829834, Accuracy: 38.77561569213867, Val Loss: 1.754628300666809, Val Accuracy: 38.5\n",
      "Iteration 5600, Epoch 8, Loss: 1.703246831893921, Accuracy: 39.87970733642578, Val Loss: 1.7279624938964844, Val Accuracy: 39.099998474121094\n",
      "Iteration 6300, Epoch 9, Loss: 1.6702899932861328, Accuracy: 40.814666748046875, Val Loss: 1.701865553855896, Val Accuracy: 39.599998474121094\n",
      "Iteration 7000, Epoch 10, Loss: 1.6410166025161743, Accuracy: 41.57418441772461, Val Loss: 1.683608055114746, Val Accuracy: 41.5\n",
      "Training took 124.122777 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with four fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, PRrelu, softmax\n",
    "    Optimizers Used: SGD\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = glorot_uniform (xavier)\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='tanh',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation=tf.keras.layers.PReLU(),\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.Dense(128, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.SGD(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "U9DyttglVeGT",
    "outputId": "69ec6d95-2435-4cd4-ed08-7dd46a2c343f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 4.872550964355469, Accuracy: 0.0, Val Loss: 4.887304782867432, Val Accuracy: 0.20000000298023224\n",
      "Iteration 700, Epoch 1, Loss: 4.674384117126465, Accuracy: 6.4439191818237305, Val Loss: 4.497355937957764, Val Accuracy: 13.799999237060547\n",
      "Iteration 1400, Epoch 2, Loss: 4.359328746795654, Accuracy: 17.40895652770996, Val Loss: 4.2574238777160645, Val Accuracy: 17.5\n",
      "Iteration 2100, Epoch 3, Loss: 4.144274711608887, Accuracy: 17.286357879638672, Val Loss: 4.062170505523682, Val Accuracy: 16.899999618530273\n",
      "Iteration 2800, Epoch 4, Loss: 3.9162659645080566, Accuracy: 16.47614288330078, Val Loss: 3.8149335384368896, Val Accuracy: 16.899999618530273\n",
      "Iteration 3500, Epoch 5, Loss: 3.556414842605591, Accuracy: 15.224541664123535, Val Loss: 3.400240421295166, Val Accuracy: 13.0\n",
      "Iteration 4200, Epoch 6, Loss: 2.9416260719299316, Accuracy: 11.986186027526855, Val Loss: 2.7763848304748535, Val Accuracy: 10.59999942779541\n",
      "Iteration 4900, Epoch 7, Loss: 2.518076181411743, Accuracy: 11.25, Val Loss: 2.4780240058898926, Val Accuracy: 10.699999809265137\n",
      "Iteration 5600, Epoch 8, Loss: 2.403082847595215, Accuracy: 11.015951156616211, Val Loss: 2.393113374710083, Val Accuracy: 10.40000057220459\n",
      "Iteration 6300, Epoch 9, Loss: 2.3624703884124756, Accuracy: 10.973627090454102, Val Loss: 2.3597869873046875, Val Accuracy: 10.40000057220459\n",
      "Iteration 7000, Epoch 10, Loss: 2.3437163829803467, Accuracy: 10.733060836791992, Val Loss: 2.343111515045166, Val Accuracy: 10.300000190734863\n",
      "Training took 128.094099 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with four fully connected layers. \n",
    "    Activation Functions Used: Relu, tanh, PRrelu, sigmoid\n",
    "    Optimizers Used: SGD with momentum\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='tanh',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation=tf.keras.layers.PReLU(),\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.Dense(128, activation='sigmoid',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.SGD(learning_rate, momentum = 0.1) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EmeC7hf-I8Za"
   },
   "outputs": [],
   "source": [
    "# Fully connected 4 layer network#2 ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__Yw0A0nJAQ-"
   },
   "outputs": [],
   "source": [
    "# Fully connected 6 layer network#3 begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "ULb4YaKwW17z",
    "outputId": "2011994c-e99d-4bff-d5b4-da55b2e5f950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 6.257875442504883, Accuracy: 0.0, Val Loss: 5.689838886260986, Val Accuracy: 0.20000000298023224\n",
      "Iteration 700, Epoch 1, Loss: 1.8572686910629272, Accuracy: 36.274070739746094, Val Loss: 1.6541603803634644, Val Accuracy: 44.10000228881836\n",
      "Iteration 1400, Epoch 2, Loss: 1.5752373933792114, Accuracy: 44.288875579833984, Val Loss: 1.5404925346374512, Val Accuracy: 45.0\n",
      "Iteration 2100, Epoch 3, Loss: 1.4780240058898926, Accuracy: 47.86906814575195, Val Loss: 1.506411075592041, Val Accuracy: 46.39999771118164\n",
      "Iteration 2800, Epoch 4, Loss: 1.3960193395614624, Accuracy: 50.42557144165039, Val Loss: 1.4578404426574707, Val Accuracy: 49.599998474121094\n",
      "Iteration 3500, Epoch 5, Loss: 1.3328073024749756, Accuracy: 53.18578338623047, Val Loss: 1.4350227117538452, Val Accuracy: 50.5\n",
      "Iteration 4200, Epoch 6, Loss: 1.2761518955230713, Accuracy: 55.327659606933594, Val Loss: 1.4524667263031006, Val Accuracy: 50.80000305175781\n",
      "Iteration 4900, Epoch 7, Loss: 1.2178043127059937, Accuracy: 56.95184326171875, Val Loss: 1.4482696056365967, Val Accuracy: 50.80000305175781\n",
      "Iteration 5600, Epoch 8, Loss: 1.1741026639938354, Accuracy: 58.61663055419922, Val Loss: 1.4412811994552612, Val Accuracy: 50.19999694824219\n",
      "Iteration 6300, Epoch 9, Loss: 1.1135156154632568, Accuracy: 60.83815002441406, Val Loss: 1.5143351554870605, Val Accuracy: 49.5\n",
      "Iteration 7000, Epoch 10, Loss: 1.0569204092025757, Accuracy: 62.5, Val Loss: 1.5771121978759766, Val Accuracy: 49.29999923706055\n",
      "Training took 157.822673 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with six fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, PRrelu, relu, relu, softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.he_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation=tf.keras.layers.PReLU(),\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc5 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)        \n",
    "        self.fc6 = tf.keras.layers.Dense(128, activation='softmax',\n",
    "                                   kernel_initializer=initializer)        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "Mq2OiMcBq5PB",
    "outputId": "42aac19d-1e73-44e3-efdd-558a9e135163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 4.852030277252197, Accuracy: 6.25, Val Loss: 4.850188255310059, Val Accuracy: 11.90000057220459\n",
      "Iteration 700, Epoch 1, Loss: 4.272616386413574, Accuracy: 10.103869438171387, Val Loss: 3.74444842338562, Val Accuracy: 9.800000190734863\n",
      "Iteration 1400, Epoch 2, Loss: 3.317512273788452, Accuracy: 9.818612098693848, Val Loss: 3.033723831176758, Val Accuracy: 9.800000190734863\n",
      "Iteration 2100, Epoch 3, Loss: 2.800137996673584, Accuracy: 9.948192596435547, Val Loss: 2.682596206665039, Val Accuracy: 9.800000190734863\n",
      "Iteration 2800, Epoch 4, Loss: 2.566826820373535, Accuracy: 9.868749618530273, Val Loss: 2.519131898880005, Val Accuracy: 9.800000190734863\n",
      "Iteration 3500, Epoch 5, Loss: 2.458301544189453, Accuracy: 9.613163948059082, Val Loss: 2.437211751937866, Val Accuracy: 10.699999809265137\n",
      "Iteration 4200, Epoch 6, Loss: 2.4021129608154297, Accuracy: 9.789105415344238, Val Loss: 2.3921749591827393, Val Accuracy: 10.699999809265137\n",
      "Iteration 4900, Epoch 7, Loss: 2.3699495792388916, Accuracy: 9.87667179107666, Val Loss: 2.3652713298797607, Val Accuracy: 7.90000057220459\n",
      "Iteration 5600, Epoch 8, Loss: 2.3499629497528076, Accuracy: 9.940732955932617, Val Loss: 2.347659111022949, Val Accuracy: 8.699999809265137\n",
      "Iteration 6300, Epoch 9, Loss: 2.3367512226104736, Accuracy: 9.876893997192383, Val Loss: 2.3358078002929688, Val Accuracy: 8.699999809265137\n",
      "Iteration 7000, Epoch 10, Loss: 2.327442169189453, Accuracy: 9.821428298950195, Val Loss: 2.3280444145202637, Val Accuracy: 8.699999809265137\n",
      "Training took 169.410285 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with six fully connected layers with weights initialized to zero. \n",
    "    Activation Functions Used: Relu, relu, PRrelu, relu, relu, softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.zeros()\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation=tf.keras.layers.PReLU(),\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc5 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)        \n",
    "        self.fc6 = tf.keras.layers.Dense(128, activation='softmax',\n",
    "                                   kernel_initializer=initializer)        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "vSIghmitWSXu",
    "outputId": "9635e29e-e415-40ef-d90b-5a0a69e5a933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 4.740220069885254, Accuracy: 6.25, Val Loss: 4.549184322357178, Val Accuracy: 10.199999809265137\n",
      "Iteration 700, Epoch 1, Loss: 1.8214665651321411, Accuracy: 36.6507682800293, Val Loss: 1.6466152667999268, Val Accuracy: 42.5\n",
      "Iteration 1400, Epoch 2, Loss: 1.5620239973068237, Accuracy: 44.78838348388672, Val Loss: 1.5124034881591797, Val Accuracy: 46.599998474121094\n",
      "Iteration 2100, Epoch 3, Loss: 1.46746826171875, Accuracy: 47.97616195678711, Val Loss: 1.479024052619934, Val Accuracy: 46.099998474121094\n",
      "Iteration 2800, Epoch 4, Loss: 1.401035189628601, Accuracy: 50.279571533203125, Val Loss: 1.4699703454971313, Val Accuracy: 48.0\n",
      "Iteration 3500, Epoch 5, Loss: 1.3516278266906738, Accuracy: 52.23112106323242, Val Loss: 1.4272321462631226, Val Accuracy: 50.400001525878906\n",
      "Iteration 4200, Epoch 6, Loss: 1.3136874437332153, Accuracy: 53.714622497558594, Val Loss: 1.411895990371704, Val Accuracy: 50.19999694824219\n",
      "Iteration 4900, Epoch 7, Loss: 1.2758690118789673, Accuracy: 55.1588134765625, Val Loss: 1.3821754455566406, Val Accuracy: 49.70000076293945\n",
      "Iteration 5600, Epoch 8, Loss: 1.2525259256362915, Accuracy: 55.78582763671875, Val Loss: 1.3828105926513672, Val Accuracy: 51.400001525878906\n",
      "Iteration 6300, Epoch 9, Loss: 1.1960172653198242, Accuracy: 57.48735809326172, Val Loss: 1.3589364290237427, Val Accuracy: 52.999996185302734\n",
      "Iteration 7000, Epoch 10, Loss: 1.162337303161621, Accuracy: 59.59403991699219, Val Loss: 1.3530566692352295, Val Accuracy: 52.29999923706055\n",
      "Training took 157.567289 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with six fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, PRrelu, tanh, sigmoid, softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(num_classes, activation=tf.keras.layers.PReLU(),\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.Dense(num_classes, activation='tanh',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc5 = tf.keras.layers.Dense(num_classes, activation='sigmoid',\n",
    "                                   kernel_initializer=initializer)        \n",
    "        self.fc6 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 128, 128, 10\n",
    "\n",
    "model5 = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "HQm7wIuCYvsp",
    "outputId": "06f3d0be-5acf-424f-b344-b04bd98732ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 4.362520217895508, Accuracy: 6.25, Val Loss: 4.269597053527832, Val Accuracy: 8.699999809265137\n",
      "Iteration 700, Epoch 1, Loss: 1.8376331329345703, Accuracy: 36.124732971191406, Val Loss: 1.6089344024658203, Val Accuracy: 43.39999771118164\n",
      "Iteration 1400, Epoch 2, Loss: 1.568851351737976, Accuracy: 44.44635772705078, Val Loss: 1.529409646987915, Val Accuracy: 45.69999694824219\n",
      "Iteration 2100, Epoch 3, Loss: 1.4876595735549927, Accuracy: 47.31985855102539, Val Loss: 1.521875262260437, Val Accuracy: 45.10000228881836\n",
      "Iteration 2800, Epoch 4, Loss: 1.4268298149108887, Accuracy: 49.58995819091797, Val Loss: 1.478678584098816, Val Accuracy: 48.10000228881836\n",
      "Iteration 3500, Epoch 5, Loss: 1.3775806427001953, Accuracy: 51.56965255737305, Val Loss: 1.4256869554519653, Val Accuracy: 50.099998474121094\n",
      "Iteration 4200, Epoch 6, Loss: 1.3351408243179321, Accuracy: 52.49747085571289, Val Loss: 1.4241743087768555, Val Accuracy: 50.400001525878906\n",
      "Iteration 4900, Epoch 7, Loss: 1.2982101440429688, Accuracy: 54.41598129272461, Val Loss: 1.4106167554855347, Val Accuracy: 51.099998474121094\n",
      "Iteration 5600, Epoch 8, Loss: 1.2746931314468384, Accuracy: 55.040531158447266, Val Loss: 1.4019949436187744, Val Accuracy: 50.599998474121094\n",
      "Iteration 6300, Epoch 9, Loss: 1.2363636493682861, Accuracy: 56.65643310546875, Val Loss: 1.401824951171875, Val Accuracy: 50.30000305175781\n",
      "Iteration 7000, Epoch 10, Loss: 1.2042394876480103, Accuracy: 57.578857421875, Val Loss: 1.4013185501098633, Val Accuracy: 53.39999771118164\n",
      "Training took 138.636502 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with six fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, PRrelu, tanh, sigmoid, softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(num_classes, activation=tf.keras.layers.PReLU(),\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.Dense(num_classes, activation='tanh',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc5 = tf.keras.layers.Dense(num_classes, activation='sigmoid',\n",
    "                                   kernel_initializer=initializer)        \n",
    "        self.fc6 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 96, 96, 10\n",
    "\n",
    "model6 = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wheIMvXhGmYT"
   },
   "outputs": [],
   "source": [
    "# Fully connected 6 layer network#3 ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "LZkkd8Nwokvo",
    "outputId": "9efd957a-a76c-412b-cb10-e9ed381449f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' At this point, I have 9 different architectures with varying number of layers and width. The different kinds of models are:\\n1. Three layer network with 42, 192 and 128 widths. \\n2. Four layer network with 32, 96, 128 widths.\\n3. Six layer network with 96 and 128 widths.\\n\\n\\nFour Different Optimizers have been used:\\n1. SGD\\n2. SGD with Momentum\\n3. Adam\\n4. RMSProp\\n\\nFive different Activation function have been used:\\n1. ReLU\\n2. PrELU\\n3. Tanh\\n4. Softmax\\n5. Sigmoid\\n\\nThree different Initializers have been used:\\n1. Zeros\\n2. Xavier\\n3. he_uniform\\n\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" At this point, I have 9 different architectures with varying number of layers and width. The different kinds of models are:\n",
    "1. Three layer network with 42, 192 and 128 widths. \n",
    "2. Four layer network with 32, 96, 128 widths.\n",
    "3. Six layer network with 96 and 128 widths.\n",
    "\n",
    "\n",
    "Four Different Optimizers have been used:\n",
    "1. SGD\n",
    "2. SGD with Momentum\n",
    "3. Adam\n",
    "4. RMSProp\n",
    "\n",
    "Five different Activation function have been used:\n",
    "1. ReLU\n",
    "2. PrELU\n",
    "3. Tanh\n",
    "4. Softmax\n",
    "5. Sigmoid\n",
    "\n",
    "Three different Initializers have been used:\n",
    "1. Zeros\n",
    "2. Xavier\n",
    "3. he_uniform\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PhgHl2GeHPmi"
   },
   "outputs": [],
   "source": [
    "# The next step in the process is to experiment with batch normalization and regularization usages. \n",
    "# The idea is to apply batch normalization and regularization to two of the best networks identified in each of the three layer and six layer networks and experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "roJjuQ2PKVGU"
   },
   "outputs": [],
   "source": [
    "# Application of batch normalization and regularization usage on fully connected 3 layer network begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "4kQT_-qVAGRz",
    "outputId": "dd79ebe0-e1fc-4738-f08f-97f2f130e5f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trian shape (50000, 32, 32, 3)\n",
      "Iteration 0, Epoch 1, Loss: 14.460289001464844, Accuracy: 0.0, Val Loss: 4.903382301330566, Val Accuracy: 0.30000001192092896\n",
      "Iteration 700, Epoch 1, Loss: 4.43472146987915, Accuracy: 4.874732494354248, Val Loss: 2.959465503692627, Val Accuracy: 9.300000190734863\n",
      "Iteration 1400, Epoch 2, Loss: 2.9847371578216553, Accuracy: 10.625, Val Loss: 2.8424923419952393, Val Accuracy: 12.899999618530273\n",
      "Iteration 2100, Epoch 3, Loss: 2.75673508644104, Accuracy: 14.685853004455566, Val Loss: 2.6703648567199707, Val Accuracy: 17.200000762939453\n",
      "Iteration 2800, Epoch 4, Loss: 2.5931546688079834, Accuracy: 17.088096618652344, Val Loss: 2.5262579917907715, Val Accuracy: 18.700000762939453\n",
      "Iteration 3500, Epoch 5, Loss: 2.4586844444274902, Accuracy: 18.367420196533203, Val Loss: 2.4074325561523438, Val Accuracy: 20.0\n",
      "Iteration 4200, Epoch 6, Loss: 2.354135274887085, Accuracy: 19.246967315673828, Val Loss: 2.318803310394287, Val Accuracy: 21.299999237060547\n",
      "Iteration 4900, Epoch 7, Loss: 2.2776670455932617, Accuracy: 19.73872947692871, Val Loss: 2.248816728591919, Val Accuracy: 22.200000762939453\n",
      "Iteration 5600, Epoch 8, Loss: 2.2174909114837646, Accuracy: 20.574007034301758, Val Loss: 2.1980886459350586, Val Accuracy: 23.700000762939453\n",
      "Iteration 6300, Epoch 9, Loss: 2.173409938812256, Accuracy: 21.1253604888916, Val Loss: 2.1596221923828125, Val Accuracy: 24.200000762939453\n",
      "Iteration 7000, Epoch 10, Loss: 2.132727861404419, Accuracy: 22.35689353942871, Val Loss: 2.127333641052246, Val Accuracy: 24.200000762939453\n",
      "Training took 199.062073 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with three fully connected layers. \n",
    "    Activation Functions Used: Relu, ReLU, Softmax\n",
    "    Optimizers Used: SGD with momentum\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = glorot_uniform\n",
    "    Batch Normalization Usage: Yes\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.BatchNormalization(axis = 1)\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc5 = tf.keras.layers.BatchNormalization(axis = 1)\n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc6 = tf.keras.layers.BatchNormalization(axis = 1)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.SGD(learning_rate, momentum = 0.1) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "6t4kX0TiAqd-",
    "outputId": "0b16693c-9fa1-4f05-cf84-380fd14fd6ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trian shape (50000, 32, 32, 3)\n",
      "Iteration 0, Epoch 1, Loss: 5.338575839996338, Accuracy: 0.0, Val Loss: 5.148134708404541, Val Accuracy: 0.0\n",
      "Iteration 700, Epoch 1, Loss: 3.077404499053955, Accuracy: 21.45149803161621, Val Loss: 2.1859166622161865, Val Accuracy: 28.5\n",
      "Iteration 1400, Epoch 2, Loss: 1.9976201057434082, Accuracy: 32.760826110839844, Val Loss: 1.941796898841858, Val Accuracy: 33.099998474121094\n",
      "Iteration 2100, Epoch 3, Loss: 1.845123291015625, Accuracy: 36.33018493652344, Val Loss: 1.8538987636566162, Val Accuracy: 35.10000228881836\n",
      "Iteration 2800, Epoch 4, Loss: 1.7697933912277222, Accuracy: 38.2424201965332, Val Loss: 1.8027914762496948, Val Accuracy: 36.5\n",
      "Iteration 3500, Epoch 5, Loss: 1.7222458124160767, Accuracy: 39.938499450683594, Val Loss: 1.7633614540100098, Val Accuracy: 38.5\n",
      "Iteration 4200, Epoch 6, Loss: 1.6880912780761719, Accuracy: 41.05037307739258, Val Loss: 1.7259232997894287, Val Accuracy: 40.900001525878906\n",
      "Iteration 4900, Epoch 7, Loss: 1.6625773906707764, Accuracy: 41.767417907714844, Val Loss: 1.7069766521453857, Val Accuracy: 42.099998474121094\n",
      "Iteration 5600, Epoch 8, Loss: 1.630440354347229, Accuracy: 43.017784118652344, Val Loss: 1.6871235370635986, Val Accuracy: 42.79999923706055\n",
      "Iteration 6300, Epoch 9, Loss: 1.599854588508606, Accuracy: 43.98482894897461, Val Loss: 1.6593540906906128, Val Accuracy: 44.20000076293945\n",
      "Iteration 7000, Epoch 10, Loss: 1.571998953819275, Accuracy: 45.21028137207031, Val Loss: 1.6452964544296265, Val Accuracy: 43.900001525878906\n",
      "Training took 115.167153 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with three fully connected layers. \n",
    "    Activation Functions Used: Relu, ReLU, Softmax\n",
    "    Optimizers Used: SGD with momentum\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = glorot_uniform\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: Yes\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation='softmax',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.SGD(learning_rate, momentum = 0.1) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "mcHQGEkyGhC_",
    "outputId": "769d5686-2a46-4b26-e97b-476cfe9aa436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trian shape (50000, 32, 32, 3)\n",
      "Iteration 0, Epoch 1, Loss: 7.430489540100098, Accuracy: 0.0, Val Loss: 5.105891704559326, Val Accuracy: 0.4000000059604645\n",
      "Iteration 700, Epoch 1, Loss: 5.9045538902282715, Accuracy: 14.481544494628906, Val Loss: 2.350713014602661, Val Accuracy: 28.0\n",
      "Iteration 1400, Epoch 2, Loss: 4.899555683135986, Accuracy: 23.28740119934082, Val Loss: 2.017662286758423, Val Accuracy: 32.60000228881836\n",
      "Iteration 2100, Epoch 3, Loss: 4.708881855010986, Accuracy: 25.991323471069336, Val Loss: 1.921962022781372, Val Accuracy: 35.10000228881836\n",
      "Iteration 2800, Epoch 4, Loss: 4.58647346496582, Accuracy: 27.568960189819336, Val Loss: 1.8640259504318237, Val Accuracy: 36.39999771118164\n",
      "Iteration 3500, Epoch 5, Loss: 4.5348968505859375, Accuracy: 28.804346084594727, Val Loss: 1.8229739665985107, Val Accuracy: 36.89999771118164\n",
      "Iteration 4200, Epoch 6, Loss: 4.474116325378418, Accuracy: 30.403470993041992, Val Loss: 1.79880952835083, Val Accuracy: 37.79999923706055\n",
      "Iteration 4900, Epoch 7, Loss: 4.479538917541504, Accuracy: 30.67110824584961, Val Loss: 1.771389365196228, Val Accuracy: 38.5\n",
      "Iteration 5600, Epoch 8, Loss: 4.490814685821533, Accuracy: 32.31563949584961, Val Loss: 1.7548887729644775, Val Accuracy: 39.0\n",
      "Iteration 6300, Epoch 9, Loss: 4.443900108337402, Accuracy: 32.13511657714844, Val Loss: 1.738852858543396, Val Accuracy: 39.0\n",
      "Iteration 7000, Epoch 10, Loss: 4.430327415466309, Accuracy: 33.352806091308594, Val Loss: 1.7235181331634521, Val Accuracy: 39.39999771118164\n",
      "Training took 133.301887 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with three fully connected layers. \n",
    "    Activation Functions Used: Relu, ReLU, Softmax\n",
    "    Optimizers Used: SGD with momentum\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = glorot_uniform\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: Yes -- Dropout\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.Dropout(0.2)\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc5 = tf.keras.layers.Dropout(0.2)\n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc6 = tf.keras.layers.Dropout(0.2)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.SGD(learning_rate, momentum = 0.1) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "exDDBQpn4wdw",
    "outputId": "bf5cdd63-3818-4c37-9e9b-1d78ae4c3407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 5.008494853973389, Accuracy: 0.0, Val Loss: 5.275371074676514, Val Accuracy: 0.30000001192092896\n",
      "Iteration 700, Epoch 1, Loss: 3.8247485160827637, Accuracy: 16.240192413330078, Val Loss: 2.4506404399871826, Val Accuracy: 26.30000114440918\n",
      "Iteration 1400, Epoch 2, Loss: 2.4668073654174805, Accuracy: 26.821273803710938, Val Loss: 2.0642549991607666, Val Accuracy: 31.299999237060547\n",
      "Iteration 2100, Epoch 3, Loss: 2.1641080379486084, Accuracy: 29.968584060668945, Val Loss: 1.9843767881393433, Val Accuracy: 32.5\n",
      "Iteration 2800, Epoch 4, Loss: 2.0472772121429443, Accuracy: 31.831249237060547, Val Loss: 1.9495902061462402, Val Accuracy: 33.599998474121094\n",
      "Iteration 3500, Epoch 5, Loss: 1.9813458919525146, Accuracy: 33.06148910522461, Val Loss: 1.9231754541397095, Val Accuracy: 34.900001525878906\n",
      "Iteration 4200, Epoch 6, Loss: 1.9375848770141602, Accuracy: 33.62790298461914, Val Loss: 1.8862206935882568, Val Accuracy: 37.599998474121094\n",
      "Iteration 4900, Epoch 7, Loss: 1.9068775177001953, Accuracy: 34.54745101928711, Val Loss: 1.8596928119659424, Val Accuracy: 38.20000076293945\n",
      "Iteration 5600, Epoch 8, Loss: 1.8754993677139282, Accuracy: 35.61422348022461, Val Loss: 1.8451224565505981, Val Accuracy: 37.5\n",
      "Iteration 6300, Epoch 9, Loss: 1.8440953493118286, Accuracy: 36.524620056152344, Val Loss: 1.8281313180923462, Val Accuracy: 39.0\n",
      "Iteration 7000, Epoch 10, Loss: 1.8178797960281372, Accuracy: 36.98979568481445, Val Loss: 1.8177772760391235, Val Accuracy: 39.599998474121094\n",
      "Training took 115.360043 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with three fully connected layers. \n",
    "    Activation Functions Used: Relu, ReLU, Softmax\n",
    "    Optimizers Used: SGD with momentum\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = glorot_uniform\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: Yes -- Data Augmentation\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.SGD(learning_rate, momentum = 0.1) \n",
    "\n",
    "X_train, y_train = load_datagen(X_train, y_train)\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "N7avqmFiB5g9",
    "outputId": "8ff26180-b9d6-485e-99cf-1e800c253c64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trian shape (50000, 32, 32, 3)\n",
      "Iteration 0, Epoch 1, Loss: 15.501960754394531, Accuracy: 1.5625, Val Loss: 7.0485029220581055, Val Accuracy: 0.5\n",
      "Iteration 700, Epoch 1, Loss: 7.164695739746094, Accuracy: 12.388551712036133, Val Loss: 4.198223114013672, Val Accuracy: 21.100000381469727\n",
      "Iteration 1400, Epoch 2, Loss: 3.196993589401245, Accuracy: 22.434444427490234, Val Loss: 3.3213348388671875, Val Accuracy: 17.600000381469727\n",
      "Iteration 2100, Epoch 3, Loss: 3.1271371841430664, Accuracy: 22.255290985107422, Val Loss: 3.2736854553222656, Val Accuracy: 19.200000762939453\n",
      "Iteration 2800, Epoch 4, Loss: 2.8874690532684326, Accuracy: 22.881250381469727, Val Loss: 2.930332899093628, Val Accuracy: 22.899999618530273\n",
      "Iteration 3500, Epoch 5, Loss: 2.6119842529296875, Accuracy: 29.102914810180664, Val Loss: 2.797142744064331, Val Accuracy: 28.799999237060547\n",
      "Iteration 4200, Epoch 6, Loss: 2.670226573944092, Accuracy: 24.786544799804688, Val Loss: 2.829084873199463, Val Accuracy: 25.0\n",
      "Iteration 4900, Epoch 7, Loss: 2.49678897857666, Accuracy: 29.84427261352539, Val Loss: 2.737774133682251, Val Accuracy: 28.200000762939453\n",
      "Iteration 5600, Epoch 8, Loss: 2.3938937187194824, Accuracy: 29.344018936157227, Val Loss: 2.6653685569763184, Val Accuracy: 27.700000762939453\n",
      "Iteration 6300, Epoch 9, Loss: 1.984223484992981, Accuracy: 31.534090042114258, Val Loss: 2.0029382705688477, Val Accuracy: 31.400001525878906\n",
      "Iteration 7000, Epoch 10, Loss: 1.9175002574920654, Accuracy: 34.70981979370117, Val Loss: 1.9700406789779663, Val Accuracy: 33.79999923706055\n",
      "Training took 209.183865 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with three fully connected layers. \n",
    "    Activation Functions Used: Relu, ReLU, Softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = he_uniform\n",
    "    Batch Normalization Usage: Yes\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.he_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.BatchNormalization(axis = 1)        \n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc5 = tf.keras.layers.BatchNormalization(axis = 1)        \n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc6 = tf.keras.layers.BatchNormalization(axis = 1)        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "edccXLOaCXWG",
    "outputId": "d6204e86-fc54-4798-d74e-fccde5e3cdf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trian shape (50000, 32, 32, 3)\n",
      "Iteration 0, Epoch 1, Loss: 5.681793212890625, Accuracy: 0.0, Val Loss: 5.520319938659668, Val Accuracy: 1.4000000953674316\n",
      "Iteration 700, Epoch 1, Loss: 1.9762439727783203, Accuracy: 34.83193588256836, Val Loss: 1.8685003519058228, Val Accuracy: 44.10000228881836\n",
      "Iteration 1400, Epoch 2, Loss: 1.6178958415985107, Accuracy: 43.68345642089844, Val Loss: 1.7776645421981812, Val Accuracy: 44.29999923706055\n",
      "Iteration 2100, Epoch 3, Loss: 1.4768613576889038, Accuracy: 48.66071319580078, Val Loss: 1.7747807502746582, Val Accuracy: 45.5\n",
      "Iteration 2800, Epoch 4, Loss: 1.3615370988845825, Accuracy: 52.88125228881836, Val Loss: 1.993119716644287, Val Accuracy: 44.10000228881836\n",
      "Iteration 3500, Epoch 5, Loss: 1.2567222118377686, Accuracy: 56.33300018310547, Val Loss: 2.0021939277648926, Val Accuracy: 45.79999923706055\n",
      "Iteration 4200, Epoch 6, Loss: 1.1709330081939697, Accuracy: 59.27680587768555, Val Loss: 2.0948681831359863, Val Accuracy: 44.900001525878906\n",
      "Iteration 4900, Epoch 7, Loss: 1.086397647857666, Accuracy: 62.72993087768555, Val Loss: 2.291015863418579, Val Accuracy: 45.89999771118164\n",
      "Iteration 5600, Epoch 8, Loss: 1.0000308752059937, Accuracy: 66.37257385253906, Val Loss: 2.3014252185821533, Val Accuracy: 45.89999771118164\n",
      "Iteration 6300, Epoch 9, Loss: 0.9113900661468506, Accuracy: 69.09091186523438, Val Loss: 2.5603554248809814, Val Accuracy: 43.0\n",
      "Iteration 7000, Epoch 10, Loss: 0.8195656538009644, Accuracy: 72.17793273925781, Val Loss: 2.611330509185791, Val Accuracy: 48.0\n",
      "Training took 127.620002 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with three fully connected layers. \n",
    "    Activation Functions Used: Relu, ReLU, Softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = he_uniform\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: Yes\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.he_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation='softmax',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "IAiK1iFNG3lZ",
    "outputId": "f841ba5f-cd35-4d41-9c8e-c1b7a7ec50a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trian shape (50000, 32, 32, 3)\n",
      "Iteration 0, Epoch 1, Loss: 7.477950096130371, Accuracy: 0.0, Val Loss: 5.167136192321777, Val Accuracy: 2.8999998569488525\n",
      "Iteration 700, Epoch 1, Loss: 4.810964584350586, Accuracy: 26.673948287963867, Val Loss: 1.788588047027588, Val Accuracy: 40.79999923706055\n",
      "Iteration 1400, Epoch 2, Loss: 4.476006507873535, Accuracy: 32.07561111450195, Val Loss: 1.801803708076477, Val Accuracy: 41.70000076293945\n",
      "Iteration 2100, Epoch 3, Loss: 4.3686041831970215, Accuracy: 34.182098388671875, Val Loss: 1.7802538871765137, Val Accuracy: 43.599998474121094\n",
      "Iteration 2800, Epoch 4, Loss: 4.4042463302612305, Accuracy: 35.015625, Val Loss: 1.8047302961349487, Val Accuracy: 43.20000076293945\n",
      "Iteration 3500, Epoch 5, Loss: 4.224153995513916, Accuracy: 36.15040588378906, Val Loss: 1.8850326538085938, Val Accuracy: 43.0\n",
      "Iteration 4200, Epoch 6, Loss: 4.254215717315674, Accuracy: 37.149932861328125, Val Loss: 1.8703242540359497, Val Accuracy: 45.599998474121094\n",
      "Iteration 4900, Epoch 7, Loss: 4.149716854095459, Accuracy: 38.5137939453125, Val Loss: 1.8477797508239746, Val Accuracy: 45.89999771118164\n",
      "Iteration 5600, Epoch 8, Loss: 4.227959632873535, Accuracy: 38.826778411865234, Val Loss: 1.7380903959274292, Val Accuracy: 46.0\n",
      "Iteration 6300, Epoch 9, Loss: 4.176342964172363, Accuracy: 38.73106002807617, Val Loss: 1.798675775527954, Val Accuracy: 47.29999923706055\n",
      "Iteration 7000, Epoch 10, Loss: 4.109694957733154, Accuracy: 39.381378173828125, Val Loss: 1.8080332279205322, Val Accuracy: 45.599998474121094\n",
      "Training took 144.234151 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with three fully connected layers. \n",
    "    Activation Functions Used: Relu, ReLU, Softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = he_uniform\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: Yes -- Dropout\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.he_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.Dropout(0.2)       \n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc5 = tf.keras.layers.Dropout(0.2)       \n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc6 = tf.keras.layers.Dropout(0.2)       \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNn9gmgpKjFM"
   },
   "outputs": [],
   "source": [
    "# Application of batch normalization and regularization usage on fully connected 3 layer network ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "stTJwDABeYCq"
   },
   "outputs": [],
   "source": [
    "# Application of batch normalization and regularization usage on fully connected 4 layer network begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "colab_type": "code",
    "id": "jnnw4ApHesaI",
    "outputId": "c4306ac3-949d-4f9e-f098-9857d9795de5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trian shape (50000, 32, 32, 3)\n",
      "Iteration 0, Epoch 1, Loss: 11.263551712036133, Accuracy: 0.0, Val Loss: 4.1783013343811035, Val Accuracy: 3.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-ac6e296d8afd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_cifar10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mtrain_part34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_init_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_init_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-59086aff6717>\u001b[0m in \u001b[0;36mtrain_part34\u001b[0;34m(model_init_fn, optimizer_init_fn, num_epochs, is_training)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0;31m# Update the metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mtrainable_variables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1715\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msub_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             extra_variables=self._trainable_weights))\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/training/tracking/layer_utils.py\u001b[0m in \u001b[0;36mgather_trainable_weights\u001b[0;34m(trainable, sub_layers, extra_variables)\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msub_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m   trainable_extra_variables = [\n\u001b[1;32m     81\u001b[0m       v for v in extra_variables if v.trainable]\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    945\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0mnested\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_children_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainable_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dedup_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnested\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_gather_children_attribute\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m   2331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_layers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2332\u001b[0m       nested_layers = trackable_layer_utils.filter_empty_layer_containers(\n\u001b[0;32m-> 2333\u001b[0;31m           self._layers)\n\u001b[0m\u001b[1;32m   2334\u001b[0m       return list(\n\u001b[1;32m   2335\u001b[0m           itertools.chain.from_iterable(\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/training/tracking/layer_utils.py\u001b[0m in \u001b[0;36mfilter_empty_layer_containers\u001b[0;34m(layer_list)\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0;31m# Trackable data structures will not show up in \".layers\" lists, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0;31m# the layers they contain will.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m       \u001b[0mto_visit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/training/tracking/data_structures.py\u001b[0m in \u001b[0;36mlayers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_empty_layer_containers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/training/tracking/data_structures.py\u001b[0m in \u001b[0;36m_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# they're wrapping if out of sync.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mcollected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m       if (isinstance(obj, TrackableDataStructure)\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mor\u001b[0m \u001b[0mlayer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with four fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, relu, softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: Yes\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(32, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc5 = tf.keras.layers.BatchNormalization(axis = 1) \n",
    "        self.fc2 = tf.keras.layers.Dense(32, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc6 = tf.keras.layers.BatchNormalization(axis = 1) \n",
    "        self.fc3 = tf.keras.layers.Dense(32, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc7 = tf.keras.layers.BatchNormalization(axis = 1) \n",
    "        self.fc4 = tf.keras.layers.Dense(64, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc8 = tf.keras.layers.BatchNormalization(axis = 1) \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc8(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aUWuFjfUgKMT"
   },
   "outputs": [],
   "source": [
    "\"\"\" Custom network with four fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, relu, softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: Yes -- Dropout\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(32, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc5 = tf.keras.layers.Dropout(0.2)\n",
    "        self.fc2 = tf.keras.layers.Dense(32, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc6 = tf.keras.layers.Dropout(0.2)\n",
    "        self.fc3 = tf.keras.layers.Dense(32, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc7 = tf.keras.layers.Dropout(0.2)\n",
    "        self.fc4 = tf.keras.layers.Dense(64, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc8 = tf.keras.layers.Dropout(0.2)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc8(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNxm1p_Fgpkm"
   },
   "outputs": [],
   "source": [
    "\"\"\" Custom network with four fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, relu, softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: Yes -- L2\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(32, activation='relu',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc2 = tf.keras.layers.Dense(32, activation='relu',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc3 = tf.keras.layers.Dense(32, activation='relu',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc4 = tf.keras.layers.Dense(64, activation='softmax',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kFAUW0V5f-Av"
   },
   "outputs": [],
   "source": [
    "\"\"\" Custom network with four fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, relu, softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: Yes -- Data Augmentation\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(32, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(32, activation='relu',\n",
    "                                   kernel_initializer=initializer\n",
    "        self.fc3 = tf.keras.layers.Dense(32, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.Dense(64, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "X_train, y_train = load_datagen(X_train, y_train)\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)                                         \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FXMoUiEXiR2I"
   },
   "outputs": [],
   "source": [
    "\"\"\" Custom network with four fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, relu, softmax\n",
    "    Optimizers Used: SGD\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = glorot_uniform(xavier)\n",
    "    Batch Normalization Usage: Yes\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc5 = tf.keras.layers.BatchNormalization(axis = 1)         \n",
    "        self.fc2 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc6 = tf.keras.layers.BatchNormalization(axis = 1)         \n",
    "        self.fc3 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc7 = tf.keras.layers.BatchNormalization(axis = 1)         \n",
    "        self.fc4 = tf.keras.layers.Dense(96, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc8 = tf.keras.layers.BatchNormalization(axis = 1) \n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc5(x)        \n",
    "        x = self.fc2(x)\n",
    "        x = self.fc6(x)        \n",
    "        x = self.fc3(x)\n",
    "        x = self.fc7(x)        \n",
    "        x = self.fc4(x)\n",
    "        x = self.fc8(x)        \n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.SGD(learning_rate) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13BXtzbyivP5"
   },
   "outputs": [],
   "source": [
    "\"\"\" Custom network with four fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, relu, softmax\n",
    "    Optimizers Used: SGD\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = glorot_uniform(xavier)\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: Yes -- Dropout\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc5 = tf.keras.layers.Dropout(0.2)         \n",
    "        self.fc2 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc6 = tf.keras.layers.Dropout(0.2)\n",
    "        self.fc3 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc7 = tf.keras.layers.Dropout(0.2)\n",
    "        self.fc4 = tf.keras.layers.Dense(96, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc8 = tf.keras.layers.Dropout(0.2)\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc5(x)        \n",
    "        x = self.fc2(x)\n",
    "        x = self.fc6(x)        \n",
    "        x = self.fc3(x)\n",
    "        x = self.fc7(x)        \n",
    "        x = self.fc4(x)\n",
    "        x = self.fc8(x)        \n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.SGD(learning_rate) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "786TGmvwjIUs"
   },
   "outputs": [],
   "source": [
    "\"\"\" Custom network with four fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, relu, softmax\n",
    "    Optimizers Used: SGD\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = glorot_uniform(xavier)\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: Yes -- L2\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc2 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc3 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc4 = tf.keras.layers.Dense(96, activation='softmax',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.SGD(learning_rate) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AscFt3lsjeBF"
   },
   "outputs": [],
   "source": [
    "\"\"\" Custom network with four fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, relu, softmax\n",
    "    Optimizers Used: SGD\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = glorot_uniform(xavier)\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: Yes -- Data Augmentation\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc2 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc3 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc4 = tf.keras.layers.Dense(96, activation='softmax',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.SGD(learning_rate) \n",
    "  \n",
    "X_train, y_train = load_datagen(X_train, y_train)\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)                                           \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNMSVdxhjn2s"
   },
   "outputs": [],
   "source": [
    "# Application of batch normalization and regularization usage on fully connected 4 layer network ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fs7Zh07uKmWq"
   },
   "outputs": [],
   "source": [
    "# Application of batch normalization and regularization usage on fully connected 6 layer network begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "jYFEO1YA3Dtc",
    "outputId": "eba37c73-f733-4a9f-ae5b-f038e2214279"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 5.1438093185424805, Accuracy: 0.0, Val Loss: 4.908463478088379, Val Accuracy: 0.10000000149011612\n",
      "Iteration 700, Epoch 1, Loss: 1.911417841911316, Accuracy: 33.00642013549805, Val Loss: 1.6266423463821411, Val Accuracy: 43.39999771118164\n",
      "Iteration 1400, Epoch 2, Loss: 1.6126383543014526, Accuracy: 42.717369079589844, Val Loss: 1.5668234825134277, Val Accuracy: 45.0\n",
      "Iteration 2100, Epoch 3, Loss: 1.4854590892791748, Accuracy: 47.60527038574219, Val Loss: 1.5635242462158203, Val Accuracy: 46.20000076293945\n",
      "Iteration 2800, Epoch 4, Loss: 1.3739393949508667, Accuracy: 51.64374542236328, Val Loss: 1.5371068716049194, Val Accuracy: 47.79999923706055\n",
      "Iteration 3500, Epoch 5, Loss: 1.2752476930618286, Accuracy: 55.6654167175293, Val Loss: 1.5476415157318115, Val Accuracy: 46.70000076293945\n",
      "Iteration 4200, Epoch 6, Loss: 1.1919894218444824, Accuracy: 58.99077606201172, Val Loss: 1.532097578048706, Val Accuracy: 48.79999923706055\n",
      "Iteration 4900, Epoch 7, Loss: 1.1204200983047485, Accuracy: 61.71091079711914, Val Loss: 1.6185203790664673, Val Accuracy: 47.60000228881836\n",
      "Iteration 5600, Epoch 8, Loss: 1.0300806760787964, Accuracy: 65.53744506835938, Val Loss: 1.6452751159667969, Val Accuracy: 46.20000076293945\n",
      "Iteration 6300, Epoch 9, Loss: 0.9634382724761963, Accuracy: 67.94507598876953, Val Loss: 1.6726852655410767, Val Accuracy: 47.0\n",
      "Iteration 7000, Epoch 10, Loss: 0.910520076751709, Accuracy: 69.86607360839844, Val Loss: 1.6710580587387085, Val Accuracy: 47.20000076293945\n",
      "Training took 160.36485 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with six fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, PRrelu, tanh, sigmoid, softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: Yes -- Data Augmentation\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation=tf.keras.layers.PReLU(),\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.Dense(128, activation='tanh',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc5 = tf.keras.layers.Dense(128, activation='sigmoid',\n",
    "                                   kernel_initializer=initializer)        \n",
    "        self.fc6 = tf.keras.layers.Dense(128, activation='softmax',\n",
    "                                   kernel_initializer=initializer)        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "X_train, y_train = load_datagen(X_train, y_train)\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "WIUBR9cFibDA",
    "outputId": "7027dd79-9f5b-408c-b661-7a85a12af30b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 12.905860900878906, Accuracy: 3.125, Val Loss: 4.785928726196289, Val Accuracy: 0.0\n",
      "Iteration 700, Epoch 1, Loss: 7.190889358520508, Accuracy: 14.635342597961426, Val Loss: 5.972823619842529, Val Accuracy: 17.5\n",
      "Iteration 1400, Epoch 2, Loss: 4.3861517906188965, Accuracy: 17.682085037231445, Val Loss: 4.4350128173828125, Val Accuracy: 20.80000114440918\n",
      "Iteration 2100, Epoch 3, Loss: 2.8762364387512207, Accuracy: 21.191234588623047, Val Loss: 2.857100486755371, Val Accuracy: 23.80000114440918\n",
      "Iteration 2800, Epoch 4, Loss: 2.6665308475494385, Accuracy: 22.378231048583984, Val Loss: 2.6825778484344482, Val Accuracy: 28.799999237060547\n",
      "Iteration 3500, Epoch 5, Loss: 2.386935234069824, Accuracy: 22.951231002807617, Val Loss: 2.224994421005249, Val Accuracy: 20.5\n",
      "Iteration 4200, Epoch 6, Loss: 2.066650390625, Accuracy: 25.1768856048584, Val Loss: 2.027719259262085, Val Accuracy: 27.599998474121094\n",
      "Iteration 4900, Epoch 7, Loss: 2.148189067840576, Accuracy: 21.07581901550293, Val Loss: 2.14200496673584, Val Accuracy: 23.19999885559082\n",
      "Iteration 5600, Epoch 8, Loss: 2.1894171237945557, Accuracy: 19.87447738647461, Val Loss: 2.15496826171875, Val Accuracy: 23.399999618530273\n",
      "Iteration 6300, Epoch 9, Loss: 2.1515045166015625, Accuracy: 22.507225036621094, Val Loss: 2.1679694652557373, Val Accuracy: 20.700000762939453\n",
      "Iteration 7000, Epoch 10, Loss: 2.1472551822662354, Accuracy: 24.693342208862305, Val Loss: 2.135911703109741, Val Accuracy: 23.299999237060547\n",
      "Training took 305.088054 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with six fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, PRrelu, tanh, sigmoid, softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: Yes\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc7 =  tf.keras.layers.BatchNormalization(axis = 1)\n",
    "        self.fc2 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc8 =  tf.keras.layers.BatchNormalization(axis = 1)\n",
    "        self.fc3 = tf.keras.layers.Dense(96, activation=tf.keras.layers.PReLU(),\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc9 =  tf.keras.layers.BatchNormalization(axis = 1)\n",
    "        self.fc4 = tf.keras.layers.Dense(96, activation='tanh',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc10 =  tf.keras.layers.BatchNormalization(axis = 1)\n",
    "        self.fc5 = tf.keras.layers.Dense(96, activation='sigmoid',\n",
    "                                   kernel_initializer=initializer)        \n",
    "        self.fc11 =  tf.keras.layers.BatchNormalization(axis = 1)\n",
    "        self.fc6 = tf.keras.layers.Dense(96, activation='softmax',\n",
    "                                   kernel_initializer=initializer)   \n",
    "        self.fc12 =  tf.keras.layers.BatchNormalization(axis = 1)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc8(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc9(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc10(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc11(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.fc12(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "fA717kCpxmn8",
    "outputId": "7cb18ac0-2906-4b29-be20-ab583e0447ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 4.937735557556152, Accuracy: 0.0, Val Loss: 4.757192611694336, Val Accuracy: 7.0\n",
      "Iteration 700, Epoch 1, Loss: 1.8446135520935059, Accuracy: 35.78593063354492, Val Loss: 1.6341135501861572, Val Accuracy: 44.400001525878906\n",
      "Iteration 1400, Epoch 2, Loss: 1.5908197164535522, Accuracy: 43.68848419189453, Val Loss: 1.5549242496490479, Val Accuracy: 45.69999694824219\n",
      "Iteration 2100, Epoch 3, Loss: 1.5030704736709595, Accuracy: 47.004066467285156, Val Loss: 1.4803909063339233, Val Accuracy: 47.900001525878906\n",
      "Iteration 2800, Epoch 4, Loss: 1.4402287006378174, Accuracy: 49.22030258178711, Val Loss: 1.4516634941101074, Val Accuracy: 47.900001525878906\n",
      "Iteration 3500, Epoch 5, Loss: 1.3890063762664795, Accuracy: 50.790191650390625, Val Loss: 1.4422556161880493, Val Accuracy: 50.099998474121094\n",
      "Iteration 4200, Epoch 6, Loss: 1.3575228452682495, Accuracy: 52.046836853027344, Val Loss: 1.4154597520828247, Val Accuracy: 50.30000305175781\n",
      "Iteration 4900, Epoch 7, Loss: 1.3146823644638062, Accuracy: 53.82684326171875, Val Loss: 1.4208494424819946, Val Accuracy: 50.30000305175781\n",
      "Iteration 5600, Epoch 8, Loss: 1.2849658727645874, Accuracy: 54.6417350769043, Val Loss: 1.3867658376693726, Val Accuracy: 51.20000076293945\n",
      "Iteration 6300, Epoch 9, Loss: 1.2550969123840332, Accuracy: 55.68099594116211, Val Loss: 1.4155853986740112, Val Accuracy: 50.900001525878906\n",
      "Iteration 7000, Epoch 10, Loss: 1.2321748733520508, Accuracy: 57.023948669433594, Val Loss: 1.3663585186004639, Val Accuracy: 52.70000076293945\n",
      "Training took 148.807115 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with six fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, PRrelu, tanh, sigmoid, softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: Yes\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc3 = tf.keras.layers.Dense(128, activation=tf.keras.layers.PReLU(),\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc4 = tf.keras.layers.Dense(128, activation='tanh',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc5 = tf.keras.layers.Dense(128, activation='sigmoid',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))        \n",
    "        self.fc6 = tf.keras.layers.Dense(128, activation='softmax',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "2QcLZQH0HIrC",
    "outputId": "a0d0d3fa-f563-46f1-d63f-b58a2e489445"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 6.434884548187256, Accuracy: 0.0, Val Loss: 4.456222057342529, Val Accuracy: 0.0\n",
      "Iteration 700, Epoch 1, Loss: 4.688473224639893, Accuracy: 24.382577896118164, Val Loss: 1.7971866130828857, Val Accuracy: 36.39999771118164\n",
      "Iteration 1400, Epoch 2, Loss: 4.4786481857299805, Accuracy: 30.939960479736328, Val Loss: 1.7016263008117676, Val Accuracy: 40.900001525878906\n",
      "Iteration 2100, Epoch 3, Loss: 4.4306159019470215, Accuracy: 33.416629791259766, Val Loss: 1.6937204599380493, Val Accuracy: 41.20000076293945\n",
      "Iteration 2800, Epoch 4, Loss: 4.371032238006592, Accuracy: 34.38431930541992, Val Loss: 1.690187931060791, Val Accuracy: 41.400001525878906\n",
      "Iteration 3500, Epoch 5, Loss: 4.310122489929199, Accuracy: 35.33323669433594, Val Loss: 1.6263463497161865, Val Accuracy: 41.29999923706055\n",
      "Iteration 4200, Epoch 6, Loss: 4.354306697845459, Accuracy: 35.827999114990234, Val Loss: 1.6064870357513428, Val Accuracy: 45.10000228881836\n",
      "Iteration 4900, Epoch 7, Loss: 4.424750328063965, Accuracy: 35.988731384277344, Val Loss: 1.5833449363708496, Val Accuracy: 45.599998474121094\n",
      "Iteration 5600, Epoch 8, Loss: 4.310168266296387, Accuracy: 37.009674072265625, Val Loss: 1.5632028579711914, Val Accuracy: 47.29999923706055\n",
      "Iteration 6300, Epoch 9, Loss: 4.333471298217773, Accuracy: 36.63294982910156, Val Loss: 1.5747323036193848, Val Accuracy: 45.599998474121094\n",
      "Iteration 7000, Epoch 10, Loss: 4.4044694900512695, Accuracy: 36.50701141357422, Val Loss: 1.5654538869857788, Val Accuracy: 46.20000076293945\n",
      "Training took 163.397422 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with six fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, PRrelu, tanh, sigmoid, softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: Yes\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc7 =  tf.keras.layers.Dropout(0.2)\n",
    "        self.fc2 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc8 =  tf.keras.layers.Dropout(0.2)\n",
    "        self.fc3 = tf.keras.layers.Dense(96, activation=tf.keras.layers.PReLU(),\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc9 =  tf.keras.layers.Dropout(0.2)\n",
    "        self.fc4 = tf.keras.layers.Dense(96, activation='tanh',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc10 =  tf.keras.layers.Dropout(0.2)\n",
    "        self.fc5 = tf.keras.layers.Dense(96, activation='sigmoid',\n",
    "                                   kernel_initializer=initializer)        \n",
    "        self.fc11 =  tf.keras.layers.Dropout(0.2)\n",
    "        self.fc6 = tf.keras.layers.Dense(96, activation='softmax',\n",
    "                                   kernel_initializer=initializer)   \n",
    "        self.fc12 =  tf.keras.layers.Dropout(0.2)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc8(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc9(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc10(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc11(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.fc12(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "1GsYcEiYaOg4",
    "outputId": "2a2f665a-ff99-49d9-a1de-86b87ab4e4e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 13.219505310058594, Accuracy: 1.5625, Val Loss: 4.530873775482178, Val Accuracy: 0.10000000149011612\n",
      "Iteration 700, Epoch 1, Loss: 7.949471473693848, Accuracy: 14.325517654418945, Val Loss: 5.933352470397949, Val Accuracy: 14.0\n",
      "Iteration 1400, Epoch 2, Loss: 4.8681254386901855, Accuracy: 20.01722526550293, Val Loss: 5.538185119628906, Val Accuracy: 13.899999618530273\n",
      "Iteration 2100, Epoch 3, Loss: 3.6516146659851074, Accuracy: 17.35775375366211, Val Loss: 3.7411880493164062, Val Accuracy: 10.899999618530273\n",
      "Iteration 2800, Epoch 4, Loss: 3.0616986751556396, Accuracy: 17.647241592407227, Val Loss: 2.7608256340026855, Val Accuracy: 19.69999885559082\n",
      "Iteration 3500, Epoch 5, Loss: 2.937623977661133, Accuracy: 20.215961456298828, Val Loss: 2.9398903846740723, Val Accuracy: 12.300000190734863\n",
      "Iteration 4200, Epoch 6, Loss: 2.1914775371551514, Accuracy: 19.604951858520508, Val Loss: 2.188457489013672, Val Accuracy: 19.400001525878906\n",
      "Iteration 4900, Epoch 7, Loss: 2.1246066093444824, Accuracy: 22.797130584716797, Val Loss: 2.1247735023498535, Val Accuracy: 23.100000381469727\n",
      "Iteration 5600, Epoch 8, Loss: 2.115118980407715, Accuracy: 22.37186050415039, Val Loss: 2.130230665206909, Val Accuracy: 19.69999885559082\n",
      "Iteration 6300, Epoch 9, Loss: 2.0597143173217773, Accuracy: 27.56502914428711, Val Loss: 1.9990447759628296, Val Accuracy: 31.60000228881836\n",
      "Iteration 7000, Epoch 10, Loss: 2.2072527408599854, Accuracy: 20.181074142456055, Val Loss: 2.1956515312194824, Val Accuracy: 18.299999237060547\n",
      "Training took 292.618646 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with six fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, PRrelu, tanh, sigmoid, softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: Yes\n",
    "    Regularization Usage: No\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(96, activation='relu',                                       \n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc7 =  tf.keras.layers.BatchNormalization(axis = 1)\n",
    "        self.fc2 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc8 =  tf.keras.layers.BatchNormalization(axis = 1)\n",
    "\n",
    "        self.fc3 = tf.keras.layers.Dense(96, activation=tf.keras.layers.PReLU(),\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc9 =  tf.keras.layers.BatchNormalization(axis = 1)\n",
    "        \n",
    "        self.fc4 = tf.keras.layers.Dense(96, activation='tanh', \n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc10 =  tf.keras.layers.BatchNormalization(axis = 1)\n",
    "        \n",
    "        self.fc5 = tf.keras.layers.Dense(96, activation='sigmoid',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc11 =  tf.keras.layers.BatchNormalization(axis = 1)\n",
    "        \n",
    "        self.fc6 = tf.keras.layers.Dense(96, activation='softmax',                                      \n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc12 =  tf.keras.layers.BatchNormalization(axis = 1)\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc8(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc9(x)        \n",
    "        x = self.fc4(x)\n",
    "        x = self.fc10(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc11(x)\n",
    "        x = self.fc6(x)  \n",
    "        x = self.fc12(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "QaBUATqSKOxo",
    "outputId": "6cb46da3-e082-44e1-ca05-f38c507e79ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trian shape (50000, 32, 32, 3)\n",
      "Iteration 0, Epoch 1, Loss: 7.134275913238525, Accuracy: 0.0, Val Loss: 4.6618852615356445, Val Accuracy: 0.0\n",
      "Iteration 700, Epoch 1, Loss: 4.712298393249512, Accuracy: 22.416635513305664, Val Loss: 1.8301550149917603, Val Accuracy: 34.900001525878906\n",
      "Iteration 1400, Epoch 2, Loss: 4.504629611968994, Accuracy: 30.14836311340332, Val Loss: 1.7332100868225098, Val Accuracy: 39.79999923706055\n",
      "Iteration 2100, Epoch 3, Loss: 4.438045978546143, Accuracy: 32.5589714050293, Val Loss: 1.6576486825942993, Val Accuracy: 43.5\n",
      "Iteration 2800, Epoch 4, Loss: 4.357651710510254, Accuracy: 34.06875228881836, Val Loss: 1.649827480316162, Val Accuracy: 41.80000305175781\n",
      "Iteration 3500, Epoch 5, Loss: 4.396771430969238, Accuracy: 35.38539505004883, Val Loss: 1.672469139099121, Val Accuracy: 43.5\n",
      "Iteration 4200, Epoch 6, Loss: 4.3443121910095215, Accuracy: 35.864925384521484, Val Loss: 1.592858076095581, Val Accuracy: 45.0\n",
      "Iteration 4900, Epoch 7, Loss: 4.430395603179932, Accuracy: 36.52800750732422, Val Loss: 1.585927128791809, Val Accuracy: 45.29999923706055\n",
      "Iteration 5600, Epoch 8, Loss: 4.3237152099609375, Accuracy: 36.941001892089844, Val Loss: 1.5616947412490845, Val Accuracy: 46.39999771118164\n",
      "Iteration 6300, Epoch 9, Loss: 4.371478080749512, Accuracy: 36.9886360168457, Val Loss: 1.56846284866333, Val Accuracy: 47.5\n",
      "Iteration 7000, Epoch 10, Loss: 4.266194820404053, Accuracy: 38.37691116333008, Val Loss: 1.603021264076233, Val Accuracy: 47.0\n",
      "Training took 191.289243 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with six fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, PRrelu, tanh, sigmoid, softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: Yes\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(96, activation='relu',                                       \n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc7 =  tf.keras.layers.Dropout(0.2)\n",
    "        self.fc2 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc8 =  tf.keras.layers.Dropout(0.2)\n",
    "\n",
    "        self.fc3 = tf.keras.layers.Dense(96, activation=tf.keras.layers.PReLU(),\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc9 =  tf.keras.layers.Dropout(0.2)\n",
    "        \n",
    "        self.fc4 = tf.keras.layers.Dense(96, activation='tanh', \n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc10 =  tf.keras.layers.Dropout(0.2)\n",
    "        \n",
    "        self.fc5 = tf.keras.layers.Dense(96, activation='sigmoid',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc11 =  tf.keras.layers.Dropout(0.2)\n",
    "        \n",
    "        self.fc6 = tf.keras.layers.Dense(96, activation='softmax',                                      \n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc12 =  tf.keras.layers.Dropout(0.2)\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc8(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc9(x)        \n",
    "        x = self.fc4(x)\n",
    "        x = self.fc10(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc11(x)\n",
    "        x = self.fc6(x)  \n",
    "        x = self.fc12(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "ICysQpFFyGsN",
    "outputId": "e12ceee6-febf-46cc-c410-0ef600d8ae08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trian shape (50000, 32, 32, 3)\n",
      "Iteration 0, Epoch 1, Loss: 4.866191864013672, Accuracy: 0.0, Val Loss: 4.631097316741943, Val Accuracy: 0.0\n",
      "Iteration 700, Epoch 1, Loss: 1.9377410411834717, Accuracy: 32.56062698364258, Val Loss: 1.641908884048462, Val Accuracy: 43.20000076293945\n",
      "Iteration 1400, Epoch 2, Loss: 1.629835844039917, Accuracy: 42.35755157470703, Val Loss: 1.569594383239746, Val Accuracy: 46.29999923706055\n",
      "Iteration 2100, Epoch 3, Loss: 1.504233717918396, Accuracy: 46.75374984741211, Val Loss: 1.5463755130767822, Val Accuracy: 45.89999771118164\n",
      "Iteration 2800, Epoch 4, Loss: 1.4029607772827148, Accuracy: 50.937496185302734, Val Loss: 1.5323801040649414, Val Accuracy: 46.70000076293945\n",
      "Iteration 3500, Epoch 5, Loss: 1.3180736303329468, Accuracy: 53.742061614990234, Val Loss: 1.5497726202011108, Val Accuracy: 47.5\n",
      "Iteration 4200, Epoch 6, Loss: 1.2424312829971313, Accuracy: 57.03125, Val Loss: 1.5363303422927856, Val Accuracy: 47.70000076293945\n",
      "Iteration 4900, Epoch 7, Loss: 1.1778843402862549, Accuracy: 59.51609802246094, Val Loss: 1.580299735069275, Val Accuracy: 47.900001525878906\n",
      "Iteration 5600, Epoch 8, Loss: 1.110268473625183, Accuracy: 62.365299224853516, Val Loss: 1.5778976678848267, Val Accuracy: 46.79999923706055\n",
      "Iteration 6300, Epoch 9, Loss: 1.0431067943572998, Accuracy: 65.20833587646484, Val Loss: 1.5836116075515747, Val Accuracy: 48.10000228881836\n",
      "Iteration 7000, Epoch 10, Loss: 0.9728603959083557, Accuracy: 67.23533630371094, Val Loss: 1.642775297164917, Val Accuracy: 47.60000228881836\n",
      "Training took 149.829618 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with six fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, PRrelu, tanh, sigmoid, softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: Yes\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc2 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc3 = tf.keras.layers.Dense(96, activation=tf.keras.layers.PReLU(),\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc4 = tf.keras.layers.Dense(96, activation='tanh',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))\n",
    "        self.fc5 = tf.keras.layers.Dense(96, activation='sigmoid',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))        \n",
    "        self.fc6 = tf.keras.layers.Dense(96, activation='softmax',\n",
    "                                   kernel_initializer=initializer, kernel_regularizer = regularizers.l2(0.01))        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "  \n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "R914WkLAzpI6",
    "outputId": "b21e02b3-4726-4f4a-e28f-60239537344a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 4.4269585609436035, Accuracy: 1.5625, Val Loss: 4.265326023101807, Val Accuracy: 3.5\n",
      "Iteration 700, Epoch 1, Loss: 1.9244894981384277, Accuracy: 32.353336334228516, Val Loss: 1.6823586225509644, Val Accuracy: 40.20000076293945\n",
      "Iteration 1400, Epoch 2, Loss: 1.640328049659729, Accuracy: 41.67241668701172, Val Loss: 1.5962867736816406, Val Accuracy: 45.599998474121094\n",
      "Iteration 2100, Epoch 3, Loss: 1.5141018629074097, Accuracy: 46.38447952270508, Val Loss: 1.563576579093933, Val Accuracy: 45.79999923706055\n",
      "Iteration 2800, Epoch 4, Loss: 1.4098576307296753, Accuracy: 50.95000457763672, Val Loss: 1.5241942405700684, Val Accuracy: 44.400001525878906\n",
      "Iteration 3500, Epoch 5, Loss: 1.3240249156951904, Accuracy: 53.91166305541992, Val Loss: 1.5048496723175049, Val Accuracy: 47.60000228881836\n",
      "Iteration 4200, Epoch 6, Loss: 1.2543821334838867, Accuracy: 56.220115661621094, Val Loss: 1.5359407663345337, Val Accuracy: 48.69999694824219\n",
      "Iteration 4900, Epoch 7, Loss: 1.1899770498275757, Accuracy: 59.134613037109375, Val Loss: 1.575454592704773, Val Accuracy: 45.79999923706055\n",
      "Iteration 5600, Epoch 8, Loss: 1.1187248229980469, Accuracy: 61.9140625, Val Loss: 1.5721080303192139, Val Accuracy: 48.79999923706055\n",
      "Iteration 6300, Epoch 9, Loss: 1.04438054561615, Accuracy: 65.01893615722656, Val Loss: 1.5762710571289062, Val Accuracy: 47.900001525878906\n",
      "Iteration 7000, Epoch 10, Loss: 1.0062252283096313, Accuracy: 65.97576904296875, Val Loss: 1.6223212480545044, Val Accuracy: 47.400001525878906\n",
      "Training took 147.645052 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Custom network with six fully connected layers. \n",
    "    Activation Functions Used: Relu, relu, PRrelu, tanh, sigmoid, softmax\n",
    "    Optimizers Used: Adam\n",
    "    Learning Rate = 0.003\n",
    "    Weight Initializer = xavier\n",
    "    Batch Normalization Usage: No\n",
    "    Regularization Usage: Yes -- Data Augmentation\n",
    "\"\"\"\n",
    "\n",
    "class CustomNet(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()        \n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "       \n",
    "\n",
    "        initializer = tf.initializers.glorot_uniform(seed=None)\n",
    "        self.fc1 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(96, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc3 = tf.keras.layers.Dense(96, activation=tf.keras.layers.PReLU(),\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc4 = tf.keras.layers.Dense(96, activation='tanh',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc5 = tf.keras.layers.Dense(96, activation='sigmoid',\n",
    "                                   kernel_initializer=initializer)        \n",
    "        self.fc6 = tf.keras.layers.Dense(96, activation='softmax',\n",
    "                                   kernel_initializer=initializer)        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################      \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "\n",
    "device = '/device:CPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "model = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "X_train, y_train = load_datagen(X_train, y_train)\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nmzhUbiuK0gy"
   },
   "outputs": [],
   "source": [
    "# Application of batch normalization and regularization usage on fully connected 6 layer network ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BwNLypU6p-t8",
    "outputId": "d9725dfa-3e5f-43b2-c5a5-0d8accdeeb18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trian shape (50000, 32, 32, 3)\n",
      "Iteration 0, Epoch 1, Loss: 2.378326654434204, Accuracy: 9.375, Val Loss: 2.406445264816284, Val Accuracy: 15.600000381469727\n",
      "Iteration 700, Epoch 1, Loss: 1.8162766695022583, Accuracy: 35.96647644042969, Val Loss: 1.706996202468872, Val Accuracy: 44.20000076293945\n",
      "Iteration 1400, Epoch 2, Loss: 1.5996094942092896, Accuracy: 43.53312301635742, Val Loss: 1.669964075088501, Val Accuracy: 45.29999923706055\n",
      "Iteration 2100, Epoch 3, Loss: 1.4700086116790771, Accuracy: 48.0296516418457, Val Loss: 1.7992178201675415, Val Accuracy: 43.29999923706055\n",
      "Iteration 2800, Epoch 4, Loss: 1.3608421087265015, Accuracy: 51.78750228881836, Val Loss: 1.850010633468628, Val Accuracy: 44.900001525878906\n",
      "Iteration 3500, Epoch 5, Loss: 1.2544454336166382, Accuracy: 55.365909576416016, Val Loss: 1.833878993988037, Val Accuracy: 46.900001525878906\n",
      "Iteration 4200, Epoch 6, Loss: 1.1603496074676514, Accuracy: 58.952354431152344, Val Loss: 1.9971764087677002, Val Accuracy: 45.0\n",
      "Iteration 4900, Epoch 7, Loss: 1.061544418334961, Accuracy: 62.933738708496094, Val Loss: 2.1923675537109375, Val Accuracy: 44.0\n",
      "Iteration 5600, Epoch 8, Loss: 0.974514901638031, Accuracy: 65.68561553955078, Val Loss: 2.441847324371338, Val Accuracy: 44.10000228881836\n",
      "Iteration 6300, Epoch 9, Loss: 0.8817739486694336, Accuracy: 68.92045593261719, Val Loss: 2.433875560760498, Val Accuracy: 46.099998474121094\n",
      "Iteration 7000, Epoch 10, Loss: 0.8074259161949158, Accuracy: 71.41262817382812, Val Loss: 2.775954484939575, Val Accuracy: 42.0\n",
      "Training took 135.195111 seconds\n",
      "Iteration 0, Epoch 1, Loss: 2.4055538177490234, Accuracy: 9.375, Val Loss: 2.4944870471954346, Val Accuracy: 16.399999618530273\n",
      "Iteration 700, Epoch 1, Loss: 1.7987715005874634, Accuracy: 36.67751693725586, Val Loss: 1.7510855197906494, Val Accuracy: 44.900001525878906\n",
      "Iteration 1400, Epoch 2, Loss: 1.5778604745864868, Accuracy: 44.49674606323242, Val Loss: 1.6853172779083252, Val Accuracy: 44.60000228881836\n",
      "Iteration 2100, Epoch 3, Loss: 1.4355846643447876, Accuracy: 49.247684478759766, Val Loss: 1.778470516204834, Val Accuracy: 46.79999923706055\n",
      "Iteration 2800, Epoch 4, Loss: 1.3092907667160034, Accuracy: 53.743751525878906, Val Loss: 1.9013452529907227, Val Accuracy: 45.599998474121094\n",
      "Iteration 3500, Epoch 5, Loss: 1.1909795999526978, Accuracy: 57.7547607421875, Val Loss: 1.9880435466766357, Val Accuracy: 48.400001525878906\n",
      "Iteration 4200, Epoch 6, Loss: 1.078457236289978, Accuracy: 61.94074630737305, Val Loss: 2.1310317516326904, Val Accuracy: 47.70000076293945\n",
      "Iteration 4900, Epoch 7, Loss: 0.9766424298286438, Accuracy: 65.67725372314453, Val Loss: 2.3833460807800293, Val Accuracy: 47.60000228881836\n",
      "Iteration 5600, Epoch 8, Loss: 0.8795121312141418, Accuracy: 69.12715148925781, Val Loss: 2.365208148956299, Val Accuracy: 45.10000228881836\n",
      "Iteration 6300, Epoch 9, Loss: 0.8066416382789612, Accuracy: 71.7140121459961, Val Loss: 2.6355302333831787, Val Accuracy: 46.599998474121094\n",
      "Iteration 7000, Epoch 10, Loss: 0.7233601808547974, Accuracy: 74.77678680419922, Val Loss: 3.1642589569091797, Val Accuracy: 44.80000305175781\n",
      "Training took 175.319557 seconds\n",
      "Iteration 0, Epoch 1, Loss: 2.3028769493103027, Accuracy: 14.0625, Val Loss: 2.518568992614746, Val Accuracy: 10.40000057220459\n",
      "Iteration 700, Epoch 1, Loss: 1.9018380641937256, Accuracy: 32.417972564697266, Val Loss: 1.771239995956421, Val Accuracy: 39.70000076293945\n",
      "Iteration 1400, Epoch 2, Loss: 1.7083237171173096, Accuracy: 39.4025993347168, Val Loss: 1.7165629863739014, Val Accuracy: 43.900001525878906\n",
      "Iteration 2100, Epoch 3, Loss: 1.6224870681762695, Accuracy: 42.28395080566406, Val Loss: 1.7535245418548584, Val Accuracy: 43.70000076293945\n",
      "Iteration 2800, Epoch 4, Loss: 1.5613524913787842, Accuracy: 44.48125076293945, Val Loss: 1.7260723114013672, Val Accuracy: 43.5\n",
      "Iteration 3500, Epoch 5, Loss: 1.5104025602340698, Accuracy: 46.23268127441406, Val Loss: 1.7702656984329224, Val Accuracy: 45.0\n",
      "Iteration 4200, Epoch 6, Loss: 1.4761027097702026, Accuracy: 47.01161193847656, Val Loss: 1.8237550258636475, Val Accuracy: 44.10000228881836\n",
      "Iteration 4900, Epoch 7, Loss: 1.435310959815979, Accuracy: 48.89736557006836, Val Loss: 1.8669155836105347, Val Accuracy: 43.0\n",
      "Iteration 5600, Epoch 8, Loss: 1.3977888822555542, Accuracy: 50.720638275146484, Val Loss: 1.8725247383117676, Val Accuracy: 44.5\n",
      "Iteration 6300, Epoch 9, Loss: 1.3561933040618896, Accuracy: 51.95075988769531, Val Loss: 1.8924652338027954, Val Accuracy: 42.099998474121094\n",
      "Iteration 7000, Epoch 10, Loss: 1.3231375217437744, Accuracy: 53.57142639160156, Val Loss: 1.9095221757888794, Val Accuracy: 44.10000228881836\n",
      "Training took 95.299835 seconds\n",
      "Iteration 0, Epoch 1, Loss: 2.296518564224243, Accuracy: 12.5, Val Loss: 2.3953776359558105, Val Accuracy: 10.0\n",
      "Iteration 700, Epoch 1, Loss: 2.232433319091797, Accuracy: 16.944543838500977, Val Loss: 2.108613967895508, Val Accuracy: 23.80000114440918\n",
      "Iteration 1400, Epoch 2, Loss: 2.107201337814331, Accuracy: 24.29268455505371, Val Loss: 1.999957799911499, Val Accuracy: 28.5\n",
      "Iteration 2100, Epoch 3, Loss: 2.027703046798706, Accuracy: 28.185626983642578, Val Loss: 1.9411842823028564, Val Accuracy: 31.299999237060547\n",
      "Iteration 2800, Epoch 4, Loss: 1.9702503681182861, Accuracy: 30.625, Val Loss: 1.9094129800796509, Val Accuracy: 32.900001525878906\n",
      "Iteration 3500, Epoch 5, Loss: 1.9275696277618408, Accuracy: 32.07635498046875, Val Loss: 1.8906993865966797, Val Accuracy: 33.89999771118164\n",
      "Iteration 4200, Epoch 6, Loss: 1.897905707359314, Accuracy: 32.974727630615234, Val Loss: 1.8673162460327148, Val Accuracy: 35.70000076293945\n",
      "Iteration 4900, Epoch 7, Loss: 1.8749918937683105, Accuracy: 33.78971481323242, Val Loss: 1.8473762273788452, Val Accuracy: 36.29999923706055\n",
      "Iteration 5600, Epoch 8, Loss: 1.848677635192871, Accuracy: 34.745418548583984, Val Loss: 1.8335530757904053, Val Accuracy: 36.79999923706055\n",
      "Iteration 6300, Epoch 9, Loss: 1.8271658420562744, Accuracy: 35.86174392700195, Val Loss: 1.8219382762908936, Val Accuracy: 38.20000076293945\n",
      "Iteration 7000, Epoch 10, Loss: 1.8004714250564575, Accuracy: 36.431758880615234, Val Loss: 1.8159170150756836, Val Accuracy: 37.900001525878906\n",
      "Training took 103.287846 seconds\n",
      "Iteration 0, Epoch 1, Loss: 2.3405094146728516, Accuracy: 14.0625, Val Loss: 2.4836337566375732, Val Accuracy: 13.0\n",
      "Iteration 700, Epoch 1, Loss: 1.8217074871063232, Accuracy: 35.62544631958008, Val Loss: 1.7543772459030151, Val Accuracy: 43.29999923706055\n",
      "Iteration 1400, Epoch 2, Loss: 1.6027506589889526, Accuracy: 43.232452392578125, Val Loss: 1.7295781373977661, Val Accuracy: 45.5\n",
      "Iteration 2100, Epoch 3, Loss: 1.4742118120193481, Accuracy: 47.81470489501953, Val Loss: 1.7837576866149902, Val Accuracy: 44.80000305175781\n",
      "Iteration 2800, Epoch 4, Loss: 1.3594366312026978, Accuracy: 51.79374694824219, Val Loss: 1.8416484594345093, Val Accuracy: 46.599998474121094\n",
      "Iteration 3500, Epoch 5, Loss: 1.2546404600143433, Accuracy: 55.34786605834961, Val Loss: 1.829417109489441, Val Accuracy: 49.20000076293945\n",
      "Iteration 4200, Epoch 6, Loss: 1.1623139381408691, Accuracy: 59.00358581542969, Val Loss: 2.1401450634002686, Val Accuracy: 48.10000228881836\n",
      "Iteration 4900, Epoch 7, Loss: 1.0682483911514282, Accuracy: 62.30664825439453, Val Loss: 2.160343647003174, Val Accuracy: 47.099998474121094\n",
      "Iteration 5600, Epoch 8, Loss: 0.9740577936172485, Accuracy: 65.59806060791016, Val Loss: 2.2152609825134277, Val Accuracy: 45.0\n",
      "Iteration 6300, Epoch 9, Loss: 0.880852997303009, Accuracy: 68.9772720336914, Val Loss: 2.271867036819458, Val Accuracy: 46.70000076293945\n",
      "Iteration 7000, Epoch 10, Loss: 0.8085896372795105, Accuracy: 71.04591369628906, Val Loss: 2.5581600666046143, Val Accuracy: 47.20000076293945\n",
      "Training took 135.979225 seconds\n",
      "Iteration 0, Epoch 1, Loss: 2.2878482341766357, Accuracy: 12.5, Val Loss: 2.448988676071167, Val Accuracy: 14.399999618530273\n",
      "Iteration 700, Epoch 1, Loss: 1.8191635608673096, Accuracy: 35.69900131225586, Val Loss: 1.7476621866226196, Val Accuracy: 42.20000076293945\n",
      "Iteration 1400, Epoch 2, Loss: 1.6014102697372437, Accuracy: 43.311317443847656, Val Loss: 1.6403192281723022, Val Accuracy: 45.5\n",
      "Iteration 2100, Epoch 3, Loss: 1.4829679727554321, Accuracy: 47.58597946166992, Val Loss: 1.7818702459335327, Val Accuracy: 45.599998474121094\n",
      "Iteration 2800, Epoch 4, Loss: 1.3779716491699219, Accuracy: 51.406253814697266, Val Loss: 1.8185598850250244, Val Accuracy: 44.60000228881836\n",
      "Iteration 3500, Epoch 5, Loss: 1.2815518379211426, Accuracy: 54.6225471496582, Val Loss: 1.8861862421035767, Val Accuracy: 47.5\n",
      "Iteration 4200, Epoch 6, Loss: 1.2055808305740356, Accuracy: 57.231895446777344, Val Loss: 2.0451254844665527, Val Accuracy: 46.20000076293945\n",
      "Iteration 4900, Epoch 7, Loss: 1.1277294158935547, Accuracy: 59.96550750732422, Val Loss: 2.09269380569458, Val Accuracy: 45.20000076293945\n",
      "Iteration 5600, Epoch 8, Loss: 1.0616816282272339, Accuracy: 62.44612503051758, Val Loss: 2.108447551727295, Val Accuracy: 47.400001525878906\n",
      "Iteration 6300, Epoch 9, Loss: 0.984520673751831, Accuracy: 65.2272720336914, Val Loss: 2.212113618850708, Val Accuracy: 47.20000076293945\n",
      "Iteration 7000, Epoch 10, Loss: 0.9063141345977783, Accuracy: 68.11224365234375, Val Loss: 2.521775245666504, Val Accuracy: 41.80000305175781\n",
      "Training took 119.439547 seconds\n",
      "[4.61911768e-05 1.58735002e-05 2.85708575e-06 ... 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Model Ensemble \n",
    "\n",
    "\n",
    "From the models trained above, I identified six of the best models.\n",
    "It turned out that there were two models from each of the 3, 4 and 6 layer networks.\n",
    "For the model ensemble, I use models with no batch normalization and regularization since their usage did not prove to make the models any better.\n",
    "In order to compute the decision, I use the statistical mode and take the most frequently occuring decision as the output.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "\n",
    "# model# 1\n",
    "input_size, hidden_size, num_classes = 128, 128, 10\n",
    "\n",
    "model1 = CustomNet(hidden_size, num_classes) \n",
    "\n",
    "def model_init_fn():    \n",
    "    return model1\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs= num_epochs, is_training=False)\n",
    "\n",
    "pred1 = model1.predict(X_test)\n",
    "\n",
    "# model# 2\n",
    "input_size, hidden_size, num_classes = 256, 256, 10\n",
    "\n",
    "model2 = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return model2\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs= num_epochs, is_training=False)\n",
    "\n",
    "pred2 = model2.predict(X_test)\n",
    "\n",
    "# model# 3\n",
    "input_size, hidden_size, num_classes = 32, 32, 10\n",
    "\n",
    "model3 = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return model3\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs= num_epochs, is_training=False)\n",
    "\n",
    "pred3 = model3.predict(X_test)\n",
    "\n",
    "\n",
    "# model# 4\n",
    "input_size, hidden_size, num_classes = 96, 96, 10\n",
    "\n",
    "model4 = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return model4\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.SGD(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs= num_epochs, is_training=False)\n",
    "\n",
    "pred4 = model4.predict(X_test)\n",
    "\n",
    " \n",
    "# model# 5\n",
    "input_size, hidden_size, num_classes = 128, 128, 10\n",
    "\n",
    "model5 = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return model5\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs= num_epochs, is_training=False)\n",
    "\n",
    "pred5 = model5.predict(X_test)\n",
    "\n",
    "\n",
    "#model# 6\n",
    "input_size, hidden_size, num_classes = 96, 96, 10\n",
    "\n",
    "model6 = CustomNet(hidden_size, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return model6\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "  \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs= num_epochs, is_training=False)\n",
    "\n",
    "pred6 = model6.predict(X_test)\n",
    "\n",
    "\n",
    "final_pred = np.array([])\n",
    "for i in range(0,len(X_test)):\n",
    "    final_pred = np.append(final_pred, stats.mode([pred1[i], pred2[i], pred3[i], pred4[i], pred5[i], pred6[i]]))\n",
    "\n",
    "print(final_pred)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1IBs7seVHXnu",
    "outputId": "b78c592f-54a5-4e20-a36e-5c7c56afb087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01618557 0.00836718 0.05860651 ... 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Us4XSQLlbjf9",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## Test set \n",
    "\n",
    "Now that we've gotten a result we're happy with, we test our final model on the test set (which you should store in best_model). Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "EQaxEKZkbjf-",
    "outputId": "180e51e4-701b-496d-d258-a9f56e3ac9c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "10000/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 92us/sample - loss: 3.1509 - sparse_categorical_accuracy: 0.4454\n",
      "test_loss: 2.7035632587432863, test_accuracy: 0.4453999996185303 \n"
     ]
    }
   ],
   "source": [
    "best_model = model5\n",
    "learning_rate = 1e-3\n",
    "\n",
    "best_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
    "print('test_loss: {}, test_accuracy: {} '.format(test_loss, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment3-TensorFlow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
